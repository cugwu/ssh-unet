------------------------------TRAINING OF MODEL 1------------------------------
You are using the following device: cuda
Batch size is: 2 epochs 5000
--------------------Folder 0-------------------

Cross Entropy Dice Loss
Total parameters count 6488886
Filters: [32, 64, 128, 256, 320],
Kernels: [[1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3]]
Strides: [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]]
GateDynUNet(
  (input_block): GateUnetResBlock(
    (conv1): Convolution(
      (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (conv2): Convolution(
      (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
    (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (shift): Gsf(
      (conv3D): Conv3d(8, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
      (tanh): Tanh()
      (bn): InstanceNorm3d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (relu): LeakyReLU(negative_slope=0.1)
      (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (sigmoid): Sigmoid()
    )
    (conv3): Convolution(
      (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  )
  (downsamples): ModuleList(
    (0): GateUnetResBlock(
      (conv1): Convolution(
        (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv2): Convolution(
        (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
      (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (shift): Gsf(
        (conv3D): Conv3d(16, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
        (tanh): Tanh()
        (bn): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (relu): LeakyReLU(negative_slope=0.1)
        (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (sigmoid): Sigmoid()
      )
      (conv3): Convolution(
        (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (1): GateUnetResBlock(
      (conv1): Convolution(
        (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv2): Convolution(
        (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
      (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (shift): Gsf(
        (conv3D): Conv3d(32, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
        (tanh): Tanh()
        (bn): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (relu): LeakyReLU(negative_slope=0.1)
        (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (sigmoid): Sigmoid()
      )
      (conv3): Convolution(
        (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (2): GateUnetResBlock(
      (conv1): Convolution(
        (conv): Conv3d(128, 256, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv2): Convolution(
        (conv): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
      (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (shift): Gsf(
        (conv3D): Conv3d(64, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
        (tanh): Tanh()
        (bn): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (relu): LeakyReLU(negative_slope=0.1)
        (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (sigmoid): Sigmoid()
      )
      (conv3): Convolution(
        (conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
  )
  (bottleneck): GateUnetResBlock(
    (conv1): Convolution(
      (conv): Conv3d(256, 320, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (conv2): Convolution(
      (conv): Conv3d(320, 320, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
    (norm1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (norm2): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (shift): Gsf(
      (conv3D): Conv3d(80, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
      (tanh): Tanh()
      (bn): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (relu): LeakyReLU(negative_slope=0.1)
      (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (sigmoid): Sigmoid()
    )
    (conv3): Convolution(
      (conv): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  )
  (upsamples): ModuleList(
    (0): GateUnetUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv_block): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(512, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (shift): Gsf(
          (conv3D): Conv3d(64, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
          (tanh): Tanh()
          (bn): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (relu): LeakyReLU(negative_slope=0.1)
          (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (sigmoid): Sigmoid()
        )
        (conv3): Convolution(
          (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (1): GateUnetUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv_block): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (shift): Gsf(
          (conv3D): Conv3d(32, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
          (tanh): Tanh()
          (bn): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (relu): LeakyReLU(negative_slope=0.1)
          (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (sigmoid): Sigmoid()
        )
        (conv3): Convolution(
          (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (2): GateUnetUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv_block): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (shift): Gsf(
          (conv3D): Conv3d(16, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
          (tanh): Tanh()
          (bn): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (relu): LeakyReLU(negative_slope=0.1)
          (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (sigmoid): Sigmoid()
        )
        (conv3): Convolution(
          (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (3): GateUnetUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv_block): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
  )
  (output_block): GateUnetOutBlock(
    (conv1): Convolution(
      (conv): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (act): LeakyReLU(negative_slope=0.01)
    (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (conv): Convolution(
      (conv): Conv3d(32, 14, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (skip_layers): DynUNetSkipLayer(
    (downsample): GateUnetResBlock(
      (conv1): Convolution(
        (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv2): Convolution(
        (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
      (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (shift): Gsf(
        (conv3D): Conv3d(8, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
        (tanh): Tanh()
        (bn): InstanceNorm3d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (relu): LeakyReLU(negative_slope=0.1)
        (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (sigmoid): Sigmoid()
      )
      (conv3): Convolution(
        (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (next_layer): DynUNetSkipLayer(
      (downsample): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (shift): Gsf(
          (conv3D): Conv3d(16, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
          (tanh): Tanh()
          (bn): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (relu): LeakyReLU(negative_slope=0.1)
          (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (sigmoid): Sigmoid()
        )
        (conv3): Convolution(
          (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
      (next_layer): DynUNetSkipLayer(
        (downsample): GateUnetResBlock(
          (conv1): Convolution(
            (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (conv2): Convolution(
            (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (shift): Gsf(
            (conv3D): Conv3d(32, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
            (tanh): Tanh()
            (bn): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (relu): LeakyReLU(negative_slope=0.1)
            (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (sigmoid): Sigmoid()
          )
          (conv3): Convolution(
            (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
        (next_layer): DynUNetSkipLayer(
          (downsample): GateUnetResBlock(
            (conv1): Convolution(
              (conv): Conv3d(128, 256, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (conv2): Convolution(
              (conv): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
            (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (shift): Gsf(
              (conv3D): Conv3d(64, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
              (tanh): Tanh()
              (bn): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (relu): LeakyReLU(negative_slope=0.1)
              (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (sigmoid): Sigmoid()
            )
            (conv3): Convolution(
              (conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (next_layer): GateUnetResBlock(
            (conv1): Convolution(
              (conv): Conv3d(256, 320, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (conv2): Convolution(
              (conv): Conv3d(320, 320, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
            (norm1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (norm2): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (shift): Gsf(
              (conv3D): Conv3d(80, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
              (tanh): Tanh()
              (bn): InstanceNorm3d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (relu): LeakyReLU(negative_slope=0.1)
              (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (sigmoid): Sigmoid()
            )
            (conv3): Convolution(
              (conv): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (upsample): GateUnetUpBlock(
            (transp_conv): Convolution(
              (conv): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (conv_block): GateUnetResBlock(
              (conv1): Convolution(
                (conv): Conv3d(512, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
                (adn): ADN(
                  (D): Dropout(p=0.0, inplace=False)
                )
              )
              (conv2): Convolution(
                (conv): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
                (adn): ADN(
                  (D): Dropout(p=0.0, inplace=False)
                )
              )
              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
              (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (shift): Gsf(
                (conv3D): Conv3d(64, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
                (tanh): Tanh()
                (bn): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (relu): LeakyReLU(negative_slope=0.1)
                (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (sigmoid): Sigmoid()
              )
              (conv3): Convolution(
                (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (adn): ADN(
                  (D): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            )
          )
        )
        (upsample): GateUnetUpBlock(
          (transp_conv): Convolution(
            (conv): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (conv_block): GateUnetResBlock(
            (conv1): Convolution(
              (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (conv2): Convolution(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
            (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (shift): Gsf(
              (conv3D): Conv3d(32, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
              (tanh): Tanh()
              (bn): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (relu): LeakyReLU(negative_slope=0.1)
              (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (sigmoid): Sigmoid()
            )
            (conv3): Convolution(
              (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (upsample): GateUnetUpBlock(
        (transp_conv): Convolution(
          (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv_block): GateUnetResBlock(
          (conv1): Convolution(
            (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (conv2): Convolution(
            (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (shift): Gsf(
            (conv3D): Conv3d(16, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
            (tanh): Tanh()
            (bn): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (relu): LeakyReLU(negative_slope=0.1)
            (channel_conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (channel_conv2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (sigmoid): Sigmoid()
          )
          (conv3): Convolution(
            (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
      )
    )
    (upsample): GateUnetUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv_block): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
  )
)
Writing Tensorboard logs to  /data/vision_group/medical/btcv/results_gsf/logs
Final training  0/4999 loss: 3.6639 time 24.14s
Final training  1/4999 loss: 3.2403 time 25.09s
Final training  2/4999 loss: 3.0213 time 25.64s
Final training  3/4999 loss: 2.8341 time 25.35s
Final training  4/4999 loss: 2.7362 time 25.31s
Final training  5/4999 loss: 2.6282 time 25.54s
Final training  6/4999 loss: 2.5752 time 25.13s
Final training  7/4999 loss: 2.4752 time 25.02s
Final training  8/4999 loss: 2.4054 time 25.14s
Final training  9/4999 loss: 2.3328 time 24.92s
Final training  10/4999 loss: 2.3371 time 25.23s
Final training  11/4999 loss: 2.2391 time 24.97s
Final training  12/4999 loss: 2.1904 time 25.00s
Final training  13/4999 loss: 2.1326 time 24.82s
Final training  14/4999 loss: 2.1166 time 25.53s
Final training  15/4999 loss: 2.0824 time 26.91s
Final training  16/4999 loss: 1.9979 time 26.28s
Final training  17/4999 loss: 2.0482 time 26.57s
Final training  18/4999 loss: 2.0227 time 26.27s
Final training  19/4999 loss: 1.8792 time 26.18s
Final training  20/4999 loss: 1.9232 time 26.46s
Final training  21/4999 loss: 1.8865 time 26.25s
Final training  22/4999 loss: 1.8920 time 24.56s
Final training  23/4999 loss: 1.8382 time 24.51s
Final training  24/4999 loss: 1.7973 time 24.81s
Final training  25/4999 loss: 1.8183 time 25.02s
Final training  26/4999 loss: 1.7528 time 24.30s
Final training  27/4999 loss: 1.7102 time 24.86s
Final training  28/4999 loss: 1.7811 time 25.06s
Final training  29/4999 loss: 1.6951 time 25.13s
Final training  30/4999 loss: 1.7430 time 24.87s
Final training  31/4999 loss: 1.6764 time 25.21s
Final training  32/4999 loss: 1.6515 time 24.98s
Final training  33/4999 loss: 1.7010 time 25.17s
Final training  34/4999 loss: 1.6272 time 25.00s
Final training  35/4999 loss: 1.6176 time 25.09s
Final training  36/4999 loss: 1.6752 time 25.24s
Final training  37/4999 loss: 1.6356 time 25.15s
Final training  38/4999 loss: 1.5853 time 25.43s
Final training  39/4999 loss: 1.6265 time 25.12s
Final training  40/4999 loss: 1.6299 time 25.26s
Final training  41/4999 loss: 1.5859 time 25.19s
Final training  42/4999 loss: 1.5728 time 25.09s
Final training  43/4999 loss: 1.5538 time 25.40s
Final training  44/4999 loss: 1.5564 time 25.17s
Final training  45/4999 loss: 1.5786 time 25.16s
Final training  46/4999 loss: 1.5878 time 25.40s
Final training  47/4999 loss: 1.5404 time 25.05s
Final training  48/4999 loss: 1.5226 time 25.42s
Final training  49/4999 loss: 1.5204 time 24.85s
Final training  50/4999 loss: 1.5484 time 25.12s
Final training  51/4999 loss: 1.5933 time 25.52s
Final training  52/4999 loss: 1.4988 time 25.25s
Final training  53/4999 loss: 1.5566 time 25.31s
Final training  54/4999 loss: 1.5852 time 25.12s
Final training  55/4999 loss: 1.5372 time 25.26s
Final training  56/4999 loss: 1.5573 time 24.93s
Final training  57/4999 loss: 1.5580 time 25.33s
Final training  58/4999 loss: 1.4265 time 25.18s
Final training  59/4999 loss: 1.5660 time 25.20s
Final training  60/4999 loss: 1.5153 time 25.23s
Final training  61/4999 loss: 1.4668 time 25.28s
Final training  62/4999 loss: 1.4914 time 25.47s
Final training  63/4999 loss: 1.5106 time 25.37s
Final training  64/4999 loss: 1.4988 time 25.16s
Final training  65/4999 loss: 1.4610 time 24.82s
Final training  66/4999 loss: 1.4341 time 24.94s
Final training  67/4999 loss: 1.4473 time 25.29s
Final training  68/4999 loss: 1.5741 time 25.21s
Final training  69/4999 loss: 1.5040 time 25.24s
Final training  70/4999 loss: 1.4317 time 25.32s
Final training  71/4999 loss: 1.4454 time 25.18s
Final training  72/4999 loss: 1.4818 time 25.04s
Final training  73/4999 loss: 1.5029 time 24.97s
Final training  74/4999 loss: 1.4786 time 25.15s
Final training  75/4999 loss: 1.4915 time 25.27s
Final training  76/4999 loss: 1.4906 time 25.06s
Final training  77/4999 loss: 1.4728 time 25.07s
Final training  78/4999 loss: 1.4884 time 24.94s
Final training  79/4999 loss: 1.3863 time 25.17s
Final training  80/4999 loss: 1.4426 time 25.17s
Final training  81/4999 loss: 1.3439 time 24.94s
Final training  82/4999 loss: 1.4450 time 25.31s
Final training  83/4999 loss: 1.4316 time 25.23s
Final training  84/4999 loss: 1.4024 time 25.18s
Final training  85/4999 loss: 1.3683 time 25.21s
Final training  86/4999 loss: 1.3851 time 24.93s
Final training  87/4999 loss: 1.4328 time 25.20s
Final training  88/4999 loss: 1.4301 time 25.26s
Final training  89/4999 loss: 1.3918 time 25.04s
Final training  90/4999 loss: 1.5174 time 25.04s
Final training  91/4999 loss: 1.4561 time 25.75s
Final training  92/4999 loss: 1.4756 time 24.98s
Final training  93/4999 loss: 1.5051 time 25.07s
Final training  94/4999 loss: 1.5428 time 25.18s
Final training  95/4999 loss: 1.3707 time 25.33s
Final training  96/4999 loss: 1.4112 time 25.03s
Final training  97/4999 loss: 1.4294 time 25.43s
Final training  98/4999 loss: 1.4367 time 25.44s
Final training  99/4999 loss: 1.4858 time 25.42s
Dice accuracy for each class:  (tensor([9.8764e-01, 1.7863e-04, 2.4165e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        8.0266e-01, 6.4988e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2867e-06,
        0.0000e+00, 0.0000e+00], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  99/4999 acc [ 0.131] time 197.64s
Reset trigger time to 0
new best (0.000000 --> 0.131268). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  100/4999 loss: 1.4041 time 25.18s
Final training  101/4999 loss: 1.4151 time 25.14s
Final training  102/4999 loss: 1.3943 time 25.49s
Final training  103/4999 loss: 1.3807 time 25.16s
Final training  104/4999 loss: 1.4483 time 25.57s
Final training  105/4999 loss: 1.3735 time 25.33s
Final training  106/4999 loss: 1.3693 time 25.67s
Final training  107/4999 loss: 1.4280 time 25.18s
Final training  108/4999 loss: 1.4012 time 25.34s
Final training  109/4999 loss: 1.3323 time 25.45s
Final training  110/4999 loss: 1.3879 time 25.19s
Final training  111/4999 loss: 1.5002 time 25.15s
Final training  112/4999 loss: 1.4046 time 25.40s
Final training  113/4999 loss: 1.3910 time 25.45s
Final training  114/4999 loss: 1.3938 time 25.11s
Final training  115/4999 loss: 1.3721 time 25.28s
Final training  116/4999 loss: 1.3534 time 24.93s
Final training  117/4999 loss: 1.3602 time 24.71s
Final training  118/4999 loss: 1.3936 time 24.87s
Final training  119/4999 loss: 1.3734 time 24.58s
Final training  120/4999 loss: 1.4418 time 25.04s
Final training  121/4999 loss: 1.3868 time 25.49s
Final training  122/4999 loss: 1.3492 time 25.57s
Final training  123/4999 loss: 1.3175 time 25.33s
Final training  124/4999 loss: 1.3568 time 25.48s
Final training  125/4999 loss: 1.3023 time 25.38s
Final training  126/4999 loss: 1.3826 time 25.17s
Final training  127/4999 loss: 1.3796 time 25.15s
Final training  128/4999 loss: 1.3072 time 25.08s
Final training  129/4999 loss: 1.3510 time 25.31s
Final training  130/4999 loss: 1.2966 time 25.29s
Final training  131/4999 loss: 1.3488 time 25.06s
Final training  132/4999 loss: 1.3190 time 25.04s
Final training  133/4999 loss: 1.3598 time 24.99s
Final training  134/4999 loss: 1.2690 time 24.94s
Final training  135/4999 loss: 1.3746 time 24.58s
Final training  136/4999 loss: 1.3268 time 25.30s
Final training  137/4999 loss: 1.3542 time 25.11s
Final training  138/4999 loss: 1.4039 time 25.16s
Final training  139/4999 loss: 1.3661 time 25.29s
Final training  140/4999 loss: 1.3730 time 25.33s
Final training  141/4999 loss: 1.2654 time 25.18s
Final training  142/4999 loss: 1.3507 time 25.14s
Final training  143/4999 loss: 1.3315 time 25.07s
Final training  144/4999 loss: 1.4418 time 25.33s
Final training  145/4999 loss: 1.3219 time 25.21s
Final training  146/4999 loss: 1.3433 time 25.35s
Final training  147/4999 loss: 1.3391 time 25.37s
Final training  148/4999 loss: 1.3830 time 24.64s
Final training  149/4999 loss: 1.3203 time 25.33s
Final training  150/4999 loss: 1.3578 time 25.46s
Final training  151/4999 loss: 1.4084 time 25.27s
Final training  152/4999 loss: 1.3720 time 25.31s
Final training  153/4999 loss: 1.3038 time 25.64s
Final training  154/4999 loss: 1.3174 time 25.74s
Final training  155/4999 loss: 1.2800 time 25.22s
Final training  156/4999 loss: 1.3511 time 25.46s
Final training  157/4999 loss: 1.4353 time 25.23s
Final training  158/4999 loss: 1.2959 time 25.21s
Final training  159/4999 loss: 1.2863 time 25.10s
Final training  160/4999 loss: 1.3479 time 25.21s
Final training  161/4999 loss: 1.3023 time 25.39s
Final training  162/4999 loss: 1.2792 time 25.36s
Final training  163/4999 loss: 1.2851 time 25.35s
Final training  164/4999 loss: 1.3155 time 25.40s
Final training  165/4999 loss: 1.3503 time 25.23s
Final training  166/4999 loss: 1.3335 time 25.46s
Final training  167/4999 loss: 1.2540 time 25.55s
Final training  168/4999 loss: 1.3777 time 25.18s
Final training  169/4999 loss: 1.3292 time 25.23s
Final training  170/4999 loss: 1.3262 time 25.19s
Final training  171/4999 loss: 1.2611 time 25.09s
Final training  172/4999 loss: 1.3395 time 25.26s
Final training  173/4999 loss: 1.2643 time 25.39s
Final training  174/4999 loss: 1.2695 time 25.10s
Final training  175/4999 loss: 1.3795 time 25.29s
Final training  176/4999 loss: 1.3028 time 25.48s
Final training  177/4999 loss: 1.2872 time 25.53s
Final training  178/4999 loss: 1.3195 time 24.80s
Final training  179/4999 loss: 1.2454 time 25.15s
Final training  180/4999 loss: 1.3001 time 25.07s
Final training  181/4999 loss: 1.3064 time 25.30s
Final training  182/4999 loss: 1.2816 time 25.33s
Final training  183/4999 loss: 1.2526 time 25.48s
Final training  184/4999 loss: 1.2748 time 25.15s
Final training  185/4999 loss: 1.2640 time 25.58s
Final training  186/4999 loss: 1.3051 time 25.20s
Final training  187/4999 loss: 1.3037 time 25.45s
Final training  188/4999 loss: 1.2995 time 25.42s
Final training  189/4999 loss: 1.3072 time 25.09s
Final training  190/4999 loss: 1.2490 time 25.62s
Final training  191/4999 loss: 1.3137 time 25.28s
Final training  192/4999 loss: 1.3038 time 25.34s
Final training  193/4999 loss: 1.2690 time 25.35s
Final training  194/4999 loss: 1.2580 time 25.31s
Final training  195/4999 loss: 1.2904 time 25.37s
Final training  196/4999 loss: 1.2600 time 25.06s
Final training  197/4999 loss: 1.3043 time 25.20s
Final training  198/4999 loss: 1.2646 time 24.97s
Final training  199/4999 loss: 1.2769 time 25.26s
Dice accuracy for each class:  (tensor([9.8941e-01, 1.1653e-02, 5.2757e-01, 3.3989e-04, 3.0435e-01, 0.0000e+00,
        7.6059e-01, 2.5374e-01, 6.1464e-01, 9.5344e-02, 7.2480e-02, 1.2216e-03,
        0.0000e+00, 0.0000e+00], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  199/4999 acc [ 0.259] time 194.94s
Reset trigger time to 0
new best (0.131268 --> 0.258885). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  200/4999 loss: 1.2353 time 25.44s
Final training  201/4999 loss: 1.3300 time 25.64s
Final training  202/4999 loss: 1.2542 time 25.32s
Final training  203/4999 loss: 1.2029 time 25.13s
Final training  204/4999 loss: 1.2549 time 25.20s
Final training  205/4999 loss: 1.2493 time 24.75s
Final training  206/4999 loss: 1.2660 time 25.36s
Final training  207/4999 loss: 1.2527 time 25.27s
Final training  208/4999 loss: 1.2433 time 25.26s
Final training  209/4999 loss: 1.2477 time 25.25s
Final training  210/4999 loss: 1.2223 time 25.33s
Final training  211/4999 loss: 1.1970 time 25.24s
Final training  212/4999 loss: 1.2510 time 24.82s
Final training  213/4999 loss: 1.2405 time 25.02s
Final training  214/4999 loss: 1.3252 time 24.69s
Final training  215/4999 loss: 1.2268 time 25.02s
Final training  216/4999 loss: 1.2152 time 24.81s
Final training  217/4999 loss: 1.2264 time 24.88s
Final training  218/4999 loss: 1.2338 time 24.67s
Final training  219/4999 loss: 1.2205 time 24.86s
Final training  220/4999 loss: 1.2060 time 25.35s
Final training  221/4999 loss: 1.1605 time 25.02s
Final training  222/4999 loss: 1.2122 time 25.28s
Final training  223/4999 loss: 1.1923 time 25.11s
Final training  224/4999 loss: 1.2167 time 24.73s
Final training  225/4999 loss: 1.2182 time 24.75s
Final training  226/4999 loss: 1.2189 time 25.03s
Final training  227/4999 loss: 1.1969 time 25.28s
Final training  228/4999 loss: 1.1463 time 25.00s
Final training  229/4999 loss: 1.2571 time 24.97s
Final training  230/4999 loss: 1.2231 time 25.20s
Final training  231/4999 loss: 1.1910 time 24.79s
Final training  232/4999 loss: 1.1462 time 24.88s
Final training  233/4999 loss: 1.2532 time 25.15s
Final training  234/4999 loss: 1.2195 time 25.08s
Final training  235/4999 loss: 1.1671 time 25.00s
Final training  236/4999 loss: 1.1801 time 24.72s
Final training  237/4999 loss: 1.1407 time 25.11s
Final training  238/4999 loss: 1.1722 time 25.37s
Final training  239/4999 loss: 1.1930 time 25.45s
Final training  240/4999 loss: 1.1956 time 25.31s
Final training  241/4999 loss: 1.2311 time 25.11s
Final training  242/4999 loss: 1.2083 time 25.29s
Final training  243/4999 loss: 1.2039 time 25.22s
Final training  244/4999 loss: 1.1983 time 25.31s
Final training  245/4999 loss: 1.1741 time 25.42s
Final training  246/4999 loss: 1.1864 time 25.16s
Final training  247/4999 loss: 1.1490 time 25.32s
Final training  248/4999 loss: 1.1504 time 25.27s
Final training  249/4999 loss: 1.1312 time 25.18s
Final training  250/4999 loss: 1.2080 time 25.25s
Final training  251/4999 loss: 1.2359 time 25.27s
Final training  252/4999 loss: 1.1877 time 24.95s
Final training  253/4999 loss: 1.1628 time 25.19s
Final training  254/4999 loss: 1.1349 time 25.16s
Final training  255/4999 loss: 1.1863 time 25.20s
Final training  256/4999 loss: 1.1499 time 25.44s
Final training  257/4999 loss: 1.1786 time 25.06s
Final training  258/4999 loss: 1.2231 time 25.33s
Final training  259/4999 loss: 1.1553 time 25.18s
Final training  260/4999 loss: 1.1542 time 25.16s
Final training  261/4999 loss: 1.1159 time 25.17s
Final training  262/4999 loss: 1.1524 time 25.07s
Final training  263/4999 loss: 1.1569 time 25.49s
Final training  264/4999 loss: 1.1656 time 25.45s
Final training  265/4999 loss: 1.1532 time 25.08s
Final training  266/4999 loss: 1.2248 time 25.22s
Final training  267/4999 loss: 1.1280 time 25.01s
Final training  268/4999 loss: 1.1797 time 25.67s
Final training  269/4999 loss: 1.1372 time 25.33s
Final training  270/4999 loss: 1.1788 time 25.37s
Final training  271/4999 loss: 1.1638 time 24.79s
Final training  272/4999 loss: 1.1435 time 25.13s
Final training  273/4999 loss: 1.1302 time 25.36s
Final training  274/4999 loss: 1.1106 time 25.25s
Final training  275/4999 loss: 1.1251 time 25.47s
Final training  276/4999 loss: 1.1780 time 25.58s
Final training  277/4999 loss: 1.1901 time 25.27s
Final training  278/4999 loss: 1.1333 time 25.49s
Final training  279/4999 loss: 1.1281 time 25.28s
Final training  280/4999 loss: 1.1670 time 24.98s
Final training  281/4999 loss: 1.1743 time 24.82s
Final training  282/4999 loss: 1.0782 time 25.28s
Final training  283/4999 loss: 1.0614 time 25.19s
Final training  284/4999 loss: 1.1320 time 25.12s
Final training  285/4999 loss: 1.1561 time 24.80s
Final training  286/4999 loss: 1.1066 time 24.83s
Final training  287/4999 loss: 1.0951 time 25.23s
Final training  288/4999 loss: 1.1073 time 25.28s
Final training  289/4999 loss: 1.1477 time 25.23s
Final training  290/4999 loss: 1.1092 time 25.28s
Final training  291/4999 loss: 1.1271 time 25.19s
Final training  292/4999 loss: 1.1317 time 25.31s
Final training  293/4999 loss: 1.1694 time 25.25s
Final training  294/4999 loss: 1.0943 time 25.31s
Final training  295/4999 loss: 1.1060 time 25.02s
Final training  296/4999 loss: 1.1070 time 25.51s
Final training  297/4999 loss: 1.1147 time 25.59s
Final training  298/4999 loss: 1.0967 time 25.51s
Final training  299/4999 loss: 1.1393 time 25.54s
Dice accuracy for each class:  (tensor([0.9925, 0.7581, 0.7019, 0.6342, 0.5572, 0.0000, 0.8637, 0.5653, 0.7848,
        0.5938, 0.4826, 0.2299, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  299/4999 acc [ 0.511] time 194.53s
Reset trigger time to 0
new best (0.258885 --> 0.511229). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  300/4999 loss: 1.0927 time 25.26s
Final training  301/4999 loss: 1.1346 time 25.21s
Final training  302/4999 loss: 1.0956 time 25.41s
Final training  303/4999 loss: 1.1376 time 25.54s
Final training  304/4999 loss: 1.1392 time 25.27s
Final training  305/4999 loss: 1.1650 time 25.46s
Final training  306/4999 loss: 1.1176 time 25.32s
Final training  307/4999 loss: 1.1084 time 25.58s
Final training  308/4999 loss: 1.0996 time 25.27s
Final training  309/4999 loss: 1.1445 time 25.13s
Final training  310/4999 loss: 1.1382 time 25.23s
Final training  311/4999 loss: 1.0849 time 24.90s
Final training  312/4999 loss: 1.1310 time 25.24s
Final training  313/4999 loss: 1.1169 time 25.11s
Final training  314/4999 loss: 1.1313 time 25.66s
Final training  315/4999 loss: 1.0746 time 24.94s
Final training  316/4999 loss: 1.1052 time 24.87s
Final training  317/4999 loss: 1.1182 time 25.01s
Final training  318/4999 loss: 1.0609 time 25.27s
Final training  319/4999 loss: 1.1203 time 25.28s
Final training  320/4999 loss: 1.0834 time 25.10s
Final training  321/4999 loss: 1.1475 time 25.01s
Final training  322/4999 loss: 1.0930 time 25.37s
Final training  323/4999 loss: 1.0774 time 25.28s
Final training  324/4999 loss: 1.0680 time 25.10s
Final training  325/4999 loss: 1.0985 time 25.47s
Final training  326/4999 loss: 1.0768 time 25.18s
Final training  327/4999 loss: 1.0785 time 25.09s
Final training  328/4999 loss: 1.0608 time 25.19s
Final training  329/4999 loss: 1.1145 time 25.12s
Final training  330/4999 loss: 1.0843 time 25.37s
Final training  331/4999 loss: 1.0593 time 25.20s
Final training  332/4999 loss: 1.0518 time 25.15s
Final training  333/4999 loss: 1.0228 time 25.10s
Final training  334/4999 loss: 1.0424 time 25.00s
Final training  335/4999 loss: 1.0390 time 25.08s
Final training  336/4999 loss: 1.1050 time 25.46s
Final training  337/4999 loss: 1.1152 time 25.22s
Final training  338/4999 loss: 1.0761 time 25.66s
Final training  339/4999 loss: 1.0681 time 25.33s
Final training  340/4999 loss: 1.1140 time 25.22s
Final training  341/4999 loss: 1.1331 time 25.36s
Final training  342/4999 loss: 1.0939 time 25.13s
Final training  343/4999 loss: 1.0694 time 25.20s
Final training  344/4999 loss: 1.1349 time 25.43s
Final training  345/4999 loss: 1.0545 time 25.45s
Final training  346/4999 loss: 1.0663 time 25.36s
Final training  347/4999 loss: 1.1151 time 25.48s
Final training  348/4999 loss: 1.0999 time 25.45s
Final training  349/4999 loss: 1.0992 time 25.29s
Final training  350/4999 loss: 1.0990 time 25.33s
Final training  351/4999 loss: 1.0718 time 25.18s
Final training  352/4999 loss: 1.0940 time 25.37s
Final training  353/4999 loss: 1.0853 time 25.51s
Final training  354/4999 loss: 1.0776 time 25.57s
Final training  355/4999 loss: 1.0828 time 25.49s
Final training  356/4999 loss: 1.0814 time 25.23s
Final training  357/4999 loss: 1.0434 time 25.43s
Final training  358/4999 loss: 1.0456 time 25.45s
Final training  359/4999 loss: 1.0389 time 25.40s
Final training  360/4999 loss: 1.0659 time 25.09s
Final training  361/4999 loss: 1.0005 time 24.91s
Final training  362/4999 loss: 1.0612 time 25.22s
Final training  363/4999 loss: 1.0572 time 25.02s
Final training  364/4999 loss: 1.0193 time 25.43s
Final training  365/4999 loss: 1.0594 time 25.22s
Final training  366/4999 loss: 1.0627 time 24.81s
Final training  367/4999 loss: 1.0505 time 25.16s
Final training  368/4999 loss: 1.0769 time 25.35s
Final training  369/4999 loss: 1.0617 time 25.23s
Final training  370/4999 loss: 1.0517 time 25.32s
Final training  371/4999 loss: 1.0506 time 25.52s
Final training  372/4999 loss: 1.0279 time 25.47s
Final training  373/4999 loss: 1.0931 time 25.09s
Final training  374/4999 loss: 1.0524 time 25.31s
Final training  375/4999 loss: 1.0730 time 24.97s
Final training  376/4999 loss: 1.0684 time 25.28s
Final training  377/4999 loss: 1.0330 time 25.20s
Final training  378/4999 loss: 1.0649 time 25.52s
Final training  379/4999 loss: 1.0994 time 25.22s
Final training  380/4999 loss: 1.0351 time 25.20s
Final training  381/4999 loss: 1.0622 time 25.56s
Final training  382/4999 loss: 1.0529 time 25.25s
Final training  383/4999 loss: 1.0441 time 25.34s
Final training  384/4999 loss: 1.0457 time 25.46s
Final training  385/4999 loss: 1.0089 time 25.55s
Final training  386/4999 loss: 1.0271 time 25.41s
Final training  387/4999 loss: 1.0670 time 25.36s
Final training  388/4999 loss: 0.9993 time 25.34s
Final training  389/4999 loss: 0.9854 time 25.62s
Final training  390/4999 loss: 0.9648 time 25.34s
Final training  391/4999 loss: 1.0279 time 25.00s
Final training  392/4999 loss: 1.0192 time 24.99s
Final training  393/4999 loss: 1.0101 time 25.35s
Final training  394/4999 loss: 1.0199 time 25.17s
Final training  395/4999 loss: 1.0497 time 25.36s
Final training  396/4999 loss: 1.0513 time 25.19s
Final training  397/4999 loss: 1.0307 time 25.03s
Final training  398/4999 loss: 1.0360 time 25.26s
Final training  399/4999 loss: 1.0352 time 24.94s
Dice accuracy for each class:  (tensor([0.9939, 0.7546, 0.8454, 0.8557, 0.5185, 0.0000, 0.9209, 0.5774, 0.8250,
        0.6585, 0.6787, 0.5477, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  399/4999 acc [ 0.585] time 192.88s
Reset trigger time to 0
new best (0.511229 --> 0.585042). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  400/4999 loss: 1.0547 time 25.21s
Final training  401/4999 loss: 1.0274 time 25.16s
Final training  402/4999 loss: 1.0083 time 25.43s
Final training  403/4999 loss: 1.0326 time 25.09s
Final training  404/4999 loss: 1.0504 time 25.73s
Final training  405/4999 loss: 1.0506 time 25.45s
Final training  406/4999 loss: 0.9952 time 25.41s
Final training  407/4999 loss: 1.0407 time 25.34s
Final training  408/4999 loss: 0.9986 time 25.14s
Final training  409/4999 loss: 0.9845 time 25.47s
Final training  410/4999 loss: 1.0166 time 25.04s
Final training  411/4999 loss: 1.0078 time 24.84s
Final training  412/4999 loss: 1.0025 time 25.33s
Final training  413/4999 loss: 1.0565 time 25.14s
Final training  414/4999 loss: 1.0028 time 24.74s
Final training  415/4999 loss: 1.0131 time 25.21s
Final training  416/4999 loss: 1.0109 time 25.52s
Final training  417/4999 loss: 1.0545 time 25.19s
Final training  418/4999 loss: 1.0187 time 25.32s
Final training  419/4999 loss: 1.0103 time 25.00s
Final training  420/4999 loss: 1.0081 time 25.44s
Final training  421/4999 loss: 1.0131 time 25.55s
Final training  422/4999 loss: 0.9656 time 25.35s
Final training  423/4999 loss: 0.9730 time 25.28s
Final training  424/4999 loss: 1.0038 time 25.65s
Final training  425/4999 loss: 1.0395 time 25.23s
Final training  426/4999 loss: 1.0183 time 25.34s
Final training  427/4999 loss: 1.0097 time 25.06s
Final training  428/4999 loss: 0.9992 time 25.22s
Final training  429/4999 loss: 1.0204 time 25.52s
Final training  430/4999 loss: 1.0169 time 25.21s
Final training  431/4999 loss: 0.9880 time 25.26s
Final training  432/4999 loss: 1.0107 time 25.40s
Final training  433/4999 loss: 1.0050 time 25.50s
Final training  434/4999 loss: 1.0264 time 25.24s
Final training  435/4999 loss: 1.0188 time 24.57s
Final training  436/4999 loss: 1.0660 time 24.79s
Final training  437/4999 loss: 1.0011 time 25.13s
Final training  438/4999 loss: 1.0387 time 25.18s
Final training  439/4999 loss: 0.9979 time 25.46s
Final training  440/4999 loss: 1.0062 time 25.22s
Final training  441/4999 loss: 0.9873 time 25.22s
Final training  442/4999 loss: 0.9912 time 25.10s
Final training  443/4999 loss: 1.0084 time 25.25s
Final training  444/4999 loss: 1.0079 time 25.43s
Final training  445/4999 loss: 0.9991 time 25.06s
Final training  446/4999 loss: 1.0072 time 25.08s
Final training  447/4999 loss: 1.0164 time 25.43s
Final training  448/4999 loss: 1.0495 time 25.34s
Final training  449/4999 loss: 0.9909 time 25.07s
Final training  450/4999 loss: 1.0037 time 25.44s
Final training  451/4999 loss: 1.0234 time 25.37s
Final training  452/4999 loss: 0.9818 time 25.22s
Final training  453/4999 loss: 0.9642 time 25.27s
Final training  454/4999 loss: 0.9809 time 25.59s
Final training  455/4999 loss: 1.0111 time 25.46s
Final training  456/4999 loss: 0.9896 time 25.26s
Final training  457/4999 loss: 0.9963 time 25.44s
Final training  458/4999 loss: 1.0298 time 25.49s
Final training  459/4999 loss: 0.9905 time 25.13s
Final training  460/4999 loss: 1.0019 time 25.36s
Final training  461/4999 loss: 1.0155 time 25.65s
Final training  462/4999 loss: 0.9930 time 25.52s
Final training  463/4999 loss: 1.0931 time 25.30s
Final training  464/4999 loss: 0.9862 time 25.21s
Final training  465/4999 loss: 1.0128 time 25.32s
Final training  466/4999 loss: 0.9687 time 25.21s
Final training  467/4999 loss: 1.0015 time 25.16s
Final training  468/4999 loss: 1.0070 time 25.70s
Final training  469/4999 loss: 0.9683 time 25.52s
Final training  470/4999 loss: 1.0254 time 25.23s
Final training  471/4999 loss: 1.0146 time 25.38s
Final training  472/4999 loss: 0.9864 time 24.98s
Final training  473/4999 loss: 0.9754 time 25.02s
Final training  474/4999 loss: 0.9753 time 25.22s
Final training  475/4999 loss: 1.0236 time 25.27s
Final training  476/4999 loss: 0.9958 time 25.17s
Final training  477/4999 loss: 0.9998 time 25.20s
Final training  478/4999 loss: 1.0010 time 25.48s
Final training  479/4999 loss: 1.0093 time 25.64s
Final training  480/4999 loss: 0.9841 time 25.49s
Final training  481/4999 loss: 0.9731 time 25.30s
Final training  482/4999 loss: 0.9923 time 25.41s
Final training  483/4999 loss: 1.0577 time 24.88s
Final training  484/4999 loss: 1.0072 time 25.31s
Final training  485/4999 loss: 1.0024 time 25.70s
Final training  486/4999 loss: 0.9695 time 25.44s
Final training  487/4999 loss: 0.9787 time 25.34s
Final training  488/4999 loss: 0.9700 time 25.04s
Final training  489/4999 loss: 0.9647 time 25.21s
Final training  490/4999 loss: 0.9646 time 25.31s
Final training  491/4999 loss: 0.9841 time 25.07s
Final training  492/4999 loss: 0.9603 time 25.45s
Final training  493/4999 loss: 1.0598 time 25.43s
Final training  494/4999 loss: 0.9918 time 25.42s
Final training  495/4999 loss: 0.9965 time 25.76s
Final training  496/4999 loss: 1.0149 time 25.21s
Final training  497/4999 loss: 0.9745 time 25.46s
Final training  498/4999 loss: 0.9863 time 25.57s
Final training  499/4999 loss: 0.9937 time 25.60s
Dice accuracy for each class:  (tensor([0.9948, 0.9056, 0.8534, 0.8733, 0.6905, 0.0000, 0.9262, 0.5882, 0.8487,
        0.7780, 0.6819, 0.5201, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  499/4999 acc [ 0.618] time 193.12s
Reset trigger time to 0
new best (0.585042 --> 0.617619). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  500/4999 loss: 0.9995 time 25.54s
Final training  501/4999 loss: 0.9560 time 25.45s
Final training  502/4999 loss: 0.9889 time 25.52s
Final training  503/4999 loss: 0.9904 time 25.35s
Final training  504/4999 loss: 0.9504 time 25.58s
Final training  505/4999 loss: 0.9531 time 25.48s
Final training  506/4999 loss: 0.9841 time 25.54s
Final training  507/4999 loss: 0.9901 time 25.18s
Final training  508/4999 loss: 0.9728 time 25.02s
Final training  509/4999 loss: 0.9667 time 24.98s
Final training  510/4999 loss: 1.0128 time 25.38s
Final training  511/4999 loss: 0.9429 time 25.35s
Final training  512/4999 loss: 0.9794 time 25.19s
Final training  513/4999 loss: 0.9715 time 24.91s
Final training  514/4999 loss: 0.9775 time 25.58s
Final training  515/4999 loss: 0.9552 time 25.40s
Final training  516/4999 loss: 0.9264 time 25.52s
Final training  517/4999 loss: 0.9435 time 25.58s
Final training  518/4999 loss: 0.9727 time 25.35s
Final training  519/4999 loss: 0.9663 time 25.49s
Final training  520/4999 loss: 0.9448 time 25.35s
Final training  521/4999 loss: 0.9602 time 25.51s
Final training  522/4999 loss: 1.0061 time 25.40s
Final training  523/4999 loss: 0.9733 time 25.19s
Final training  524/4999 loss: 0.9930 time 25.20s
Final training  525/4999 loss: 1.0041 time 25.46s
Final training  526/4999 loss: 0.9662 time 25.47s
Final training  527/4999 loss: 0.9748 time 25.70s
Final training  528/4999 loss: 0.9640 time 25.54s
Final training  529/4999 loss: 0.9874 time 25.74s
Final training  530/4999 loss: 1.0325 time 25.57s
Final training  531/4999 loss: 1.0225 time 25.29s
Final training  532/4999 loss: 0.9718 time 25.84s
Final training  533/4999 loss: 0.9901 time 25.65s
Final training  534/4999 loss: 0.9796 time 25.45s
Final training  535/4999 loss: 0.9579 time 24.80s
Final training  536/4999 loss: 0.9861 time 25.60s
Final training  537/4999 loss: 0.9685 time 25.93s
Final training  538/4999 loss: 0.9682 time 25.53s
Final training  539/4999 loss: 0.9811 time 25.78s
Final training  540/4999 loss: 0.9473 time 25.44s
Final training  541/4999 loss: 0.9859 time 25.57s
Final training  542/4999 loss: 0.9774 time 25.43s
Final training  543/4999 loss: 0.9702 time 25.16s
Final training  544/4999 loss: 0.9594 time 25.68s
Final training  545/4999 loss: 0.9557 time 25.75s
Final training  546/4999 loss: 0.9798 time 25.74s
Final training  547/4999 loss: 0.9439 time 25.65s
Final training  548/4999 loss: 0.9977 time 25.68s
Final training  549/4999 loss: 0.9543 time 25.41s
Final training  550/4999 loss: 0.9666 time 25.60s
Final training  551/4999 loss: 0.9460 time 25.42s
Final training  552/4999 loss: 0.9562 time 25.49s
Final training  553/4999 loss: 0.9627 time 25.46s
Final training  554/4999 loss: 0.9784 time 25.40s
Final training  555/4999 loss: 0.9576 time 25.40s
Final training  556/4999 loss: 0.9844 time 25.52s
Final training  557/4999 loss: 0.9664 time 25.59s
Final training  558/4999 loss: 0.9500 time 25.35s
Final training  559/4999 loss: 0.9940 time 25.33s
Final training  560/4999 loss: 0.9354 time 25.62s
Final training  561/4999 loss: 0.9346 time 25.48s
Final training  562/4999 loss: 0.9373 time 25.58s
Final training  563/4999 loss: 0.9494 time 25.57s
Final training  564/4999 loss: 0.9830 time 25.66s
Final training  565/4999 loss: 0.9700 time 25.72s
Final training  566/4999 loss: 0.9952 time 25.42s
Final training  567/4999 loss: 0.9446 time 25.89s
Final training  568/4999 loss: 0.9486 time 25.90s
Final training  569/4999 loss: 0.9456 time 24.97s
Final training  570/4999 loss: 0.9625 time 25.44s
Final training  571/4999 loss: 0.9576 time 25.76s
Final training  572/4999 loss: 0.9842 time 25.63s
Final training  573/4999 loss: 0.9718 time 25.47s
Final training  574/4999 loss: 0.9441 time 25.74s
Final training  575/4999 loss: 0.9768 time 25.58s
Final training  576/4999 loss: 0.9366 time 25.55s
Final training  577/4999 loss: 0.9689 time 25.42s
Final training  578/4999 loss: 1.0414 time 25.36s
Final training  579/4999 loss: 1.0046 time 25.46s
Final training  580/4999 loss: 0.9848 time 25.63s
Final training  581/4999 loss: 0.9891 time 25.42s
Final training  582/4999 loss: 0.9398 time 25.72s
Final training  583/4999 loss: 0.9261 time 25.83s
Final training  584/4999 loss: 0.9446 time 25.44s
Final training  585/4999 loss: 0.9377 time 25.38s
Final training  586/4999 loss: 0.9556 time 25.68s
Final training  587/4999 loss: 0.9606 time 25.55s
Final training  588/4999 loss: 0.9581 time 26.02s
Final training  589/4999 loss: 0.9487 time 25.63s
Final training  590/4999 loss: 0.9683 time 25.46s
Final training  591/4999 loss: 0.9373 time 25.48s
Final training  592/4999 loss: 0.9257 time 25.23s
Final training  593/4999 loss: 0.9537 time 25.07s
Final training  594/4999 loss: 0.9626 time 25.27s
Final training  595/4999 loss: 0.9701 time 25.31s
Final training  596/4999 loss: 0.9542 time 25.43s
Final training  597/4999 loss: 0.9481 time 25.52s
Final training  598/4999 loss: 0.9332 time 25.49s
Final training  599/4999 loss: 0.9276 time 25.21s
Dice accuracy for each class:  (tensor([0.9957, 0.9082, 0.8623, 0.8807, 0.7491, 0.0000, 0.9439, 0.7271, 0.8529,
        0.7666, 0.6979, 0.7006, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  599/4999 acc [ 0.648] time 192.70s
Reset trigger time to 0
new best (0.617619 --> 0.647751). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  600/4999 loss: 0.9920 time 25.20s
Final training  601/4999 loss: 0.9704 time 25.19s
Final training  602/4999 loss: 0.9497 time 25.28s
Final training  603/4999 loss: 0.9453 time 25.00s
Final training  604/4999 loss: 0.9668 time 25.67s
Final training  605/4999 loss: 0.9732 time 25.36s
Final training  606/4999 loss: 0.9517 time 25.17s
Final training  607/4999 loss: 0.9690 time 25.42s
Final training  608/4999 loss: 0.9660 time 24.20s
Final training  609/4999 loss: 0.9218 time 25.47s
Final training  610/4999 loss: 0.9404 time 25.20s
Final training  611/4999 loss: 0.9584 time 24.73s
Final training  612/4999 loss: 0.9330 time 25.05s
Final training  613/4999 loss: 0.9435 time 25.69s
Final training  614/4999 loss: 0.9475 time 25.39s
Final training  615/4999 loss: 0.9486 time 25.29s
Final training  616/4999 loss: 0.9593 time 25.37s
Final training  617/4999 loss: 0.9472 time 25.17s
Final training  618/4999 loss: 0.9766 time 25.57s
Final training  619/4999 loss: 0.9491 time 25.41s
Final training  620/4999 loss: 0.9243 time 25.69s
Final training  621/4999 loss: 0.9348 time 25.94s
Final training  622/4999 loss: 0.9545 time 25.71s
Final training  623/4999 loss: 0.9505 time 25.70s
Final training  624/4999 loss: 0.9696 time 25.34s
Final training  625/4999 loss: 0.9176 time 25.50s
Final training  626/4999 loss: 0.9665 time 25.33s
Final training  627/4999 loss: 0.9312 time 25.52s
Final training  628/4999 loss: 0.9512 time 25.43s
Final training  629/4999 loss: 0.9346 time 25.65s
Final training  630/4999 loss: 0.9275 time 25.75s
Final training  631/4999 loss: 0.9200 time 25.55s
Final training  632/4999 loss: 0.9424 time 25.46s
Final training  633/4999 loss: 0.9265 time 25.19s
Final training  634/4999 loss: 0.9681 time 25.89s
Final training  635/4999 loss: 0.9452 time 25.50s
Final training  636/4999 loss: 0.9327 time 25.51s
Final training  637/4999 loss: 0.9222 time 25.60s
Final training  638/4999 loss: 0.9192 time 25.37s
Final training  639/4999 loss: 0.9378 time 25.63s
Final training  640/4999 loss: 0.9457 time 25.64s
Final training  641/4999 loss: 0.9976 time 25.69s
Final training  642/4999 loss: 0.9979 time 25.50s
Final training  643/4999 loss: 0.9295 time 25.12s
Final training  644/4999 loss: 0.9258 time 25.35s
Final training  645/4999 loss: 0.9649 time 25.22s
Final training  646/4999 loss: 0.9649 time 25.33s
Final training  647/4999 loss: 0.9458 time 25.23s
Final training  648/4999 loss: 0.9368 time 25.44s
Final training  649/4999 loss: 0.9446 time 25.31s
Final training  650/4999 loss: 0.9459 time 25.56s
Final training  651/4999 loss: 0.9398 time 25.49s
Final training  652/4999 loss: 0.9176 time 25.01s
Final training  653/4999 loss: 0.9230 time 25.61s
Final training  654/4999 loss: 0.9737 time 25.35s
Final training  655/4999 loss: 0.9485 time 25.49s
Final training  656/4999 loss: 0.9434 time 25.34s
Final training  657/4999 loss: 1.0003 time 25.45s
Final training  658/4999 loss: 0.9681 time 25.16s
Final training  659/4999 loss: 0.9497 time 25.41s
Final training  660/4999 loss: 0.9400 time 25.25s
Final training  661/4999 loss: 0.9464 time 25.40s
Final training  662/4999 loss: 0.9557 time 25.23s
Final training  663/4999 loss: 0.9714 time 25.31s
Final training  664/4999 loss: 0.9620 time 25.53s
Final training  665/4999 loss: 0.9212 time 25.47s
Final training  666/4999 loss: 0.9392 time 25.58s
Final training  667/4999 loss: 0.9361 time 25.99s
Final training  668/4999 loss: 0.9827 time 25.66s
Final training  669/4999 loss: 0.9227 time 24.76s
Final training  670/4999 loss: 0.9348 time 25.32s
Final training  671/4999 loss: 0.9199 time 25.45s
Final training  672/4999 loss: 0.9193 time 25.89s
Final training  673/4999 loss: 0.9174 time 25.87s
Final training  674/4999 loss: 0.9298 time 25.46s
Final training  675/4999 loss: 0.9193 time 25.40s
Final training  676/4999 loss: 0.9377 time 25.14s
Final training  677/4999 loss: 0.9555 time 25.06s
Final training  678/4999 loss: 0.9396 time 25.12s
Final training  679/4999 loss: 0.9218 time 25.55s
Final training  680/4999 loss: 0.9446 time 25.40s
Final training  681/4999 loss: 0.9531 time 25.43s
Final training  682/4999 loss: 0.9330 time 25.48s
Final training  683/4999 loss: 0.9319 time 25.31s
Final training  684/4999 loss: 0.9335 time 25.37s
Final training  685/4999 loss: 0.9222 time 25.35s
Final training  686/4999 loss: 0.8995 time 25.53s
Final training  687/4999 loss: 0.8934 time 25.23s
Final training  688/4999 loss: 0.9080 time 25.19s
Final training  689/4999 loss: 0.8979 time 25.29s
Final training  690/4999 loss: 0.9159 time 25.10s
Final training  691/4999 loss: 0.9240 time 25.35s
Final training  692/4999 loss: 0.9097 time 25.61s
Final training  693/4999 loss: 0.9163 time 25.62s
Final training  694/4999 loss: 0.9252 time 25.07s
Final training  695/4999 loss: 0.9257 time 25.27s
Final training  696/4999 loss: 0.9625 time 25.62s
Final training  697/4999 loss: 0.9333 time 25.23s
Final training  698/4999 loss: 0.9499 time 25.38s
Final training  699/4999 loss: 0.9415 time 25.03s
Dice accuracy for each class:  (tensor([0.9949, 0.9360, 0.9231, 0.9274, 0.7469, 0.2239, 0.9210, 0.6214, 0.8534,
        0.8160, 0.7077, 0.5712, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  699/4999 acc [ 0.659] time 194.71s
Reset trigger time to 0
new best (0.647751 --> 0.658932). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  700/4999 loss: 0.9527 time 25.84s
Final training  701/4999 loss: 0.9375 time 25.20s
Final training  702/4999 loss: 0.9195 time 25.54s
Final training  703/4999 loss: 0.9206 time 25.45s
Final training  704/4999 loss: 0.9324 time 25.13s
Final training  705/4999 loss: 0.9006 time 25.42s
Final training  706/4999 loss: 0.9484 time 25.45s
Final training  707/4999 loss: 0.9085 time 25.21s
Final training  708/4999 loss: 0.9179 time 24.82s
Final training  709/4999 loss: 0.9080 time 24.98s
Final training  710/4999 loss: 0.9126 time 25.18s
Final training  711/4999 loss: 0.9429 time 25.32s
Final training  712/4999 loss: 0.8907 time 25.40s
Final training  713/4999 loss: 0.9048 time 25.19s
Final training  714/4999 loss: 0.9168 time 25.61s
Final training  715/4999 loss: 0.9359 time 25.39s
Final training  716/4999 loss: 0.9109 time 25.52s
Final training  717/4999 loss: 0.9309 time 25.23s
Final training  718/4999 loss: 0.9150 time 25.26s
Final training  719/4999 loss: 0.9140 time 25.54s
Final training  720/4999 loss: 0.9224 time 25.20s
Final training  721/4999 loss: 0.9299 time 25.44s
Final training  722/4999 loss: 0.8911 time 25.21s
Final training  723/4999 loss: 0.9148 time 25.04s
Final training  724/4999 loss: 0.9405 time 25.46s
Final training  725/4999 loss: 0.9300 time 25.26s
Final training  726/4999 loss: 0.9392 time 25.54s
Final training  727/4999 loss: 0.9145 time 25.30s
Final training  728/4999 loss: 0.9163 time 25.33s
Final training  729/4999 loss: 0.9568 time 25.54s
Final training  730/4999 loss: 0.9050 time 25.60s
Final training  731/4999 loss: 0.9406 time 25.28s
Final training  732/4999 loss: 0.9479 time 25.28s
Final training  733/4999 loss: 0.9094 time 25.12s
Final training  734/4999 loss: 0.9709 time 25.14s
Final training  735/4999 loss: 0.9245 time 24.94s
Final training  736/4999 loss: 0.9238 time 24.97s
Final training  737/4999 loss: 0.9257 time 25.23s
Final training  738/4999 loss: 0.8986 time 25.31s
Final training  739/4999 loss: 0.9226 time 24.94s
Final training  740/4999 loss: 0.9301 time 25.11s
Final training  741/4999 loss: 0.9306 time 25.08s
Final training  742/4999 loss: 0.9165 time 24.78s
Final training  743/4999 loss: 0.9154 time 25.26s
Final training  744/4999 loss: 0.9094 time 24.79s
Final training  745/4999 loss: 0.9165 time 24.66s
Final training  746/4999 loss: 0.9184 time 25.21s
Final training  747/4999 loss: 0.9318 time 25.14s
Final training  748/4999 loss: 0.9136 time 25.32s
Final training  749/4999 loss: 0.9589 time 25.09s
Final training  750/4999 loss: 0.9042 time 25.25s
Final training  751/4999 loss: 0.9369 time 25.53s
Final training  752/4999 loss: 0.8946 time 25.66s
Final training  753/4999 loss: 0.9098 time 25.32s
Final training  754/4999 loss: 0.9082 time 25.21s
Final training  755/4999 loss: 0.9380 time 25.32s
Final training  756/4999 loss: 0.9034 time 25.38s
Final training  757/4999 loss: 0.9034 time 25.50s
Final training  758/4999 loss: 0.9158 time 25.15s
Final training  759/4999 loss: 0.9123 time 25.48s
Final training  760/4999 loss: 0.9122 time 25.30s
Final training  761/4999 loss: 0.9142 time 25.63s
Final training  762/4999 loss: 0.9166 time 25.86s
Final training  763/4999 loss: 0.9059 time 25.55s
Final training  764/4999 loss: 0.9294 time 25.55s
Final training  765/4999 loss: 0.9610 time 25.59s
Final training  766/4999 loss: 0.9147 time 25.37s
Final training  767/4999 loss: 0.9122 time 25.35s
Final training  768/4999 loss: 0.9127 time 25.59s
Final training  769/4999 loss: 0.9064 time 25.37s
Final training  770/4999 loss: 0.9096 time 25.47s
Final training  771/4999 loss: 0.9086 time 25.64s
Final training  772/4999 loss: 0.9069 time 25.51s
Final training  773/4999 loss: 0.9173 time 25.21s
Final training  774/4999 loss: 0.8868 time 25.28s
Final training  775/4999 loss: 0.8842 time 25.42s
Final training  776/4999 loss: 0.8899 time 25.34s
Final training  777/4999 loss: 0.9080 time 25.43s
Final training  778/4999 loss: 0.8886 time 25.31s
Final training  779/4999 loss: 0.8854 time 25.27s
Final training  780/4999 loss: 0.8803 time 25.20s
Final training  781/4999 loss: 0.8845 time 25.39s
Final training  782/4999 loss: 0.9120 time 25.46s
Final training  783/4999 loss: 0.9221 time 25.45s
Final training  784/4999 loss: 0.9147 time 25.02s
Final training  785/4999 loss: 0.8927 time 25.49s
Final training  786/4999 loss: 0.9195 time 25.68s
Final training  787/4999 loss: 0.8938 time 25.49s
Final training  788/4999 loss: 0.9269 time 25.54s
Final training  789/4999 loss: 0.9529 time 25.54s
Final training  790/4999 loss: 0.9320 time 25.33s
Final training  791/4999 loss: 0.9363 time 25.58s
Final training  792/4999 loss: 0.9285 time 25.56s
Final training  793/4999 loss: 0.9220 time 25.67s
Final training  794/4999 loss: 0.9156 time 25.78s
Final training  795/4999 loss: 0.9113 time 25.36s
Final training  796/4999 loss: 0.9035 time 25.21s
Final training  797/4999 loss: 0.9152 time 25.43s
Final training  798/4999 loss: 0.9149 time 25.69s
Final training  799/4999 loss: 0.8982 time 25.36s
Dice accuracy for each class:  (tensor([0.9946, 0.9286, 0.8819, 0.9061, 0.7637, 0.5214, 0.8991, 0.6424, 0.8581,
        0.7997, 0.7270, 0.6738, 0.3511, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  799/4999 acc [ 0.710] time 194.54s
Reset trigger time to 0
new best (0.658932 --> 0.710063). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  800/4999 loss: 0.9100 time 25.17s
Final training  801/4999 loss: 0.8795 time 25.33s
Final training  802/4999 loss: 0.9070 time 25.47s
Final training  803/4999 loss: 0.9180 time 25.07s
Final training  804/4999 loss: 0.8682 time 24.90s
Final training  805/4999 loss: 0.9029 time 25.05s
Final training  806/4999 loss: 0.8793 time 25.24s
Final training  807/4999 loss: 0.9038 time 24.61s
Final training  808/4999 loss: 0.9172 time 25.20s
Final training  809/4999 loss: 0.9047 time 25.28s
Final training  810/4999 loss: 0.9163 time 25.52s
Final training  811/4999 loss: 0.9205 time 25.43s
Final training  812/4999 loss: 0.8912 time 25.27s
Final training  813/4999 loss: 0.9040 time 25.70s
Final training  814/4999 loss: 0.9091 time 25.37s
Final training  815/4999 loss: 0.9118 time 25.71s
Final training  816/4999 loss: 0.9253 time 25.20s
Final training  817/4999 loss: 0.9020 time 25.32s
Final training  818/4999 loss: 0.8918 time 25.56s
Final training  819/4999 loss: 0.8920 time 25.31s
Final training  820/4999 loss: 0.9494 time 25.62s
Final training  821/4999 loss: 0.9437 time 25.52s
Final training  822/4999 loss: 0.9376 time 25.12s
Final training  823/4999 loss: 0.9507 time 25.65s
Final training  824/4999 loss: 0.9245 time 25.21s
Final training  825/4999 loss: 0.9291 time 25.42s
Final training  826/4999 loss: 0.9177 time 25.29s
Final training  827/4999 loss: 0.8877 time 25.36s
Final training  828/4999 loss: 0.8960 time 25.43s
Final training  829/4999 loss: 0.8746 time 25.42s
Final training  830/4999 loss: 0.9140 time 25.43s
Final training  831/4999 loss: 0.8859 time 25.62s
Final training  832/4999 loss: 0.9276 time 25.63s
Final training  833/4999 loss: 0.9115 time 25.15s
Final training  834/4999 loss: 0.9211 time 25.34s
Final training  835/4999 loss: 0.9073 time 25.44s
Final training  836/4999 loss: 0.9247 time 25.45s
Final training  837/4999 loss: 0.9217 time 25.55s
Final training  838/4999 loss: 0.8987 time 25.43s
Final training  839/4999 loss: 0.8716 time 25.40s
Final training  840/4999 loss: 0.9062 time 25.20s
Final training  841/4999 loss: 0.9135 time 25.38s
Final training  842/4999 loss: 0.9034 time 25.52s
Final training  843/4999 loss: 0.8894 time 25.38s
Final training  844/4999 loss: 0.8852 time 25.66s
Final training  845/4999 loss: 0.8904 time 25.49s
Final training  846/4999 loss: 0.8965 time 25.54s
Final training  847/4999 loss: 0.9128 time 25.23s
Final training  848/4999 loss: 0.9473 time 25.47s
Final training  849/4999 loss: 0.8939 time 25.63s
Final training  850/4999 loss: 0.9316 time 25.49s
Final training  851/4999 loss: 0.8897 time 25.69s
Final training  852/4999 loss: 0.8929 time 25.40s
Final training  853/4999 loss: 0.8866 time 25.54s
Final training  854/4999 loss: 0.9013 time 25.58s
Final training  855/4999 loss: 0.8885 time 25.39s
Final training  856/4999 loss: 0.8810 time 25.68s
Final training  857/4999 loss: 0.8781 time 25.92s
Final training  858/4999 loss: 0.9018 time 25.73s
Final training  859/4999 loss: 0.9009 time 25.53s
Final training  860/4999 loss: 0.8901 time 25.56s
Final training  861/4999 loss: 0.9438 time 25.38s
Final training  862/4999 loss: 0.9305 time 25.25s
Final training  863/4999 loss: 0.8973 time 25.45s
Final training  864/4999 loss: 0.9199 time 25.35s
Final training  865/4999 loss: 0.8795 time 25.48s
Final training  866/4999 loss: 0.9007 time 25.79s
Final training  867/4999 loss: 0.9015 time 25.69s
Final training  868/4999 loss: 0.8786 time 24.87s
Final training  869/4999 loss: 0.8705 time 25.33s
Final training  870/4999 loss: 0.8994 time 25.55s
Final training  871/4999 loss: 0.9125 time 25.71s
Final training  872/4999 loss: 0.8763 time 25.50s
Final training  873/4999 loss: 0.9224 time 25.42s
Final training  874/4999 loss: 0.8824 time 25.33s
Final training  875/4999 loss: 0.8936 time 25.73s
Final training  876/4999 loss: 0.8772 time 25.43s
Final training  877/4999 loss: 0.8795 time 25.48s
Final training  878/4999 loss: 0.8655 time 25.54s
Final training  879/4999 loss: 0.8931 time 25.49s
Final training  880/4999 loss: 0.8834 time 25.46s
Final training  881/4999 loss: 0.9104 time 25.47s
Final training  882/4999 loss: 0.8851 time 25.45s
Final training  883/4999 loss: 0.8716 time 25.06s
Final training  884/4999 loss: 0.8961 time 25.12s
Final training  885/4999 loss: 0.8725 time 25.66s
Final training  886/4999 loss: 0.9093 time 25.55s
Final training  887/4999 loss: 0.8872 time 25.39s
Final training  888/4999 loss: 0.8651 time 25.32s
Final training  889/4999 loss: 0.8836 time 25.44s
Final training  890/4999 loss: 0.8917 time 25.85s
Final training  891/4999 loss: 0.8816 time 25.73s
Final training  892/4999 loss: 0.8642 time 25.33s
Final training  893/4999 loss: 0.8810 time 25.77s
Final training  894/4999 loss: 0.9060 time 25.10s
Final training  895/4999 loss: 0.8922 time 25.38s
Final training  896/4999 loss: 0.8699 time 25.51s
Final training  897/4999 loss: 0.8741 time 25.58s
Final training  898/4999 loss: 0.8722 time 25.56s
Final training  899/4999 loss: 0.8778 time 25.65s
Dice accuracy for each class:  (tensor([0.9953, 0.9197, 0.9306, 0.9287, 0.7516, 0.5923, 0.9229, 0.7095, 0.8652,
        0.7949, 0.7031, 0.7419, 0.5258, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  899/4999 acc [ 0.741] time 195.01s
Reset trigger time to 0
new best (0.710063 --> 0.741273). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  900/4999 loss: 0.9305 time 25.40s
Final training  901/4999 loss: 0.8789 time 25.29s
Final training  902/4999 loss: 0.9318 time 25.38s
Final training  903/4999 loss: 0.8980 time 25.01s
Final training  904/4999 loss: 0.8961 time 24.99s
Final training  905/4999 loss: 0.9173 time 25.03s
Final training  906/4999 loss: 0.9351 time 24.94s
Final training  907/4999 loss: 0.8770 time 25.52s
Final training  908/4999 loss: 0.8721 time 25.31s
Final training  909/4999 loss: 0.8947 time 25.91s
Final training  910/4999 loss: 0.8947 time 25.68s
Final training  911/4999 loss: 0.8693 time 25.45s
Final training  912/4999 loss: 0.8785 time 25.76s
Final training  913/4999 loss: 0.8914 time 25.36s
Final training  914/4999 loss: 0.8729 time 25.51s
Final training  915/4999 loss: 0.8803 time 25.79s
Final training  916/4999 loss: 0.8790 time 25.74s
Final training  917/4999 loss: 0.8799 time 25.82s
Final training  918/4999 loss: 0.9194 time 25.29s
Final training  919/4999 loss: 0.9087 time 25.46s
Final training  920/4999 loss: 0.8861 time 25.34s
Final training  921/4999 loss: 0.8950 time 25.63s
Final training  922/4999 loss: 0.9078 time 25.79s
Final training  923/4999 loss: 0.8837 time 25.39s
Final training  924/4999 loss: 0.8846 time 25.48s
Final training  925/4999 loss: 0.9050 time 25.21s
Final training  926/4999 loss: 0.8983 time 25.39s
Final training  927/4999 loss: 0.8901 time 25.40s
Final training  928/4999 loss: 0.8880 time 25.47s
Final training  929/4999 loss: 0.9118 time 25.03s
Final training  930/4999 loss: 0.8943 time 25.34s
Final training  931/4999 loss: 0.8873 time 25.16s
Final training  932/4999 loss: 0.8810 time 25.56s
Final training  933/4999 loss: 0.8726 time 25.10s
Final training  934/4999 loss: 0.9011 time 25.19s
Final training  935/4999 loss: 0.8948 time 25.34s
Final training  936/4999 loss: 0.8887 time 25.34s
Final training  937/4999 loss: 0.8811 time 25.93s
Final training  938/4999 loss: 0.8887 time 25.50s
Final training  939/4999 loss: 0.8717 time 25.55s
Final training  940/4999 loss: 0.8957 time 25.40s
Final training  941/4999 loss: 0.8708 time 25.23s
Final training  942/4999 loss: 0.8860 time 25.72s
Final training  943/4999 loss: 0.8735 time 25.53s
Final training  944/4999 loss: 0.8740 time 25.38s
Final training  945/4999 loss: 0.8676 time 25.42s
Final training  946/4999 loss: 0.8774 time 25.58s
Final training  947/4999 loss: 0.8853 time 25.43s
Final training  948/4999 loss: 0.8772 time 25.49s
Final training  949/4999 loss: 0.8685 time 25.65s
Final training  950/4999 loss: 0.8553 time 25.23s
Final training  951/4999 loss: 0.8831 time 25.53s
Final training  952/4999 loss: 0.8569 time 25.61s
Final training  953/4999 loss: 0.9016 time 25.59s
Final training  954/4999 loss: 0.8875 time 25.61s
Final training  955/4999 loss: 0.8821 time 25.15s
Final training  956/4999 loss: 0.8971 time 24.89s
Final training  957/4999 loss: 0.9082 time 25.14s
Final training  958/4999 loss: 0.8684 time 25.07s
Final training  959/4999 loss: 0.8971 time 25.46s
Final training  960/4999 loss: 0.9292 time 25.48s
Final training  961/4999 loss: 0.9239 time 25.34s
Final training  962/4999 loss: 0.8678 time 25.19s
Final training  963/4999 loss: 0.9118 time 25.15s
Final training  964/4999 loss: 1.0105 time 25.25s
Final training  965/4999 loss: 0.9090 time 25.33s
Final training  966/4999 loss: 0.9305 time 25.11s
Final training  967/4999 loss: 0.9224 time 25.38s
Final training  968/4999 loss: 0.9069 time 25.26s
Final training  969/4999 loss: 0.8898 time 25.53s
Final training  970/4999 loss: 0.8931 time 25.70s
Final training  971/4999 loss: 0.9080 time 25.59s
Final training  972/4999 loss: 0.8920 time 25.53s
Final training  973/4999 loss: 0.9178 time 25.75s
Final training  974/4999 loss: 0.8949 time 25.35s
Final training  975/4999 loss: 0.9329 time 25.49s
Final training  976/4999 loss: 0.8976 time 25.35s
Final training  977/4999 loss: 0.8742 time 25.49s
Final training  978/4999 loss: 0.8827 time 25.36s
Final training  979/4999 loss: 0.8889 time 25.65s
Final training  980/4999 loss: 0.8678 time 25.35s
Final training  981/4999 loss: 0.9080 time 25.37s
Final training  982/4999 loss: 0.8625 time 25.45s
Final training  983/4999 loss: 0.8897 time 25.62s
Final training  984/4999 loss: 0.8760 time 25.67s
Final training  985/4999 loss: 0.9113 time 25.58s
Final training  986/4999 loss: 0.9074 time 25.58s
Final training  987/4999 loss: 0.8950 time 25.61s
Final training  988/4999 loss: 0.8763 time 25.34s
Final training  989/4999 loss: 0.8798 time 25.79s
Final training  990/4999 loss: 0.8640 time 25.56s
Final training  991/4999 loss: 0.8988 time 25.76s
Final training  992/4999 loss: 0.8723 time 25.31s
Final training  993/4999 loss: 0.8880 time 25.57s
Final training  994/4999 loss: 0.8717 time 25.43s
Final training  995/4999 loss: 0.8955 time 25.25s
Final training  996/4999 loss: 0.8938 time 25.68s
Final training  997/4999 loss: 0.8894 time 25.26s
Final training  998/4999 loss: 0.8719 time 25.46s
Final training  999/4999 loss: 0.8509 time 25.43s
Dice accuracy for each class:  (tensor([0.9953, 0.8640, 0.8255, 0.7731, 0.7690, 0.6832, 0.9321, 0.6863, 0.8885,
        0.8165, 0.7215, 0.7392, 0.6225, 0.0104], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  999/4999 acc [ 0.737] time 194.40s
trigger times: 1
Final training  1000/4999 loss: 0.8957 time 25.24s
Final training  1001/4999 loss: 0.8640 time 25.22s
Final training  1002/4999 loss: 0.8646 time 24.54s
Final training  1003/4999 loss: 0.8679 time 25.15s
Final training  1004/4999 loss: 0.8732 time 24.67s
Final training  1005/4999 loss: 0.8689 time 24.98s
Final training  1006/4999 loss: 0.8644 time 25.31s
Final training  1007/4999 loss: 0.8850 time 25.48s
Final training  1008/4999 loss: 0.8936 time 25.27s
Final training  1009/4999 loss: 0.8960 time 25.24s
Final training  1010/4999 loss: 0.8812 time 25.51s
Final training  1011/4999 loss: 0.8556 time 25.34s
Final training  1012/4999 loss: 0.8595 time 25.41s
Final training  1013/4999 loss: 0.8938 time 25.66s
Final training  1014/4999 loss: 0.8779 time 25.43s
Final training  1015/4999 loss: 0.8823 time 25.63s
Final training  1016/4999 loss: 0.8490 time 25.39s
Final training  1017/4999 loss: 0.9251 time 25.54s
Final training  1018/4999 loss: 0.9130 time 25.50s
Final training  1019/4999 loss: 0.9131 time 25.18s
Final training  1020/4999 loss: 0.9089 time 25.18s
Final training  1021/4999 loss: 0.8999 time 25.38s
Final training  1022/4999 loss: 0.8680 time 25.53s
Final training  1023/4999 loss: 0.8652 time 25.68s
Final training  1024/4999 loss: 0.8836 time 25.77s
Final training  1025/4999 loss: 0.8957 time 25.42s
Final training  1026/4999 loss: 0.8603 time 25.26s
Final training  1027/4999 loss: 0.8613 time 25.34s
Final training  1028/4999 loss: 0.8874 time 25.52s
Final training  1029/4999 loss: 0.8710 time 25.58s
Final training  1030/4999 loss: 0.8675 time 25.41s
Final training  1031/4999 loss: 0.9079 time 25.72s
Final training  1032/4999 loss: 0.8959 time 25.38s
Final training  1033/4999 loss: 0.8994 time 25.05s
Final training  1034/4999 loss: 0.8837 time 25.44s
Final training  1035/4999 loss: 0.9218 time 25.21s
Final training  1036/4999 loss: 0.8993 time 25.62s
Final training  1037/4999 loss: 0.8612 time 25.50s
Final training  1038/4999 loss: 0.8754 time 25.66s
Final training  1039/4999 loss: 0.8772 time 25.36s
Final training  1040/4999 loss: 0.9036 time 25.27s
Final training  1041/4999 loss: 0.8714 time 25.28s
Final training  1042/4999 loss: 0.9080 time 25.17s
Final training  1043/4999 loss: 0.8689 time 25.30s
Final training  1044/4999 loss: 0.8675 time 25.25s
Final training  1045/4999 loss: 0.8963 time 25.44s
Final training  1046/4999 loss: 0.8548 time 25.50s
Final training  1047/4999 loss: 0.8632 time 25.44s
Final training  1048/4999 loss: 0.8740 time 25.28s
Final training  1049/4999 loss: 0.8846 time 25.72s
Final training  1050/4999 loss: 0.8649 time 25.52s
Final training  1051/4999 loss: 0.8777 time 25.54s
Final training  1052/4999 loss: 0.8642 time 25.68s
Final training  1053/4999 loss: 0.8486 time 25.30s
Final training  1054/4999 loss: 0.8604 time 25.37s
Final training  1055/4999 loss: 0.8753 time 25.52s
Final training  1056/4999 loss: 0.8606 time 25.50s
Final training  1057/4999 loss: 0.8887 time 25.31s
Final training  1058/4999 loss: 0.8718 time 25.38s
Final training  1059/4999 loss: 0.8524 time 25.24s
Final training  1060/4999 loss: 0.8902 time 25.15s
Final training  1061/4999 loss: 0.8781 time 25.38s
Final training  1062/4999 loss: 0.8514 time 25.68s
Final training  1063/4999 loss: 0.9020 time 25.56s
Final training  1064/4999 loss: 0.8966 time 25.35s
Final training  1065/4999 loss: 0.8907 time 25.20s
Final training  1066/4999 loss: 0.8627 time 25.25s
Final training  1067/4999 loss: 0.8856 time 25.50s
Final training  1068/4999 loss: 0.8797 time 25.28s
Final training  1069/4999 loss: 0.8961 time 25.37s
Final training  1070/4999 loss: 0.8526 time 25.34s
Final training  1071/4999 loss: 0.8589 time 25.65s
Final training  1072/4999 loss: 0.8691 time 25.29s
Final training  1073/4999 loss: 0.8567 time 25.43s
Final training  1074/4999 loss: 0.8535 time 25.49s
Final training  1075/4999 loss: 0.8371 time 25.41s
Final training  1076/4999 loss: 0.8483 time 25.48s
Final training  1077/4999 loss: 0.8474 time 25.39s
Final training  1078/4999 loss: 0.8493 time 25.60s
Final training  1079/4999 loss: 0.8595 time 25.41s
Final training  1080/4999 loss: 0.8782 time 25.83s
Final training  1081/4999 loss: 0.8720 time 25.33s
Final training  1082/4999 loss: 0.8658 time 25.59s
Final training  1083/4999 loss: 0.8535 time 25.68s
Final training  1084/4999 loss: 0.8703 time 25.73s
Final training  1085/4999 loss: 0.8865 time 25.64s
Final training  1086/4999 loss: 0.8606 time 25.49s
Final training  1087/4999 loss: 0.8456 time 25.30s
Final training  1088/4999 loss: 0.8470 time 25.35s
Final training  1089/4999 loss: 0.8674 time 25.27s
Final training  1090/4999 loss: 0.8517 time 25.73s
Final training  1091/4999 loss: 0.8534 time 25.70s
Final training  1092/4999 loss: 0.8573 time 25.25s
Final training  1093/4999 loss: 0.8989 time 25.38s
Final training  1094/4999 loss: 0.8515 time 25.38s
Final training  1095/4999 loss: 0.8402 time 25.47s
Final training  1096/4999 loss: 0.8526 time 25.42s
Final training  1097/4999 loss: 0.8434 time 25.64s
Final training  1098/4999 loss: 0.8540 time 25.32s
Final training  1099/4999 loss: 0.8606 time 25.39s
Dice accuracy for each class:  (tensor([0.9954, 0.8907, 0.9344, 0.9235, 0.7646, 0.6928, 0.9309, 0.7145, 0.8865,
        0.8234, 0.7313, 0.7243, 0.6403, 0.4848], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1099/4999 acc [ 0.795] time 193.27s
Reset trigger time to 0
new best (0.741273 --> 0.795323). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  1100/4999 loss: 0.8497 time 25.07s
Final training  1101/4999 loss: 0.8670 time 25.34s
Final training  1102/4999 loss: 0.8690 time 24.64s
Final training  1103/4999 loss: 0.9055 time 24.99s
Final training  1104/4999 loss: 0.9158 time 25.46s
Final training  1105/4999 loss: 0.8761 time 25.38s
Final training  1106/4999 loss: 0.8961 time 25.34s
Final training  1107/4999 loss: 0.8607 time 25.39s
Final training  1108/4999 loss: 0.8844 time 25.16s
Final training  1109/4999 loss: 0.9138 time 25.59s
Final training  1110/4999 loss: 0.8979 time 25.30s
Final training  1111/4999 loss: 0.8664 time 24.71s
Final training  1112/4999 loss: 0.8663 time 25.60s
Final training  1113/4999 loss: 0.8952 time 25.43s
Final training  1114/4999 loss: 0.8724 time 25.48s
Final training  1115/4999 loss: 0.8559 time 25.61s
Final training  1116/4999 loss: 0.8669 time 25.48s
Final training  1117/4999 loss: 0.8617 time 25.15s
Final training  1118/4999 loss: 0.8594 time 25.33s
Final training  1119/4999 loss: 0.8693 time 25.35s
Final training  1120/4999 loss: 0.9094 time 25.56s
Final training  1121/4999 loss: 0.8853 time 25.44s
Final training  1122/4999 loss: 0.8444 time 25.49s
Final training  1123/4999 loss: 0.8517 time 25.57s
Final training  1124/4999 loss: 0.8606 time 25.16s
Final training  1125/4999 loss: 0.8749 time 25.16s
Final training  1126/4999 loss: 0.8816 time 25.19s
Final training  1127/4999 loss: 0.8583 time 25.20s
Final training  1128/4999 loss: 0.8646 time 25.45s
Final training  1129/4999 loss: 0.8642 time 25.07s
Final training  1130/4999 loss: 0.8543 time 25.27s
Final training  1131/4999 loss: 0.8756 time 25.07s
Final training  1132/4999 loss: 0.8496 time 25.20s
Final training  1133/4999 loss: 0.8814 time 25.36s
Final training  1134/4999 loss: 0.8538 time 25.08s
Final training  1135/4999 loss: 0.8813 time 25.13s
Final training  1136/4999 loss: 0.8465 time 24.93s
Final training  1137/4999 loss: 0.8568 time 24.95s
Final training  1138/4999 loss: 0.8603 time 24.67s
Final training  1139/4999 loss: 0.8597 time 24.75s
Final training  1140/4999 loss: 0.8605 time 24.65s
Final training  1141/4999 loss: 0.8442 time 24.83s
Final training  1142/4999 loss: 0.8595 time 24.93s
Final training  1143/4999 loss: 0.8652 time 25.47s
Final training  1144/4999 loss: 0.8679 time 25.41s
Final training  1145/4999 loss: 0.8810 time 25.77s
Final training  1146/4999 loss: 0.8712 time 25.33s
Final training  1147/4999 loss: 0.8634 time 25.54s
Final training  1148/4999 loss: 0.8740 time 25.42s
Final training  1149/4999 loss: 0.8499 time 25.53s
Final training  1150/4999 loss: 0.8738 time 25.69s
Final training  1151/4999 loss: 0.8643 time 25.62s
Final training  1152/4999 loss: 0.8821 time 25.61s
Final training  1153/4999 loss: 0.8379 time 25.37s
Final training  1154/4999 loss: 0.8627 time 25.47s
Final training  1155/4999 loss: 0.8562 time 25.22s
Final training  1156/4999 loss: 0.8663 time 25.63s
Final training  1157/4999 loss: 0.8601 time 25.24s
Final training  1158/4999 loss: 0.8586 time 25.54s
Final training  1159/4999 loss: 0.8640 time 25.36s
Final training  1160/4999 loss: 0.8662 time 25.51s
Final training  1161/4999 loss: 0.8819 time 25.39s
Final training  1162/4999 loss: 0.8699 time 25.60s
Final training  1163/4999 loss: 0.8587 time 25.43s
Final training  1164/4999 loss: 0.8825 time 25.57s
Final training  1165/4999 loss: 0.9121 time 25.60s
Final training  1166/4999 loss: 0.8931 time 25.35s
Final training  1167/4999 loss: 0.8594 time 25.52s
Final training  1168/4999 loss: 0.8475 time 25.61s
Final training  1169/4999 loss: 0.8483 time 25.66s
Final training  1170/4999 loss: 0.8563 time 25.40s
Final training  1171/4999 loss: 0.8642 time 25.47s
Final training  1172/4999 loss: 0.8691 time 25.38s
Final training  1173/4999 loss: 0.8653 time 25.25s
Final training  1174/4999 loss: 0.8525 time 25.25s
Final training  1175/4999 loss: 0.8506 time 25.59s
Final training  1176/4999 loss: 0.8800 time 25.41s
Final training  1177/4999 loss: 0.8725 time 26.21s
Final training  1178/4999 loss: 0.8691 time 25.42s
Final training  1179/4999 loss: 0.8684 time 25.58s
Final training  1180/4999 loss: 0.8765 time 25.31s
Final training  1181/4999 loss: 0.8469 time 25.42s
Final training  1182/4999 loss: 0.8903 time 25.46s
Final training  1183/4999 loss: 0.8694 time 25.19s
Final training  1184/4999 loss: 0.8760 time 25.59s
Final training  1185/4999 loss: 0.8431 time 25.01s
Final training  1186/4999 loss: 0.8791 time 25.45s
Final training  1187/4999 loss: 0.8622 time 25.29s
Final training  1188/4999 loss: 0.8937 time 25.49s
Final training  1189/4999 loss: 0.8748 time 25.53s
Final training  1190/4999 loss: 0.8746 time 25.08s
Final training  1191/4999 loss: 0.8301 time 25.48s
Final training  1192/4999 loss: 0.8815 time 25.23s
Final training  1193/4999 loss: 0.8726 time 25.35s
Final training  1194/4999 loss: 0.9210 time 25.12s
Final training  1195/4999 loss: 0.8826 time 25.82s
Final training  1196/4999 loss: 0.8766 time 25.80s
Final training  1197/4999 loss: 0.8665 time 25.50s
Final training  1198/4999 loss: 0.8423 time 25.48s
Final training  1199/4999 loss: 0.8727 time 25.65s
Dice accuracy for each class:  (tensor([0.9951, 0.9211, 0.9072, 0.8938, 0.7728, 0.7019, 0.9107, 0.7129, 0.8798,
        0.8195, 0.7279, 0.6598, 0.6603, 0.5541], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1199/4999 acc [ 0.794] time 195.80s
trigger times: 1
Final training  1200/4999 loss: 0.8327 time 24.96s
Final training  1201/4999 loss: 0.8413 time 24.94s
Final training  1202/4999 loss: 0.8700 time 25.08s
Final training  1203/4999 loss: 0.8414 time 25.22s
Final training  1204/4999 loss: 0.8761 time 25.16s
Final training  1205/4999 loss: 0.8665 time 25.50s
Final training  1206/4999 loss: 0.8525 time 25.20s
Final training  1207/4999 loss: 0.8462 time 25.63s
Final training  1208/4999 loss: 0.8633 time 25.41s
Final training  1209/4999 loss: 0.8916 time 25.25s
Final training  1210/4999 loss: 0.8570 time 25.26s
Final training  1211/4999 loss: 0.8858 time 25.06s
Final training  1212/4999 loss: 0.8778 time 25.57s
Final training  1213/4999 loss: 0.8377 time 25.51s
Final training  1214/4999 loss: 0.8816 time 25.46s
Final training  1215/4999 loss: 0.8769 time 25.48s
Final training  1216/4999 loss: 0.8702 time 25.19s
Final training  1217/4999 loss: 0.8676 time 25.52s
Final training  1218/4999 loss: 0.8441 time 25.41s
Final training  1219/4999 loss: 0.8847 time 25.81s
Final training  1220/4999 loss: 0.9043 time 25.53s
Final training  1221/4999 loss: 0.8893 time 25.50s
Final training  1222/4999 loss: 0.8625 time 25.85s
Final training  1223/4999 loss: 0.8443 time 25.52s
Final training  1224/4999 loss: 0.8422 time 25.71s
Final training  1225/4999 loss: 0.8588 time 25.83s
Final training  1226/4999 loss: 0.8415 time 25.45s
Final training  1227/4999 loss: 0.8446 time 25.49s
Final training  1228/4999 loss: 0.8598 time 25.53s
Final training  1229/4999 loss: 0.8381 time 25.37s
Final training  1230/4999 loss: 0.8644 time 25.29s
Final training  1231/4999 loss: 0.8592 time 25.76s
Final training  1232/4999 loss: 0.8712 time 25.57s
Final training  1233/4999 loss: 0.8696 time 25.43s
Final training  1234/4999 loss: 0.8445 time 25.43s
Final training  1235/4999 loss: 0.8654 time 25.60s
Final training  1236/4999 loss: 0.8544 time 25.42s
Final training  1237/4999 loss: 0.9043 time 25.56s
Final training  1238/4999 loss: 0.8523 time 25.35s
Final training  1239/4999 loss: 0.8784 time 24.91s
Final training  1240/4999 loss: 0.8879 time 26.15s
Final training  1241/4999 loss: 0.8845 time 25.75s
Final training  1242/4999 loss: 0.8453 time 25.51s
Final training  1243/4999 loss: 0.8301 time 25.50s
Final training  1244/4999 loss: 0.8338 time 25.33s
Final training  1245/4999 loss: 0.8847 time 25.17s
Final training  1246/4999 loss: 0.8636 time 25.64s
Final training  1247/4999 loss: 0.8461 time 25.56s
Final training  1248/4999 loss: 0.8568 time 25.16s
Final training  1249/4999 loss: 0.8385 time 25.42s
Final training  1250/4999 loss: 0.8683 time 25.14s
Final training  1251/4999 loss: 0.8524 time 25.30s
Final training  1252/4999 loss: 0.8564 time 25.47s
Final training  1253/4999 loss: 0.8487 time 25.48s
Final training  1254/4999 loss: 0.8674 time 25.31s
Final training  1255/4999 loss: 0.8458 time 25.24s
Final training  1256/4999 loss: 0.8534 time 25.46s
Final training  1257/4999 loss: 0.8412 time 25.62s
Final training  1258/4999 loss: 0.8452 time 25.23s
Final training  1259/4999 loss: 0.8453 time 25.62s
Final training  1260/4999 loss: 0.8507 time 25.40s
Final training  1261/4999 loss: 0.8561 time 25.58s
Final training  1262/4999 loss: 0.8593 time 25.49s
Final training  1263/4999 loss: 0.8680 time 25.26s
Final training  1264/4999 loss: 0.8663 time 25.36s
Final training  1265/4999 loss: 0.8756 time 25.29s
Final training  1266/4999 loss: 0.8460 time 25.53s
Final training  1267/4999 loss: 0.8763 time 25.87s
Final training  1268/4999 loss: 0.8547 time 25.55s
Final training  1269/4999 loss: 0.8735 time 25.76s
Final training  1270/4999 loss: 0.8449 time 25.22s
Final training  1271/4999 loss: 0.8511 time 25.45s
Final training  1272/4999 loss: 0.8245 time 25.67s
Final training  1273/4999 loss: 0.8496 time 26.02s
Final training  1274/4999 loss: 0.8663 time 25.26s
Final training  1275/4999 loss: 0.8673 time 25.27s
Final training  1276/4999 loss: 0.8724 time 25.55s
Final training  1277/4999 loss: 0.8472 time 25.50s
Final training  1278/4999 loss: 0.8653 time 25.75s
Final training  1279/4999 loss: 0.8615 time 25.53s
Final training  1280/4999 loss: 0.8570 time 25.44s
Final training  1281/4999 loss: 0.8628 time 25.46s
Final training  1282/4999 loss: 0.8410 time 25.70s
Final training  1283/4999 loss: 0.8463 time 25.36s
Final training  1284/4999 loss: 0.8486 time 25.61s
Final training  1285/4999 loss: 0.8364 time 25.50s
Final training  1286/4999 loss: 0.8400 time 25.36s
Final training  1287/4999 loss: 0.8560 time 25.49s
Final training  1288/4999 loss: 0.8705 time 25.45s
Final training  1289/4999 loss: 0.8256 time 25.31s
Final training  1290/4999 loss: 0.8547 time 25.06s
Final training  1291/4999 loss: 0.8299 time 25.43s
Final training  1292/4999 loss: 0.8521 time 25.40s
Final training  1293/4999 loss: 0.8517 time 25.85s
Final training  1294/4999 loss: 0.8643 time 25.90s
Final training  1295/4999 loss: 0.8611 time 25.53s
Final training  1296/4999 loss: 0.8560 time 25.89s
Final training  1297/4999 loss: 0.8455 time 25.76s
Final training  1298/4999 loss: 0.8533 time 25.46s
Final training  1299/4999 loss: 0.8432 time 25.55s
Dice accuracy for each class:  (tensor([0.9928, 0.9320, 0.9286, 0.9218, 0.7778, 0.6359, 0.8345, 0.7120, 0.8961,
        0.8444, 0.7395, 0.7046, 0.5980, 0.5300], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1299/4999 acc [ 0.789] time 194.28s
trigger times: 2
Final training  1300/4999 loss: 0.8451 time 24.85s
Final training  1301/4999 loss: 0.8422 time 24.97s
Final training  1302/4999 loss: 0.8201 time 25.64s
Final training  1303/4999 loss: 0.8341 time 25.59s
Final training  1304/4999 loss: 0.8586 time 25.24s
Final training  1305/4999 loss: 0.8285 time 25.29s
Final training  1306/4999 loss: 0.8426 time 25.39s
Final training  1307/4999 loss: 0.8430 time 25.60s
Final training  1308/4999 loss: 0.8341 time 25.57s
Final training  1309/4999 loss: 0.8053 time 25.64s
Final training  1310/4999 loss: 0.8305 time 25.62s
Final training  1311/4999 loss: 0.8593 time 25.65s
Final training  1312/4999 loss: 0.8396 time 25.29s
Final training  1313/4999 loss: 0.8374 time 25.09s
Final training  1314/4999 loss: 0.8489 time 24.96s
Final training  1315/4999 loss: 0.8315 time 25.66s
Final training  1316/4999 loss: 0.8252 time 25.39s
Final training  1317/4999 loss: 0.8375 time 25.63s
Final training  1318/4999 loss: 0.8795 time 25.69s
Final training  1319/4999 loss: 0.8465 time 25.56s
Final training  1320/4999 loss: 0.8461 time 25.62s
Final training  1321/4999 loss: 0.8685 time 25.50s
Final training  1322/4999 loss: 0.8479 time 25.61s
Final training  1323/4999 loss: 0.8584 time 25.10s
Final training  1324/4999 loss: 0.8765 time 25.73s
Final training  1325/4999 loss: 0.8697 time 25.36s
Final training  1326/4999 loss: 0.8634 time 25.42s
Final training  1327/4999 loss: 0.8893 time 25.89s
Final training  1328/4999 loss: 0.8881 time 25.68s
Final training  1329/4999 loss: 0.8581 time 25.50s
Final training  1330/4999 loss: 0.8549 time 25.53s
Final training  1331/4999 loss: 0.8379 time 25.27s
Final training  1332/4999 loss: 0.8340 time 25.86s
Final training  1333/4999 loss: 0.8711 time 25.79s
Final training  1334/4999 loss: 0.8317 time 25.59s
Final training  1335/4999 loss: 0.8540 time 25.74s
Final training  1336/4999 loss: 0.8445 time 25.71s
Final training  1337/4999 loss: 0.8544 time 25.40s
Final training  1338/4999 loss: 0.8455 time 25.65s
Final training  1339/4999 loss: 0.8700 time 25.48s
Final training  1340/4999 loss: 0.8377 time 25.16s
Final training  1341/4999 loss: 0.8431 time 25.65s
Final training  1342/4999 loss: 0.8721 time 25.59s
Final training  1343/4999 loss: 0.8366 time 25.60s
Final training  1344/4999 loss: 0.8551 time 25.53s
Final training  1345/4999 loss: 0.8392 time 25.91s
Final training  1346/4999 loss: 0.8416 time 25.63s
Final training  1347/4999 loss: 0.8202 time 25.30s
Final training  1348/4999 loss: 0.8499 time 25.91s
Final training  1349/4999 loss: 0.8528 time 25.65s
Final training  1350/4999 loss: 0.8321 time 25.28s
Final training  1351/4999 loss: 0.8265 time 25.24s
Final training  1352/4999 loss: 0.8364 time 25.76s
Final training  1353/4999 loss: 0.8255 time 25.46s
Final training  1354/4999 loss: 0.8480 time 25.49s
Final training  1355/4999 loss: 0.8480 time 25.46s
Final training  1356/4999 loss: 0.8475 time 25.48s
Final training  1357/4999 loss: 0.8323 time 25.56s
Final training  1358/4999 loss: 0.8557 time 25.40s
Final training  1359/4999 loss: 0.8435 time 25.63s
Final training  1360/4999 loss: 0.8407 time 25.39s
Final training  1361/4999 loss: 0.8558 time 25.41s
Final training  1362/4999 loss: 0.8445 time 25.51s
Final training  1363/4999 loss: 0.8321 time 25.62s
Final training  1364/4999 loss: 0.8627 time 25.62s
Final training  1365/4999 loss: 0.8615 time 25.19s
Final training  1366/4999 loss: 0.8250 time 25.05s
Final training  1367/4999 loss: 0.8287 time 25.33s
Final training  1368/4999 loss: 0.8466 time 25.65s
Final training  1369/4999 loss: 0.8463 time 25.80s
Final training  1370/4999 loss: 0.8281 time 25.75s
Final training  1371/4999 loss: 0.8292 time 25.39s
Final training  1372/4999 loss: 0.8239 time 25.70s
Final training  1373/4999 loss: 0.8511 time 25.63s
Final training  1374/4999 loss: 0.8498 time 25.34s
Final training  1375/4999 loss: 0.8793 time 25.43s
Final training  1376/4999 loss: 0.8867 time 26.04s
Final training  1377/4999 loss: 0.8806 time 25.84s
Final training  1378/4999 loss: 0.8656 time 25.79s
Final training  1379/4999 loss: 0.8644 time 25.62s
Final training  1380/4999 loss: 0.8508 time 25.74s
Final training  1381/4999 loss: 0.8485 time 25.84s
Final training  1382/4999 loss: 0.8391 time 25.49s
Final training  1383/4999 loss: 0.8185 time 25.24s
Final training  1384/4999 loss: 0.8118 time 25.22s
Final training  1385/4999 loss: 0.8429 time 25.87s
Final training  1386/4999 loss: 0.8481 time 25.24s
Final training  1387/4999 loss: 0.8519 time 25.39s
Final training  1388/4999 loss: 0.8411 time 25.63s
Final training  1389/4999 loss: 0.8287 time 25.38s
Final training  1390/4999 loss: 0.8212 time 26.10s
Final training  1391/4999 loss: 0.8340 time 25.42s
Final training  1392/4999 loss: 0.8173 time 25.64s
Final training  1393/4999 loss: 0.8520 time 25.45s
Final training  1394/4999 loss: 0.8483 time 25.64s
Final training  1395/4999 loss: 0.8368 time 25.26s
Final training  1396/4999 loss: 0.8371 time 25.35s
Final training  1397/4999 loss: 0.8508 time 25.23s
Final training  1398/4999 loss: 0.8368 time 25.38s
Final training  1399/4999 loss: 0.8402 time 25.41s
Dice accuracy for each class:  (tensor([0.9929, 0.8601, 0.9250, 0.9282, 0.7528, 0.7059, 0.8662, 0.6657, 0.8979,
        0.8209, 0.7354, 0.7617, 0.6334, 0.5882], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1399/4999 acc [ 0.795] time 194.98s
trigger times: 3
Final training  1400/4999 loss: 0.8306 time 25.39s
Final training  1401/4999 loss: 0.8484 time 25.31s
Final training  1402/4999 loss: 0.8515 time 25.25s
Final training  1403/4999 loss: 0.8407 time 25.06s
Final training  1404/4999 loss: 0.8312 time 25.18s
Final training  1405/4999 loss: 0.8339 time 25.20s
Final training  1406/4999 loss: 0.8445 time 25.33s
Final training  1407/4999 loss: 0.8896 time 25.45s
Final training  1408/4999 loss: 0.9663 time 25.38s
Final training  1409/4999 loss: 0.9294 time 25.38s
Final training  1410/4999 loss: 0.8929 time 25.48s
Final training  1411/4999 loss: 0.8778 time 25.36s
Final training  1412/4999 loss: 0.8795 time 25.32s
Final training  1413/4999 loss: 0.8484 time 25.24s
Final training  1414/4999 loss: 0.8490 time 25.38s
Final training  1415/4999 loss: 0.8353 time 25.33s
Final training  1416/4999 loss: 0.8645 time 25.24s
Final training  1417/4999 loss: 0.8496 time 25.17s
Final training  1418/4999 loss: 0.8536 time 25.59s
Final training  1419/4999 loss: 0.8296 time 25.42s
Final training  1420/4999 loss: 0.8533 time 25.81s
Final training  1421/4999 loss: 0.8305 time 25.09s
Final training  1422/4999 loss: 0.8266 time 25.19s
Final training  1423/4999 loss: 0.8192 time 25.41s
Final training  1424/4999 loss: 0.8404 time 25.51s
Final training  1425/4999 loss: 0.8259 time 25.58s
Final training  1426/4999 loss: 0.8284 time 25.63s
Final training  1427/4999 loss: 0.8579 time 25.53s
Final training  1428/4999 loss: 0.8571 time 25.20s
Final training  1429/4999 loss: 0.8385 time 25.43s
Final training  1430/4999 loss: 0.8210 time 25.02s
Final training  1431/4999 loss: 0.8410 time 25.60s
Final training  1432/4999 loss: 0.8125 time 25.22s
Final training  1433/4999 loss: 0.8343 time 25.01s
Final training  1434/4999 loss: 0.8289 time 25.30s
Final training  1435/4999 loss: 0.8407 time 25.72s
Final training  1436/4999 loss: 0.8155 time 25.40s
Final training  1437/4999 loss: 0.8480 time 25.38s
Final training  1438/4999 loss: 0.8306 time 25.23s
Final training  1439/4999 loss: 0.8500 time 25.43s
Final training  1440/4999 loss: 0.8443 time 25.21s
Final training  1441/4999 loss: 0.8289 time 25.44s
Final training  1442/4999 loss: 0.8313 time 25.44s
Final training  1443/4999 loss: 0.8157 time 25.59s
Final training  1444/4999 loss: 0.8317 time 25.49s
Final training  1445/4999 loss: 0.8226 time 25.31s
Final training  1446/4999 loss: 0.8157 time 25.56s
Final training  1447/4999 loss: 0.8297 time 25.59s
Final training  1448/4999 loss: 0.8280 time 25.37s
Final training  1449/4999 loss: 0.8532 time 25.51s
Final training  1450/4999 loss: 0.8515 time 25.07s
Final training  1451/4999 loss: 0.8522 time 25.32s
Final training  1452/4999 loss: 0.8474 time 25.64s
Final training  1453/4999 loss: 0.8591 time 25.40s
Final training  1454/4999 loss: 0.8353 time 25.14s
Final training  1455/4999 loss: 0.8458 time 25.48s
Final training  1456/4999 loss: 0.8423 time 25.45s
Final training  1457/4999 loss: 0.8132 time 25.64s
Final training  1458/4999 loss: 0.8202 time 25.42s
Final training  1459/4999 loss: 0.8328 time 25.38s
Final training  1460/4999 loss: 0.8554 time 25.74s
Final training  1461/4999 loss: 0.8657 time 25.56s
Final training  1462/4999 loss: 0.8520 time 25.33s
Final training  1463/4999 loss: 0.8528 time 25.74s
Final training  1464/4999 loss: 0.8474 time 25.42s
Final training  1465/4999 loss: 0.8418 time 25.42s
Final training  1466/4999 loss: 0.8730 time 24.92s
Final training  1467/4999 loss: 0.8327 time 25.25s
Final training  1468/4999 loss: 0.8401 time 25.35s
Final training  1469/4999 loss: 0.8475 time 25.34s
Final training  1470/4999 loss: 0.8653 time 25.42s
Final training  1471/4999 loss: 0.8891 time 25.43s
Final training  1472/4999 loss: 0.8962 time 25.61s
Final training  1473/4999 loss: 0.8635 time 25.24s
Final training  1474/4999 loss: 0.8792 time 25.46s
Final training  1475/4999 loss: 0.8594 time 25.51s
Final training  1476/4999 loss: 0.8804 time 25.38s
Final training  1477/4999 loss: 0.8747 time 25.27s
Final training  1478/4999 loss: 0.8385 time 25.27s
Final training  1479/4999 loss: 0.8369 time 25.29s
Final training  1480/4999 loss: 0.8396 time 25.18s
Final training  1481/4999 loss: 0.8409 time 24.82s
Final training  1482/4999 loss: 0.8408 time 25.39s
Final training  1483/4999 loss: 0.8633 time 25.54s
Final training  1484/4999 loss: 0.8329 time 25.48s
Final training  1485/4999 loss: 0.8280 time 25.45s
Final training  1486/4999 loss: 0.8438 time 25.50s
Final training  1487/4999 loss: 0.8377 time 25.53s
Final training  1488/4999 loss: 0.8362 time 25.38s
Final training  1489/4999 loss: 0.8490 time 25.30s
Final training  1490/4999 loss: 0.8343 time 25.44s
Final training  1491/4999 loss: 0.8445 time 25.67s
Final training  1492/4999 loss: 0.8261 time 25.16s
Final training  1493/4999 loss: 0.8532 time 25.58s
Final training  1494/4999 loss: 0.8399 time 25.39s
Final training  1495/4999 loss: 0.8135 time 25.73s
Final training  1496/4999 loss: 0.8383 time 24.91s
Final training  1497/4999 loss: 0.8228 time 25.31s
Final training  1498/4999 loss: 0.8211 time 25.38s
Final training  1499/4999 loss: 0.8405 time 25.18s
Dice accuracy for each class:  (tensor([0.9929, 0.9062, 0.9325, 0.9274, 0.7790, 0.6553, 0.8556, 0.6617, 0.8951,
        0.8330, 0.7417, 0.7584, 0.6485, 0.6228], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1499/4999 acc [ 0.800] time 193.12s
Reset trigger time to 0
new best (0.795323 --> 0.800337). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  1500/4999 loss: 0.8440 time 25.16s
Final training  1501/4999 loss: 0.8564 time 25.34s
Final training  1502/4999 loss: 0.8358 time 25.33s
Final training  1503/4999 loss: 0.8401 time 25.34s
Final training  1504/4999 loss: 0.8920 time 25.23s
Final training  1505/4999 loss: 0.8636 time 25.10s
Final training  1506/4999 loss: 0.8602 time 25.28s
Final training  1507/4999 loss: 0.8579 time 25.38s
Final training  1508/4999 loss: 0.8613 time 24.97s
Final training  1509/4999 loss: 0.8417 time 25.27s
Final training  1510/4999 loss: 0.8496 time 25.14s
Final training  1511/4999 loss: 0.8526 time 25.25s
Final training  1512/4999 loss: 0.8425 time 25.21s
Final training  1513/4999 loss: 0.8552 time 25.59s
Final training  1514/4999 loss: 0.8400 time 25.63s
Final training  1515/4999 loss: 0.8571 time 25.46s
Final training  1516/4999 loss: 0.8444 time 25.69s
Final training  1517/4999 loss: 0.8534 time 24.84s
Final training  1518/4999 loss: 0.8365 time 25.16s
Final training  1519/4999 loss: 0.8499 time 25.44s
Final training  1520/4999 loss: 0.8473 time 25.05s
Final training  1521/4999 loss: 0.8494 time 25.25s
Final training  1522/4999 loss: 0.8507 time 24.92s
Final training  1523/4999 loss: 0.8246 time 25.51s
Final training  1524/4999 loss: 0.8085 time 25.23s
Final training  1525/4999 loss: 0.8427 time 25.21s
Final training  1526/4999 loss: 0.8074 time 24.58s
Final training  1527/4999 loss: 0.8513 time 25.57s
Final training  1528/4999 loss: 0.8341 time 25.19s
Final training  1529/4999 loss: 0.8513 time 25.51s
Final training  1530/4999 loss: 0.8181 time 25.11s
Final training  1531/4999 loss: 0.8397 time 25.06s
Final training  1532/4999 loss: 0.8287 time 25.12s
Final training  1533/4999 loss: 0.8288 time 25.09s
Final training  1534/4999 loss: 0.8378 time 25.18s
Final training  1535/4999 loss: 0.8402 time 25.52s
Final training  1536/4999 loss: 0.8302 time 25.10s
Final training  1537/4999 loss: 0.8396 time 25.50s
Final training  1538/4999 loss: 0.8422 time 25.23s
Final training  1539/4999 loss: 0.8204 time 25.40s
Final training  1540/4999 loss: 0.8169 time 25.51s
Final training  1541/4999 loss: 0.8399 time 25.09s
Final training  1542/4999 loss: 0.8330 time 25.34s
Final training  1543/4999 loss: 0.8313 time 25.68s
Final training  1544/4999 loss: 0.8172 time 25.38s
Final training  1545/4999 loss: 0.8468 time 24.99s
Final training  1546/4999 loss: 0.8118 time 25.54s
Final training  1547/4999 loss: 0.8565 time 25.28s
Final training  1548/4999 loss: 0.8675 time 25.16s
Final training  1549/4999 loss: 0.8557 time 25.49s
Final training  1550/4999 loss: 0.8728 time 24.95s
Final training  1551/4999 loss: 0.8263 time 25.25s
Final training  1552/4999 loss: 0.8380 time 25.10s
Final training  1553/4999 loss: 0.8336 time 25.13s
Final training  1554/4999 loss: 0.8415 time 25.47s
Final training  1555/4999 loss: 0.8352 time 25.42s
Final training  1556/4999 loss: 0.8297 time 25.57s
Final training  1557/4999 loss: 0.8247 time 25.01s
Final training  1558/4999 loss: 0.8441 time 25.26s
Final training  1559/4999 loss: 0.8204 time 25.23s
Final training  1560/4999 loss: 0.8292 time 25.36s
Final training  1561/4999 loss: 0.8255 time 25.38s
Final training  1562/4999 loss: 0.7937 time 25.19s
Final training  1563/4999 loss: 0.8202 time 25.34s
Final training  1564/4999 loss: 0.8165 time 25.57s
Final training  1565/4999 loss: 0.8265 time 25.40s
Final training  1566/4999 loss: 0.8403 time 25.35s
Final training  1567/4999 loss: 0.8314 time 25.25s
Final training  1568/4999 loss: 0.8124 time 25.37s
Final training  1569/4999 loss: 0.8414 time 25.42s
Final training  1570/4999 loss: 0.8398 time 25.61s
Final training  1571/4999 loss: 0.8554 time 25.43s
Final training  1572/4999 loss: 0.8329 time 25.33s
Final training  1573/4999 loss: 0.8648 time 25.25s
Final training  1574/4999 loss: 0.8949 time 24.83s
Final training  1575/4999 loss: 0.8697 time 25.34s
Final training  1576/4999 loss: 0.8477 time 25.61s
Final training  1577/4999 loss: 0.8578 time 25.22s
Final training  1578/4999 loss: 0.8329 time 25.06s
Final training  1579/4999 loss: 0.8483 time 25.04s
Final training  1580/4999 loss: 0.8451 time 25.03s
Final training  1581/4999 loss: 0.8317 time 25.57s
Final training  1582/4999 loss: 0.8083 time 25.24s
Final training  1583/4999 loss: 0.8473 time 25.39s
Final training  1584/4999 loss: 0.8350 time 25.37s
Final training  1585/4999 loss: 0.8713 time 25.20s
Final training  1586/4999 loss: 0.8539 time 24.92s
Final training  1587/4999 loss: 0.8399 time 25.17s
Final training  1588/4999 loss: 0.8751 time 25.42s
Final training  1589/4999 loss: 0.8319 time 25.34s
Final training  1590/4999 loss: 0.8397 time 25.34s
Final training  1591/4999 loss: 0.8372 time 25.39s
Final training  1592/4999 loss: 0.8559 time 25.36s
Final training  1593/4999 loss: 0.8315 time 25.23s
Final training  1594/4999 loss: 0.8333 time 25.29s
Final training  1595/4999 loss: 0.8414 time 25.07s
Final training  1596/4999 loss: 0.8217 time 25.06s
Final training  1597/4999 loss: 0.8376 time 25.14s
Final training  1598/4999 loss: 0.8287 time 25.39s
Final training  1599/4999 loss: 0.8331 time 25.18s
Dice accuracy for each class:  (tensor([0.9945, 0.7995, 0.9197, 0.9258, 0.7888, 0.7073, 0.9026, 0.7491, 0.8898,
        0.8378, 0.7379, 0.7738, 0.6438, 0.6045], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1599/4999 acc [ 0.805] time 194.81s
Reset trigger time to 0
new best (0.800337 --> 0.804909). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  1600/4999 loss: 0.8391 time 25.03s
Final training  1601/4999 loss: 0.8397 time 25.78s
Final training  1602/4999 loss: 0.8146 time 25.44s
Final training  1603/4999 loss: 0.8239 time 25.53s
Final training  1604/4999 loss: 0.8307 time 25.13s
Final training  1605/4999 loss: 0.8442 time 25.58s
Final training  1606/4999 loss: 0.8244 time 25.34s
Final training  1607/4999 loss: 0.8156 time 25.74s
Final training  1608/4999 loss: 0.8123 time 25.21s
Final training  1609/4999 loss: 0.8521 time 25.70s
Final training  1610/4999 loss: 0.8190 time 25.49s
Final training  1611/4999 loss: 0.8351 time 25.61s
Final training  1612/4999 loss: 0.8372 time 25.41s
Final training  1613/4999 loss: 0.8281 time 25.29s
Final training  1614/4999 loss: 0.8429 time 25.41s
Final training  1615/4999 loss: 0.8481 time 25.53s
Final training  1616/4999 loss: 0.8851 time 25.46s
Final training  1617/4999 loss: 0.8596 time 25.59s
Final training  1618/4999 loss: 0.8476 time 26.22s
Final training  1619/4999 loss: 0.8517 time 25.55s
Final training  1620/4999 loss: 0.8253 time 25.50s
Final training  1621/4999 loss: 0.8308 time 25.57s
Final training  1622/4999 loss: 0.8324 time 25.81s
Final training  1623/4999 loss: 0.8403 time 25.36s
Final training  1624/4999 loss: 0.8462 time 25.65s
Final training  1625/4999 loss: 0.8462 time 25.25s
Final training  1626/4999 loss: 0.7964 time 25.29s
Final training  1627/4999 loss: 0.8263 time 25.60s
Final training  1628/4999 loss: 0.8210 time 25.66s
Final training  1629/4999 loss: 0.8079 time 25.76s
Final training  1630/4999 loss: 0.8213 time 25.56s
Final training  1631/4999 loss: 0.8401 time 25.61s
Final training  1632/4999 loss: 0.8180 time 25.63s
Final training  1633/4999 loss: 0.8270 time 25.45s
Final training  1634/4999 loss: 0.8546 time 25.74s
Final training  1635/4999 loss: 0.8272 time 25.22s
Final training  1636/4999 loss: 0.8364 time 25.34s
Final training  1637/4999 loss: 0.8314 time 25.49s
Final training  1638/4999 loss: 0.8248 time 25.52s
Final training  1639/4999 loss: 0.8424 time 25.34s
Final training  1640/4999 loss: 0.8672 time 25.44s
Final training  1641/4999 loss: 0.8268 time 25.65s
Final training  1642/4999 loss: 0.8447 time 25.50s
Final training  1643/4999 loss: 0.8373 time 25.57s
Final training  1644/4999 loss: 0.8326 time 25.43s
Final training  1645/4999 loss: 0.8517 time 25.48s
Final training  1646/4999 loss: 0.8495 time 25.44s
Final training  1647/4999 loss: 0.8217 time 25.51s
Final training  1648/4999 loss: 0.8276 time 25.68s
Final training  1649/4999 loss: 0.8373 time 25.43s
Final training  1650/4999 loss: 0.8609 time 25.71s
Final training  1651/4999 loss: 0.8348 time 25.11s
Final training  1652/4999 loss: 0.8562 time 25.16s
Final training  1653/4999 loss: 0.8131 time 25.48s
Final training  1654/4999 loss: 0.8275 time 25.25s
Final training  1655/4999 loss: 0.8154 time 25.43s
Final training  1656/4999 loss: 0.8320 time 25.21s
Final training  1657/4999 loss: 0.8182 time 25.60s
Final training  1658/4999 loss: 0.8139 time 25.11s
Final training  1659/4999 loss: 0.8232 time 25.52s
Final training  1660/4999 loss: 0.8328 time 25.61s
Final training  1661/4999 loss: 0.8422 time 25.57s
Final training  1662/4999 loss: 0.8330 time 25.64s
Final training  1663/4999 loss: 0.8388 time 25.50s
Final training  1664/4999 loss: 0.8346 time 25.53s
Final training  1665/4999 loss: 0.8398 time 25.57s
Final training  1666/4999 loss: 0.8080 time 25.50s
Final training  1667/4999 loss: 0.8237 time 25.55s
Final training  1668/4999 loss: 0.8353 time 25.29s
Final training  1669/4999 loss: 0.8419 time 25.80s
Final training  1670/4999 loss: 0.8622 time 25.22s
Final training  1671/4999 loss: 0.8418 time 25.66s
Final training  1672/4999 loss: 0.8584 time 25.51s
Final training  1673/4999 loss: 0.8328 time 25.38s
Final training  1674/4999 loss: 0.8380 time 25.61s
Final training  1675/4999 loss: 0.8297 time 25.25s
Final training  1676/4999 loss: 0.8362 time 25.28s
Final training  1677/4999 loss: 0.8347 time 25.01s
Final training  1678/4999 loss: 0.8039 time 25.51s
Final training  1679/4999 loss: 0.8273 time 25.54s
Final training  1680/4999 loss: 0.8137 time 25.64s
Final training  1681/4999 loss: 0.8124 time 25.51s
Final training  1682/4999 loss: 0.8252 time 25.49s
Final training  1683/4999 loss: 0.8357 time 25.23s
Final training  1684/4999 loss: 0.8361 time 25.48s
Final training  1685/4999 loss: 0.8305 time 25.34s
Final training  1686/4999 loss: 0.8268 time 25.39s
Final training  1687/4999 loss: 0.8387 time 25.29s
Final training  1688/4999 loss: 0.8380 time 25.36s
Final training  1689/4999 loss: 0.8334 time 25.45s
Final training  1690/4999 loss: 0.8193 time 25.22s
Final training  1691/4999 loss: 0.8676 time 25.45s
Final training  1692/4999 loss: 0.8271 time 25.56s
Final training  1693/4999 loss: 0.8274 time 25.52s
Final training  1694/4999 loss: 0.8180 time 25.54s
Final training  1695/4999 loss: 0.8474 time 25.30s
Final training  1696/4999 loss: 0.8387 time 24.70s
Final training  1697/4999 loss: 0.8244 time 25.20s
Final training  1698/4999 loss: 0.8161 time 25.10s
Final training  1699/4999 loss: 0.8346 time 25.09s
Dice accuracy for each class:  (tensor([0.9958, 0.9308, 0.9325, 0.9414, 0.7670, 0.7690, 0.9138, 0.7798, 0.8979,
        0.8456, 0.7503, 0.7820, 0.6734, 0.6394], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1699/4999 acc [ 0.830] time 192.27s
Reset trigger time to 0
new best (0.804909 --> 0.830441). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  1700/4999 loss: 0.8399 time 25.26s
Final training  1701/4999 loss: 0.8087 time 25.41s
Final training  1702/4999 loss: 0.8199 time 25.29s
Final training  1703/4999 loss: 0.8123 time 25.19s
Final training  1704/4999 loss: 0.8262 time 25.34s
Final training  1705/4999 loss: 0.8505 time 25.45s
Final training  1706/4999 loss: 0.8312 time 25.57s
Final training  1707/4999 loss: 0.8105 time 25.11s
Final training  1708/4999 loss: 0.8439 time 25.68s
Final training  1709/4999 loss: 0.8398 time 25.52s
Final training  1710/4999 loss: 0.8231 time 25.56s
Final training  1711/4999 loss: 0.8284 time 25.62s
Final training  1712/4999 loss: 0.8238 time 25.87s
Final training  1713/4999 loss: 0.8435 time 25.59s
Final training  1714/4999 loss: 0.8408 time 25.26s
Final training  1715/4999 loss: 0.8167 time 25.58s
Final training  1716/4999 loss: 0.8322 time 25.59s
Final training  1717/4999 loss: 0.8259 time 25.32s
Final training  1718/4999 loss: 0.8260 time 25.64s
Final training  1719/4999 loss: 0.8356 time 25.34s
Final training  1720/4999 loss: 0.8373 time 25.56s
Final training  1721/4999 loss: 0.8246 time 25.61s
Final training  1722/4999 loss: 0.8313 time 25.43s
Final training  1723/4999 loss: 0.8157 time 25.81s
Final training  1724/4999 loss: 0.8236 time 25.69s
Final training  1725/4999 loss: 0.8437 time 25.89s
Final training  1726/4999 loss: 0.8427 time 25.88s
Final training  1727/4999 loss: 0.8502 time 25.86s
Final training  1728/4999 loss: 0.8288 time 25.48s
Final training  1729/4999 loss: 0.8083 time 25.54s
Final training  1730/4999 loss: 0.8399 time 25.12s
Final training  1731/4999 loss: 0.8233 time 25.69s
Final training  1732/4999 loss: 0.8214 time 25.68s
Final training  1733/4999 loss: 0.8279 time 25.42s
Final training  1734/4999 loss: 0.8130 time 25.29s
Final training  1735/4999 loss: 0.8358 time 25.54s
Final training  1736/4999 loss: 0.7951 time 25.55s
Final training  1737/4999 loss: 0.8185 time 25.95s
Final training  1738/4999 loss: 0.8155 time 25.43s
Final training  1739/4999 loss: 0.8240 time 25.25s
Final training  1740/4999 loss: 0.8238 time 25.29s
Final training  1741/4999 loss: 0.8002 time 25.33s
Final training  1742/4999 loss: 0.8021 time 25.58s
Final training  1743/4999 loss: 0.8151 time 25.39s
Final training  1744/4999 loss: 0.8191 time 25.38s
Final training  1745/4999 loss: 0.8382 time 25.25s
Final training  1746/4999 loss: 0.8170 time 25.20s
Final training  1747/4999 loss: 0.8334 time 25.19s
Final training  1748/4999 loss: 0.8420 time 25.64s
Final training  1749/4999 loss: 0.8003 time 25.80s
Final training  1750/4999 loss: 0.8385 time 25.41s
Final training  1751/4999 loss: 0.8417 time 25.27s
Final training  1752/4999 loss: 0.8415 time 25.14s
Final training  1753/4999 loss: 0.8491 time 25.30s
Final training  1754/4999 loss: 0.8176 time 25.63s
Final training  1755/4999 loss: 0.8477 time 25.56s
Final training  1756/4999 loss: 0.8315 time 25.43s
Final training  1757/4999 loss: 0.8319 time 24.94s
Final training  1758/4999 loss: 0.8316 time 25.46s
Final training  1759/4999 loss: 0.8269 time 25.42s
Final training  1760/4999 loss: 0.8424 time 25.29s
Final training  1761/4999 loss: 0.8223 time 25.36s
Final training  1762/4999 loss: 0.8107 time 25.41s
Final training  1763/4999 loss: 0.8441 time 25.35s
Final training  1764/4999 loss: 0.8295 time 25.28s
Final training  1765/4999 loss: 0.8436 time 25.39s
Final training  1766/4999 loss: 0.8427 time 25.56s
Final training  1767/4999 loss: 0.8369 time 25.45s
Final training  1768/4999 loss: 0.7875 time 25.30s
Final training  1769/4999 loss: 0.8307 time 25.13s
Final training  1770/4999 loss: 0.8342 time 25.46s
Final training  1771/4999 loss: 0.8554 time 25.19s
Final training  1772/4999 loss: 0.8319 time 25.40s
Final training  1773/4999 loss: 0.8188 time 25.64s
Final training  1774/4999 loss: 0.7956 time 25.49s
Final training  1775/4999 loss: 0.8156 time 25.71s
Final training  1776/4999 loss: 0.8340 time 25.23s
Final training  1777/4999 loss: 0.8141 time 25.70s
Final training  1778/4999 loss: 0.8511 time 25.68s
Final training  1779/4999 loss: 0.8376 time 25.68s
Final training  1780/4999 loss: 0.8265 time 25.44s
Final training  1781/4999 loss: 0.8205 time 25.62s
Final training  1782/4999 loss: 0.8118 time 25.69s
Final training  1783/4999 loss: 0.8173 time 25.30s
Final training  1784/4999 loss: 0.8143 time 25.64s
Final training  1785/4999 loss: 0.8430 time 25.43s
Final training  1786/4999 loss: 0.8094 time 25.66s
Final training  1787/4999 loss: 0.8202 time 25.02s
Final training  1788/4999 loss: 0.8120 time 25.09s
Final training  1789/4999 loss: 0.8337 time 25.24s
Final training  1790/4999 loss: 0.8332 time 25.22s
Final training  1791/4999 loss: 0.8272 time 25.02s
Final training  1792/4999 loss: 0.8341 time 25.17s
Final training  1793/4999 loss: 0.8147 time 25.18s
Final training  1794/4999 loss: 0.8115 time 24.94s
Final training  1795/4999 loss: 0.8418 time 25.64s
Final training  1796/4999 loss: 0.8356 time 24.74s
Final training  1797/4999 loss: 0.7977 time 25.05s
Final training  1798/4999 loss: 0.8310 time 25.18s
Final training  1799/4999 loss: 0.8083 time 24.50s
Dice accuracy for each class:  (tensor([0.9964, 0.9109, 0.9322, 0.9356, 0.7731, 0.6920, 0.9621, 0.7127, 0.8878,
        0.8358, 0.7349, 0.7388, 0.6754, 0.6424], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1799/4999 acc [ 0.817] time 194.08s
trigger times: 1
Final training  1800/4999 loss: 0.8224 time 25.33s
Final training  1801/4999 loss: 0.8347 time 25.20s
Final training  1802/4999 loss: 0.8106 time 25.25s
Final training  1803/4999 loss: 0.8320 time 25.43s
Final training  1804/4999 loss: 0.8295 time 25.36s
Final training  1805/4999 loss: 0.7867 time 25.41s
Final training  1806/4999 loss: 0.8140 time 25.17s
Final training  1807/4999 loss: 0.8255 time 25.55s
Final training  1808/4999 loss: 0.8056 time 25.82s
Final training  1809/4999 loss: 0.7981 time 24.90s
Final training  1810/4999 loss: 0.8122 time 25.45s
Final training  1811/4999 loss: 0.8209 time 25.30s
Final training  1812/4999 loss: 0.8258 time 25.35s
Final training  1813/4999 loss: 0.8334 time 25.29s
Final training  1814/4999 loss: 0.8154 time 25.33s
Final training  1815/4999 loss: 0.8444 time 25.38s
Final training  1816/4999 loss: 0.8238 time 25.48s
Final training  1817/4999 loss: 0.8191 time 25.51s
Final training  1818/4999 loss: 0.8165 time 25.20s
Final training  1819/4999 loss: 0.8401 time 25.57s
Final training  1820/4999 loss: 0.8280 time 25.42s
Final training  1821/4999 loss: 0.8457 time 25.37s
Final training  1822/4999 loss: 0.8207 time 25.32s
Final training  1823/4999 loss: 0.8248 time 25.56s
Final training  1824/4999 loss: 0.8077 time 25.22s
Final training  1825/4999 loss: 0.8053 time 25.41s
Final training  1826/4999 loss: 0.8247 time 25.49s
Final training  1827/4999 loss: 0.8375 time 25.57s
Final training  1828/4999 loss: 0.8494 time 25.11s
Final training  1829/4999 loss: 0.8289 time 25.67s
Final training  1830/4999 loss: 0.8302 time 25.53s
Final training  1831/4999 loss: 0.8297 time 25.53s
Final training  1832/4999 loss: 0.8421 time 25.33s
Final training  1833/4999 loss: 0.8176 time 25.50s
Final training  1834/4999 loss: 0.8139 time 25.46s
Final training  1835/4999 loss: 0.8143 time 25.47s
Final training  1836/4999 loss: 0.8137 time 25.42s
Final training  1837/4999 loss: 0.8274 time 25.27s
Final training  1838/4999 loss: 0.8390 time 25.39s
Final training  1839/4999 loss: 0.8294 time 25.45s
Final training  1840/4999 loss: 0.8179 time 25.70s
Final training  1841/4999 loss: 0.8146 time 25.35s
Final training  1842/4999 loss: 0.8337 time 25.51s
Final training  1843/4999 loss: 0.7894 time 25.39s
Final training  1844/4999 loss: 0.8266 time 25.23s
Final training  1845/4999 loss: 0.8323 time 25.32s
Final training  1846/4999 loss: 0.8089 time 25.24s
Final training  1847/4999 loss: 0.8308 time 25.53s
Final training  1848/4999 loss: 0.8406 time 25.17s
Final training  1849/4999 loss: 0.8407 time 25.13s
Final training  1850/4999 loss: 0.8136 time 25.38s
Final training  1851/4999 loss: 0.8234 time 24.82s
Final training  1852/4999 loss: 0.8399 time 25.36s
Final training  1853/4999 loss: 0.8369 time 25.38s
Final training  1854/4999 loss: 0.8129 time 25.08s
Final training  1855/4999 loss: 0.8096 time 25.38s
Final training  1856/4999 loss: 0.8385 time 25.19s
Final training  1857/4999 loss: 0.8342 time 25.48s
Final training  1858/4999 loss: 0.8384 time 25.49s
Final training  1859/4999 loss: 0.8040 time 25.79s
Final training  1860/4999 loss: 0.8273 time 25.52s
Final training  1861/4999 loss: 0.8219 time 25.60s
Final training  1862/4999 loss: 0.8135 time 25.65s
Final training  1863/4999 loss: 0.8009 time 25.40s
Final training  1864/4999 loss: 0.7913 time 25.32s
Final training  1865/4999 loss: 0.7970 time 25.24s
Final training  1866/4999 loss: 0.8195 time 25.53s
Final training  1867/4999 loss: 0.8126 time 25.32s
Final training  1868/4999 loss: 0.8124 time 25.30s
Final training  1869/4999 loss: 0.8155 time 25.55s
Final training  1870/4999 loss: 0.8204 time 25.30s
Final training  1871/4999 loss: 0.7894 time 25.63s
Final training  1872/4999 loss: 0.8277 time 25.41s
Final training  1873/4999 loss: 0.8141 time 25.26s
Final training  1874/4999 loss: 0.8359 time 25.42s
Final training  1875/4999 loss: 0.8024 time 25.18s
Final training  1876/4999 loss: 0.8269 time 25.05s
Final training  1877/4999 loss: 0.8338 time 24.88s
Final training  1878/4999 loss: 0.8385 time 25.13s
Final training  1879/4999 loss: 0.7962 time 25.30s
Final training  1880/4999 loss: 0.8278 time 25.40s
Final training  1881/4999 loss: 0.8323 time 25.44s
Final training  1882/4999 loss: 0.8094 time 25.26s
Final training  1883/4999 loss: 0.8139 time 25.47s
Final training  1884/4999 loss: 0.8305 time 25.54s
Final training  1885/4999 loss: 0.8347 time 25.48s
Final training  1886/4999 loss: 0.8186 time 25.40s
Final training  1887/4999 loss: 0.8005 time 25.43s
Final training  1888/4999 loss: 0.8322 time 25.32s
Final training  1889/4999 loss: 0.8233 time 25.37s
Final training  1890/4999 loss: 0.8255 time 25.34s
Final training  1891/4999 loss: 0.8449 time 25.44s
Final training  1892/4999 loss: 0.9728 time 25.49s
Final training  1893/4999 loss: 0.8697 time 25.57s
Final training  1894/4999 loss: 0.8253 time 25.39s
Final training  1895/4999 loss: 0.8447 time 25.26s
Final training  1896/4999 loss: 0.8195 time 25.46s
Final training  1897/4999 loss: 0.8210 time 25.18s
Final training  1898/4999 loss: 0.8253 time 24.97s
Final training  1899/4999 loss: 0.8469 time 25.14s
Dice accuracy for each class:  (tensor([0.9942, 0.9037, 0.8534, 0.9263, 0.7823, 0.7074, 0.8664, 0.7775, 0.9027,
        0.8248, 0.7376, 0.7804, 0.6654, 0.6002], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1899/4999 acc [ 0.809] time 192.87s
trigger times: 2
Final training  1900/4999 loss: 0.8382 time 25.37s
Final training  1901/4999 loss: 0.8391 time 25.74s
Final training  1902/4999 loss: 0.8199 time 25.29s
Final training  1903/4999 loss: 0.8220 time 25.73s
Final training  1904/4999 loss: 0.8145 time 25.12s
Final training  1905/4999 loss: 0.8016 time 25.40s
Final training  1906/4999 loss: 0.8085 time 25.02s
Final training  1907/4999 loss: 0.8071 time 25.29s
Final training  1908/4999 loss: 0.7968 time 25.33s
Final training  1909/4999 loss: 0.8202 time 24.81s
Final training  1910/4999 loss: 0.8068 time 25.19s
Final training  1911/4999 loss: 0.8207 time 25.40s
Final training  1912/4999 loss: 0.7991 time 25.17s
Final training  1913/4999 loss: 0.8056 time 25.50s
Final training  1914/4999 loss: 0.7847 time 25.40s
Final training  1915/4999 loss: 0.8355 time 25.48s
Final training  1916/4999 loss: 0.7948 time 25.31s
Final training  1917/4999 loss: 0.8270 time 25.30s
Final training  1918/4999 loss: 0.8115 time 25.43s
Final training  1919/4999 loss: 0.8144 time 25.18s
Final training  1920/4999 loss: 0.8447 time 25.37s
Final training  1921/4999 loss: 0.8260 time 25.52s
Final training  1922/4999 loss: 0.8283 time 24.91s
Final training  1923/4999 loss: 0.8126 time 25.09s
Final training  1924/4999 loss: 0.8055 time 25.41s
Final training  1925/4999 loss: 0.8290 time 25.63s
Final training  1926/4999 loss: 0.8273 time 25.55s
Final training  1927/4999 loss: 0.8339 time 25.00s
Final training  1928/4999 loss: 0.8380 time 25.18s
Final training  1929/4999 loss: 0.8207 time 25.31s
Final training  1930/4999 loss: 0.7873 time 25.33s
Final training  1931/4999 loss: 0.8433 time 25.43s
Final training  1932/4999 loss: 0.8356 time 25.26s
Final training  1933/4999 loss: 0.8349 time 25.25s
Final training  1934/4999 loss: 0.8330 time 25.77s
Final training  1935/4999 loss: 0.8248 time 25.61s
Final training  1936/4999 loss: 0.8025 time 25.46s
Final training  1937/4999 loss: 0.8179 time 25.24s
Final training  1938/4999 loss: 0.8393 time 25.37s
Final training  1939/4999 loss: 0.8253 time 25.17s
Final training  1940/4999 loss: 0.8356 time 25.54s
Final training  1941/4999 loss: 0.8111 time 25.42s
Final training  1942/4999 loss: 0.8159 time 25.53s
Final training  1943/4999 loss: 0.8156 time 25.78s
Final training  1944/4999 loss: 0.8337 time 25.54s
Final training  1945/4999 loss: 0.8212 time 25.44s
Final training  1946/4999 loss: 0.8113 time 25.12s
Final training  1947/4999 loss: 0.8199 time 25.21s
Final training  1948/4999 loss: 0.8469 time 24.74s
Final training  1949/4999 loss: 0.8295 time 25.44s
Final training  1950/4999 loss: 0.8134 time 25.34s
Final training  1951/4999 loss: 0.8368 time 25.31s
Final training  1952/4999 loss: 0.8043 time 25.34s
Final training  1953/4999 loss: 0.8365 time 25.61s
Final training  1954/4999 loss: 0.8329 time 25.38s
Final training  1955/4999 loss: 0.8247 time 25.63s
Final training  1956/4999 loss: 0.8229 time 25.77s
Final training  1957/4999 loss: 0.8168 time 25.75s
Final training  1958/4999 loss: 0.8143 time 25.67s
Final training  1959/4999 loss: 0.8102 time 25.61s
Final training  1960/4999 loss: 0.8210 time 25.47s
Final training  1961/4999 loss: 0.8076 time 25.28s
Final training  1962/4999 loss: 0.8269 time 25.60s
Final training  1963/4999 loss: 0.8129 time 25.16s
Final training  1964/4999 loss: 0.8192 time 25.28s
Final training  1965/4999 loss: 0.8273 time 25.56s
Final training  1966/4999 loss: 0.8238 time 25.68s
Final training  1967/4999 loss: 0.7869 time 25.40s
Final training  1968/4999 loss: 0.8186 time 25.93s
Final training  1969/4999 loss: 0.8200 time 25.24s
Final training  1970/4999 loss: 0.8047 time 25.14s
Final training  1971/4999 loss: 0.8085 time 25.17s
Final training  1972/4999 loss: 0.8416 time 25.18s
Final training  1973/4999 loss: 0.8803 time 25.28s
Final training  1974/4999 loss: 0.8305 time 24.79s
Final training  1975/4999 loss: 0.8375 time 25.14s
Final training  1976/4999 loss: 0.8395 time 25.15s
Final training  1977/4999 loss: 0.8240 time 25.26s
Final training  1978/4999 loss: 0.8289 time 25.07s
Final training  1979/4999 loss: 0.8060 time 25.33s
Final training  1980/4999 loss: 0.8024 time 25.34s
Final training  1981/4999 loss: 0.8080 time 25.43s
Final training  1982/4999 loss: 0.8122 time 25.06s
Final training  1983/4999 loss: 0.8056 time 25.41s
Final training  1984/4999 loss: 0.8023 time 25.39s
Final training  1985/4999 loss: 0.8423 time 25.29s
Final training  1986/4999 loss: 0.8189 time 25.29s
Final training  1987/4999 loss: 0.8255 time 25.22s
Final training  1988/4999 loss: 0.8997 time 25.33s
Final training  1989/4999 loss: 0.8558 time 25.51s
Final training  1990/4999 loss: 0.8169 time 25.24s
Final training  1991/4999 loss: 0.8299 time 24.98s
Final training  1992/4999 loss: 0.8130 time 25.32s
Final training  1993/4999 loss: 0.8393 time 25.37s
Final training  1994/4999 loss: 0.8240 time 25.35s
Final training  1995/4999 loss: 0.8172 time 25.05s
Final training  1996/4999 loss: 0.8272 time 24.69s
Final training  1997/4999 loss: 0.8293 time 25.22s
Final training  1998/4999 loss: 0.8161 time 25.45s
Final training  1999/4999 loss: 0.8271 time 25.40s
Dice accuracy for each class:  (tensor([0.9894, 0.9523, 0.9294, 0.9276, 0.7850, 0.7110, 0.7679, 0.7809, 0.9042,
        0.8320, 0.7441, 0.8037, 0.6796, 0.6594], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1999/4999 acc [ 0.819] time 194.38s
trigger times: 3
Final training  2000/4999 loss: 0.8189 time 25.37s
Final training  2001/4999 loss: 0.8411 time 25.39s
Final training  2002/4999 loss: 0.8509 time 25.56s
Final training  2003/4999 loss: 0.8412 time 25.43s
Final training  2004/4999 loss: 0.8211 time 25.53s
Final training  2005/4999 loss: 0.7999 time 25.40s
Final training  2006/4999 loss: 0.8359 time 25.38s
Final training  2007/4999 loss: 0.8029 time 25.72s
Final training  2008/4999 loss: 0.8227 time 25.56s
Final training  2009/4999 loss: 0.8084 time 25.55s
Final training  2010/4999 loss: 0.8095 time 25.41s
Final training  2011/4999 loss: 0.8014 time 25.17s
Final training  2012/4999 loss: 0.7982 time 25.15s
Final training  2013/4999 loss: 0.8444 time 25.32s
Final training  2014/4999 loss: 0.8134 time 25.12s
Final training  2015/4999 loss: 0.8274 time 25.17s
Final training  2016/4999 loss: 0.8213 time 25.63s
Final training  2017/4999 loss: 0.8381 time 25.37s
Final training  2018/4999 loss: 0.8093 time 25.22s
Final training  2019/4999 loss: 0.7989 time 25.21s
Final training  2020/4999 loss: 0.8260 time 25.19s
Final training  2021/4999 loss: 0.8102 time 25.14s
Final training  2022/4999 loss: 0.7979 time 25.14s
Final training  2023/4999 loss: 0.8424 time 25.56s
Final training  2024/4999 loss: 0.8206 time 25.55s
Final training  2025/4999 loss: 0.8051 time 25.31s
Final training  2026/4999 loss: 0.8181 time 24.99s
Final training  2027/4999 loss: 0.8257 time 25.06s
Final training  2028/4999 loss: 0.8079 time 25.08s
Final training  2029/4999 loss: 0.7892 time 24.98s
Final training  2030/4999 loss: 0.8109 time 25.36s
Final training  2031/4999 loss: 0.8033 time 25.37s
Final training  2032/4999 loss: 0.7997 time 24.63s
Final training  2033/4999 loss: 0.8183 time 25.16s
Final training  2034/4999 loss: 0.8134 time 25.01s
Final training  2035/4999 loss: 0.8410 time 24.95s
Final training  2036/4999 loss: 0.8337 time 24.97s
Final training  2037/4999 loss: 0.8113 time 25.52s
Final training  2038/4999 loss: 0.8098 time 25.15s
Final training  2039/4999 loss: 0.8114 time 25.55s
Final training  2040/4999 loss: 0.7857 time 25.45s
Final training  2041/4999 loss: 0.8160 time 25.22s
Final training  2042/4999 loss: 0.8360 time 25.28s
Final training  2043/4999 loss: 0.8142 time 24.90s
Final training  2044/4999 loss: 0.7981 time 25.34s
Final training  2045/4999 loss: 0.8150 time 25.31s
Final training  2046/4999 loss: 0.8260 time 25.38s
Final training  2047/4999 loss: 0.7952 time 25.14s
Final training  2048/4999 loss: 0.8145 time 25.13s
Final training  2049/4999 loss: 0.8273 time 25.41s
Final training  2050/4999 loss: 0.8099 time 25.18s
Final training  2051/4999 loss: 0.7974 time 25.46s
Final training  2052/4999 loss: 0.7991 time 25.32s
Final training  2053/4999 loss: 0.8102 time 25.20s
Final training  2054/4999 loss: 0.8202 time 25.57s
Final training  2055/4999 loss: 0.8112 time 25.53s
Final training  2056/4999 loss: 0.8170 time 25.57s
Final training  2057/4999 loss: 0.8060 time 25.19s
Final training  2058/4999 loss: 0.8233 time 25.26s
Final training  2059/4999 loss: 0.8343 time 25.42s
Final training  2060/4999 loss: 0.8226 time 25.56s
Final training  2061/4999 loss: 0.8230 time 25.53s
Final training  2062/4999 loss: 0.8059 time 26.04s
Final training  2063/4999 loss: 0.8266 time 25.40s
Final training  2064/4999 loss: 0.7921 time 24.90s
Final training  2065/4999 loss: 0.8045 time 25.28s
Final training  2066/4999 loss: 0.8215 time 25.73s
Final training  2067/4999 loss: 0.8494 time 25.58s
Final training  2068/4999 loss: 0.8024 time 25.61s
Final training  2069/4999 loss: 0.8104 time 25.57s
Final training  2070/4999 loss: 0.8081 time 25.70s
Final training  2071/4999 loss: 0.8037 time 25.45s
Final training  2072/4999 loss: 0.8089 time 25.38s
Final training  2073/4999 loss: 0.8094 time 25.79s
Final training  2074/4999 loss: 0.7948 time 25.65s
Final training  2075/4999 loss: 0.8097 time 25.44s
Final training  2076/4999 loss: 0.8067 time 25.44s
Final training  2077/4999 loss: 0.8090 time 25.41s
Final training  2078/4999 loss: 0.8007 time 25.54s
Final training  2079/4999 loss: 0.8068 time 25.49s
Final training  2080/4999 loss: 0.7853 time 25.60s
Final training  2081/4999 loss: 0.8055 time 25.45s
Final training  2082/4999 loss: 0.8309 time 25.46s
Final training  2083/4999 loss: 0.8159 time 25.45s
Final training  2084/4999 loss: 0.8224 time 25.42s
Final training  2085/4999 loss: 0.8315 time 25.58s
Final training  2086/4999 loss: 0.8190 time 25.73s
Final training  2087/4999 loss: 0.7847 time 25.28s
Final training  2088/4999 loss: 0.8054 time 25.32s
Final training  2089/4999 loss: 0.7992 time 25.22s
Final training  2090/4999 loss: 0.8175 time 25.35s
Final training  2091/4999 loss: 0.8006 time 25.07s
Final training  2092/4999 loss: 0.8242 time 25.11s
Final training  2093/4999 loss: 0.7910 time 25.03s
Final training  2094/4999 loss: 0.8047 time 24.86s
Final training  2095/4999 loss: 0.8145 time 25.32s
Final training  2096/4999 loss: 0.8102 time 25.60s
Final training  2097/4999 loss: 0.7991 time 25.35s
Final training  2098/4999 loss: 0.8202 time 25.39s
Final training  2099/4999 loss: 0.7914 time 25.40s
Dice accuracy for each class:  (tensor([0.9957, 0.8689, 0.9367, 0.9309, 0.7025, 0.6808, 0.9239, 0.7797, 0.8978,
        0.8377, 0.7383, 0.7643, 0.6759, 0.6175], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2099/4999 acc [ 0.812] time 193.37s
trigger times: 4
Final training  2100/4999 loss: 0.8011 time 25.21s
Final training  2101/4999 loss: 0.8015 time 25.37s
Final training  2102/4999 loss: 0.8002 time 25.50s
Final training  2103/4999 loss: 0.8096 time 25.54s
Final training  2104/4999 loss: 0.8145 time 25.65s
Final training  2105/4999 loss: 0.8070 time 25.60s
Final training  2106/4999 loss: 0.8196 time 25.70s
Final training  2107/4999 loss: 0.8134 time 25.61s
Final training  2108/4999 loss: 0.8169 time 25.38s
Final training  2109/4999 loss: 0.8105 time 25.57s
Final training  2110/4999 loss: 0.8167 time 25.35s
Final training  2111/4999 loss: 0.8219 time 25.59s
Final training  2112/4999 loss: 0.8411 time 25.16s
Final training  2113/4999 loss: 0.8364 time 25.58s
Final training  2114/4999 loss: 0.8214 time 25.41s
Final training  2115/4999 loss: 0.8032 time 25.40s
Final training  2116/4999 loss: 0.8145 time 25.15s
Final training  2117/4999 loss: 0.8159 time 25.69s
Final training  2118/4999 loss: 0.8183 time 25.72s
Final training  2119/4999 loss: 0.8183 time 25.84s
Final training  2120/4999 loss: 0.8187 time 25.79s
Final training  2121/4999 loss: 0.7947 time 25.49s
Final training  2122/4999 loss: 0.7837 time 25.57s
Final training  2123/4999 loss: 0.8155 time 25.43s
Final training  2124/4999 loss: 0.8384 time 25.55s
Final training  2125/4999 loss: 0.8366 time 25.51s
Final training  2126/4999 loss: 0.8554 time 25.91s
Final training  2127/4999 loss: 0.8113 time 25.34s
Final training  2128/4999 loss: 0.7982 time 25.72s
Final training  2129/4999 loss: 0.8428 time 25.35s
Final training  2130/4999 loss: 0.8261 time 25.47s
Final training  2131/4999 loss: 0.8018 time 25.76s
Final training  2132/4999 loss: 0.8240 time 25.42s
Final training  2133/4999 loss: 0.8067 time 25.55s
Final training  2134/4999 loss: 0.8112 time 25.69s
Final training  2135/4999 loss: 0.8115 time 25.15s
Final training  2136/4999 loss: 0.8359 time 25.58s
Final training  2137/4999 loss: 0.8048 time 25.49s
Final training  2138/4999 loss: 0.8223 time 25.45s
Final training  2139/4999 loss: 0.8042 time 25.36s
Final training  2140/4999 loss: 0.8157 time 25.67s
Final training  2141/4999 loss: 0.7922 time 25.40s
Final training  2142/4999 loss: 0.7945 time 25.41s
Final training  2143/4999 loss: 0.8172 time 25.20s
Final training  2144/4999 loss: 0.8152 time 25.69s
Final training  2145/4999 loss: 0.8004 time 25.32s
Final training  2146/4999 loss: 0.7995 time 25.40s
Final training  2147/4999 loss: 0.8126 time 25.13s
Final training  2148/4999 loss: 0.8195 time 25.50s
Final training  2149/4999 loss: 0.7975 time 25.50s
Final training  2150/4999 loss: 0.8207 time 25.68s
Final training  2151/4999 loss: 0.8287 time 25.14s
Final training  2152/4999 loss: 0.8177 time 25.30s
Final training  2153/4999 loss: 0.8261 time 25.66s
Final training  2154/4999 loss: 0.8139 time 25.41s
Final training  2155/4999 loss: 0.8081 time 25.57s
Final training  2156/4999 loss: 0.7927 time 25.39s
Final training  2157/4999 loss: 0.8075 time 25.36s
Final training  2158/4999 loss: 0.8181 time 24.24s
Final training  2159/4999 loss: 0.8133 time 26.01s
Final training  2160/4999 loss: 0.7964 time 25.67s
Final training  2161/4999 loss: 0.7946 time 25.47s
Final training  2162/4999 loss: 0.8160 time 25.54s
Final training  2163/4999 loss: 0.8044 time 25.22s
Final training  2164/4999 loss: 0.8025 time 25.48s
Final training  2165/4999 loss: 0.7945 time 25.40s
Final training  2166/4999 loss: 0.8036 time 25.42s
Final training  2167/4999 loss: 0.8144 time 25.45s
Final training  2168/4999 loss: 0.8096 time 25.31s
Final training  2169/4999 loss: 0.8277 time 25.41s
Final training  2170/4999 loss: 0.8284 time 25.67s
Final training  2171/4999 loss: 0.8086 time 25.61s
Final training  2172/4999 loss: 0.8099 time 25.22s
Final training  2173/4999 loss: 0.8190 time 25.47s
Final training  2174/4999 loss: 0.7918 time 25.70s
Final training  2175/4999 loss: 0.8195 time 25.37s
Final training  2176/4999 loss: 0.8213 time 25.57s
Final training  2177/4999 loss: 0.8298 time 25.52s
Final training  2178/4999 loss: 0.7926 time 25.51s
Final training  2179/4999 loss: 0.7867 time 25.43s
Final training  2180/4999 loss: 0.8184 time 25.19s
Final training  2181/4999 loss: 0.8222 time 25.29s
Final training  2182/4999 loss: 0.8170 time 25.42s
Final training  2183/4999 loss: 0.8068 time 25.43s
Final training  2184/4999 loss: 0.7841 time 25.63s
Final training  2185/4999 loss: 0.7945 time 25.57s
Final training  2186/4999 loss: 0.8328 time 25.39s
Final training  2187/4999 loss: 0.8219 time 25.45s
Final training  2188/4999 loss: 0.8024 time 25.64s
Final training  2189/4999 loss: 0.8223 time 25.34s
Final training  2190/4999 loss: 0.8258 time 25.21s
Final training  2191/4999 loss: 0.8320 time 25.54s
Final training  2192/4999 loss: 0.8125 time 25.41s
Final training  2193/4999 loss: 0.8285 time 25.26s
Final training  2194/4999 loss: 0.8140 time 25.37s
Final training  2195/4999 loss: 0.8271 time 25.80s
Final training  2196/4999 loss: 0.7844 time 25.57s
Final training  2197/4999 loss: 0.7913 time 25.55s
Final training  2198/4999 loss: 0.7980 time 25.52s
Final training  2199/4999 loss: 0.8031 time 25.52s
Dice accuracy for each class:  (tensor([0.9960, 0.9297, 0.9411, 0.9417, 0.7760, 0.7048, 0.9201, 0.7964, 0.9023,
        0.8505, 0.7447, 0.7421, 0.6880, 0.6441], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2199/4999 acc [ 0.827] time 194.65s
trigger times: 5
Final training  2200/4999 loss: 0.8041 time 25.05s
Final training  2201/4999 loss: 0.8074 time 25.44s
Final training  2202/4999 loss: 0.7963 time 25.32s
Final training  2203/4999 loss: 0.8106 time 25.25s
Final training  2204/4999 loss: 0.7978 time 25.62s
Final training  2205/4999 loss: 0.8278 time 25.36s
Final training  2206/4999 loss: 0.8099 time 25.47s
Final training  2207/4999 loss: 0.7994 time 25.29s
Final training  2208/4999 loss: 0.8361 time 25.52s
Final training  2209/4999 loss: 0.8197 time 25.80s
Final training  2210/4999 loss: 0.8076 time 25.40s
Final training  2211/4999 loss: 0.8148 time 25.79s
Final training  2212/4999 loss: 0.8422 time 25.54s
Final training  2213/4999 loss: 0.7936 time 25.68s
Final training  2214/4999 loss: 0.8199 time 25.42s
Final training  2215/4999 loss: 0.8155 time 25.39s
Final training  2216/4999 loss: 0.8108 time 25.01s
Final training  2217/4999 loss: 0.8350 time 25.31s
Final training  2218/4999 loss: 0.8061 time 25.61s
Final training  2219/4999 loss: 0.8139 time 25.15s
Final training  2220/4999 loss: 0.8261 time 25.25s
Final training  2221/4999 loss: 0.8316 time 25.66s
Final training  2222/4999 loss: 0.8078 time 25.29s
Final training  2223/4999 loss: 0.7935 time 25.08s
Final training  2224/4999 loss: 0.8124 time 25.73s
Final training  2225/4999 loss: 0.8085 time 25.49s
Final training  2226/4999 loss: 0.8133 time 25.16s
Final training  2227/4999 loss: 0.7871 time 25.31s
Final training  2228/4999 loss: 0.7977 time 25.15s
Final training  2229/4999 loss: 0.7860 time 25.00s
Final training  2230/4999 loss: 0.7835 time 25.30s
Final training  2231/4999 loss: 0.8032 time 25.25s
Final training  2232/4999 loss: 0.8039 time 25.32s
Final training  2233/4999 loss: 0.8195 time 25.17s
Final training  2234/4999 loss: 0.8051 time 25.23s
Final training  2235/4999 loss: 0.7913 time 25.19s
Final training  2236/4999 loss: 0.8194 time 24.92s
Final training  2237/4999 loss: 0.7963 time 24.80s
Final training  2238/4999 loss: 0.8002 time 24.71s
Final training  2239/4999 loss: 0.7971 time 25.05s
Final training  2240/4999 loss: 0.8099 time 25.06s
Final training  2241/4999 loss: 0.8151 time 25.23s
Final training  2242/4999 loss: 0.7981 time 25.20s
Final training  2243/4999 loss: 0.8404 time 24.83s
Final training  2244/4999 loss: 0.7974 time 25.15s
Final training  2245/4999 loss: 0.8178 time 25.20s
Final training  2246/4999 loss: 0.7972 time 25.68s
Final training  2247/4999 loss: 0.7857 time 25.70s
Final training  2248/4999 loss: 0.8098 time 25.36s
Final training  2249/4999 loss: 0.8264 time 25.34s
Final training  2250/4999 loss: 0.8499 time 25.44s
Final training  2251/4999 loss: 0.8077 time 25.83s
Final training  2252/4999 loss: 0.8125 time 25.36s
Final training  2253/4999 loss: 0.8216 time 25.68s
Final training  2254/4999 loss: 0.8342 time 25.54s
Final training  2255/4999 loss: 0.8338 time 25.15s
Final training  2256/4999 loss: 0.8019 time 25.61s
Final training  2257/4999 loss: 0.7906 time 25.30s
Final training  2258/4999 loss: 0.8136 time 25.70s
Final training  2259/4999 loss: 0.8380 time 25.70s
Final training  2260/4999 loss: 0.8270 time 25.72s
Final training  2261/4999 loss: 0.8418 time 25.39s
Final training  2262/4999 loss: 0.8242 time 25.54s
Final training  2263/4999 loss: 0.8315 time 25.28s
Final training  2264/4999 loss: 0.8335 time 25.11s
Final training  2265/4999 loss: 0.8096 time 25.45s
Final training  2266/4999 loss: 0.8033 time 25.49s
Final training  2267/4999 loss: 0.7973 time 25.68s
Final training  2268/4999 loss: 0.8365 time 25.33s
Final training  2269/4999 loss: 0.7863 time 25.53s
Final training  2270/4999 loss: 0.8028 time 25.42s
Final training  2271/4999 loss: 0.7930 time 25.29s
Final training  2272/4999 loss: 0.7911 time 25.65s
Final training  2273/4999 loss: 0.7898 time 25.40s
Final training  2274/4999 loss: 0.8073 time 25.27s
Final training  2275/4999 loss: 0.8526 time 25.57s
Final training  2276/4999 loss: 0.8198 time 25.76s
Final training  2277/4999 loss: 0.8083 time 25.62s
Final training  2278/4999 loss: 0.7958 time 25.30s
Final training  2279/4999 loss: 0.8038 time 25.61s
Final training  2280/4999 loss: 0.7839 time 25.54s
Final training  2281/4999 loss: 0.8039 time 25.41s
Final training  2282/4999 loss: 0.8080 time 25.24s
Final training  2283/4999 loss: 0.7889 time 25.51s
Final training  2284/4999 loss: 0.8040 time 25.44s
Final training  2285/4999 loss: 0.7913 time 25.34s
Final training  2286/4999 loss: 0.8204 time 25.03s
Final training  2287/4999 loss: 0.7934 time 25.37s
Final training  2288/4999 loss: 0.8158 time 25.08s
Final training  2289/4999 loss: 0.8116 time 25.22s
Final training  2290/4999 loss: 0.8106 time 25.50s
Final training  2291/4999 loss: 0.8216 time 24.93s
Final training  2292/4999 loss: 0.8049 time 25.47s
Final training  2293/4999 loss: 0.7884 time 25.43s
Final training  2294/4999 loss: 0.8002 time 25.38s
Final training  2295/4999 loss: 0.8190 time 25.64s
Final training  2296/4999 loss: 0.8015 time 25.52s
Final training  2297/4999 loss: 0.7994 time 25.76s
Final training  2298/4999 loss: 0.8078 time 25.53s
Final training  2299/4999 loss: 0.8070 time 25.29s
Dice accuracy for each class:  (tensor([0.9959, 0.8691, 0.9412, 0.9390, 0.7486, 0.7292, 0.9465, 0.7076, 0.9036,
        0.8473, 0.7508, 0.7582, 0.6677, 0.6575], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2299/4999 acc [ 0.819] time 194.51s
trigger times: 6
Final training  2300/4999 loss: 0.8187 time 25.32s
Final training  2301/4999 loss: 0.8131 time 25.48s
Final training  2302/4999 loss: 0.7930 time 25.34s
Final training  2303/4999 loss: 0.8091 time 25.38s
Final training  2304/4999 loss: 0.8596 time 25.09s
Final training  2305/4999 loss: 0.8424 time 25.22s
Final training  2306/4999 loss: 0.8367 time 25.20s
Final training  2307/4999 loss: 0.8187 time 25.52s
Final training  2308/4999 loss: 0.8462 time 25.38s
Final training  2309/4999 loss: 0.8167 time 25.25s
Final training  2310/4999 loss: 0.8438 time 25.14s
Final training  2311/4999 loss: 0.8288 time 25.32s
Final training  2312/4999 loss: 0.8695 time 25.02s
Final training  2313/4999 loss: 0.8379 time 25.70s
Final training  2314/4999 loss: 0.8245 time 25.68s
Final training  2315/4999 loss: 0.8150 time 25.37s
Final training  2316/4999 loss: 0.8305 time 25.46s
Final training  2317/4999 loss: 0.7801 time 25.69s
Final training  2318/4999 loss: 0.8176 time 25.21s
Final training  2319/4999 loss: 0.8149 time 25.97s
Final training  2320/4999 loss: 0.8126 time 25.48s
Final training  2321/4999 loss: 0.8257 time 25.72s
Final training  2322/4999 loss: 0.8080 time 25.28s
Final training  2323/4999 loss: 0.7975 time 25.64s
Final training  2324/4999 loss: 0.8330 time 25.46s
Final training  2325/4999 loss: 0.8592 time 25.27s
Final training  2326/4999 loss: 0.8498 time 25.50s
Final training  2327/4999 loss: 0.8236 time 25.56s
Final training  2328/4999 loss: 0.8215 time 25.46s
Final training  2329/4999 loss: 0.7864 time 25.58s
Final training  2330/4999 loss: 0.8084 time 25.59s
Final training  2331/4999 loss: 0.8187 time 25.29s
Final training  2332/4999 loss: 0.7994 time 25.59s
Final training  2333/4999 loss: 0.8236 time 25.47s
Final training  2334/4999 loss: 0.7989 time 25.62s
Final training  2335/4999 loss: 0.8093 time 25.75s
Final training  2336/4999 loss: 0.7937 time 24.79s
Final training  2337/4999 loss: 0.7763 time 25.40s
Final training  2338/4999 loss: 0.8086 time 25.62s
Final training  2339/4999 loss: 0.8295 time 25.68s
Final training  2340/4999 loss: 0.8307 time 25.37s
Final training  2341/4999 loss: 0.7801 time 25.40s
Final training  2342/4999 loss: 0.7861 time 25.80s
Final training  2343/4999 loss: 0.8170 time 25.72s
Final training  2344/4999 loss: 0.7953 time 25.69s
Final training  2345/4999 loss: 0.8204 time 25.36s
Final training  2346/4999 loss: 0.8180 time 25.10s
Final training  2347/4999 loss: 0.7932 time 24.85s
Final training  2348/4999 loss: 0.8086 time 25.27s
Final training  2349/4999 loss: 0.8109 time 25.62s
Final training  2350/4999 loss: 0.8173 time 25.54s
Final training  2351/4999 loss: 0.8566 time 25.54s
Final training  2352/4999 loss: 0.8139 time 25.67s
Final training  2353/4999 loss: 0.8337 time 25.65s
Final training  2354/4999 loss: 0.8244 time 25.39s
Final training  2355/4999 loss: 0.8326 time 25.31s
Final training  2356/4999 loss: 0.7883 time 25.46s
Final training  2357/4999 loss: 0.7892 time 24.98s
Final training  2358/4999 loss: 0.7939 time 25.52s
Final training  2359/4999 loss: 0.8040 time 25.76s
Final training  2360/4999 loss: 0.8331 time 25.56s
Final training  2361/4999 loss: 0.8094 time 25.33s
Final training  2362/4999 loss: 0.7827 time 25.21s
Final training  2363/4999 loss: 0.8020 time 25.38s
Final training  2364/4999 loss: 0.7996 time 25.13s
Final training  2365/4999 loss: 0.8027 time 24.78s
Final training  2366/4999 loss: 0.8064 time 25.54s
Final training  2367/4999 loss: 0.8308 time 25.66s
Final training  2368/4999 loss: 0.8336 time 25.01s
Final training  2369/4999 loss: 0.8462 time 25.12s
Final training  2370/4999 loss: 0.8145 time 25.57s
Final training  2371/4999 loss: 0.8123 time 25.48s
Final training  2372/4999 loss: 0.8064 time 25.41s
Final training  2373/4999 loss: 0.8069 time 25.52s
Final training  2374/4999 loss: 0.7944 time 25.51s
Final training  2375/4999 loss: 0.7770 time 25.83s
Final training  2376/4999 loss: 0.7952 time 25.40s
Final training  2377/4999 loss: 0.7996 time 25.33s
Final training  2378/4999 loss: 0.8211 time 25.48s
Final training  2379/4999 loss: 0.8133 time 25.26s
Final training  2380/4999 loss: 0.8112 time 25.37s
Final training  2381/4999 loss: 0.7954 time 25.46s
Final training  2382/4999 loss: 0.7941 time 25.85s
Final training  2383/4999 loss: 0.7959 time 25.73s
Final training  2384/4999 loss: 0.7870 time 25.57s
Final training  2385/4999 loss: 0.8158 time 25.25s
Final training  2386/4999 loss: 0.7926 time 25.69s
Final training  2387/4999 loss: 0.8044 time 25.31s
Final training  2388/4999 loss: 0.8040 time 25.58s
Final training  2389/4999 loss: 0.8154 time 25.04s
Final training  2390/4999 loss: 0.7960 time 25.00s
Final training  2391/4999 loss: 0.8035 time 25.40s
Final training  2392/4999 loss: 0.7767 time 25.75s
Final training  2393/4999 loss: 0.7820 time 25.54s
Final training  2394/4999 loss: 0.7942 time 25.17s
Final training  2395/4999 loss: 0.8040 time 25.24s
Final training  2396/4999 loss: 0.7885 time 25.69s
Final training  2397/4999 loss: 0.8047 time 25.28s
Final training  2398/4999 loss: 0.8036 time 25.68s
Final training  2399/4999 loss: 0.8068 time 25.62s
Dice accuracy for each class:  (tensor([0.9959, 0.8452, 0.9407, 0.9344, 0.7570, 0.7243, 0.9360, 0.7755, 0.9010,
        0.8514, 0.7441, 0.7968, 0.6615, 0.6474], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2399/4999 acc [ 0.823] time 193.23s
trigger times: 7
Final training  2400/4999 loss: 0.8020 time 25.36s
Final training  2401/4999 loss: 0.8234 time 25.67s
Final training  2402/4999 loss: 0.8129 time 25.50s
Final training  2403/4999 loss: 0.8122 time 25.56s
Final training  2404/4999 loss: 0.8090 time 25.04s
Final training  2405/4999 loss: 0.8163 time 25.13s
Final training  2406/4999 loss: 0.8161 time 25.19s
Final training  2407/4999 loss: 0.8094 time 25.11s
Final training  2408/4999 loss: 0.7953 time 25.24s
Final training  2409/4999 loss: 0.8163 time 25.33s
Final training  2410/4999 loss: 0.7926 time 25.58s
Final training  2411/4999 loss: 0.7952 time 25.68s
Final training  2412/4999 loss: 0.8019 time 25.58s
Final training  2413/4999 loss: 0.8188 time 25.52s
Final training  2414/4999 loss: 0.8085 time 25.78s
Final training  2415/4999 loss: 0.7931 time 24.82s
Final training  2416/4999 loss: 0.8124 time 25.47s
Final training  2417/4999 loss: 0.8064 time 25.91s
Final training  2418/4999 loss: 0.7913 time 25.41s
Final training  2419/4999 loss: 0.8044 time 25.48s
Final training  2420/4999 loss: 0.8188 time 25.64s
Final training  2421/4999 loss: 0.8047 time 25.46s
Final training  2422/4999 loss: 0.7907 time 25.48s
Final training  2423/4999 loss: 0.7952 time 25.18s
Final training  2424/4999 loss: 0.8203 time 25.63s
Final training  2425/4999 loss: 0.7940 time 25.52s
Final training  2426/4999 loss: 0.8074 time 25.54s
Final training  2427/4999 loss: 0.8254 time 25.50s
Final training  2428/4999 loss: 0.8197 time 25.33s
Final training  2429/4999 loss: 0.7919 time 25.55s
Final training  2430/4999 loss: 0.7979 time 25.28s
Final training  2431/4999 loss: 0.7941 time 25.74s
Final training  2432/4999 loss: 0.8030 time 25.68s
Final training  2433/4999 loss: 0.7950 time 25.48s
Final training  2434/4999 loss: 0.8069 time 25.37s
Final training  2435/4999 loss: 0.7817 time 25.42s
Final training  2436/4999 loss: 0.7869 time 25.39s
Final training  2437/4999 loss: 0.8221 time 25.47s
Final training  2438/4999 loss: 0.7935 time 25.60s
Final training  2439/4999 loss: 0.8029 time 25.73s
Final training  2440/4999 loss: 0.7964 time 25.40s
Final training  2441/4999 loss: 0.8182 time 25.53s
Final training  2442/4999 loss: 0.7967 time 25.35s
Final training  2443/4999 loss: 0.7969 time 25.41s
Final training  2444/4999 loss: 0.7990 time 25.36s
Final training  2445/4999 loss: 0.7962 time 25.76s
Final training  2446/4999 loss: 0.8088 time 25.54s
Final training  2447/4999 loss: 0.7752 time 25.37s
Final training  2448/4999 loss: 0.7964 time 25.65s
Final training  2449/4999 loss: 0.8143 time 26.07s
Final training  2450/4999 loss: 0.7860 time 25.66s
Final training  2451/4999 loss: 0.7914 time 25.33s
Final training  2452/4999 loss: 0.8073 time 25.45s
Final training  2453/4999 loss: 0.8113 time 25.43s
Final training  2454/4999 loss: 0.8132 time 25.51s
Final training  2455/4999 loss: 0.8010 time 25.45s
Final training  2456/4999 loss: 0.8029 time 25.37s
Final training  2457/4999 loss: 0.7995 time 25.78s
Final training  2458/4999 loss: 0.8132 time 25.44s
Final training  2459/4999 loss: 0.8113 time 25.79s
Final training  2460/4999 loss: 0.8113 time 25.15s
Final training  2461/4999 loss: 0.7954 time 25.41s
Final training  2462/4999 loss: 0.8226 time 25.79s
Final training  2463/4999 loss: 0.7965 time 25.38s
Final training  2464/4999 loss: 0.7931 time 25.67s
Final training  2465/4999 loss: 0.7871 time 25.28s
Final training  2466/4999 loss: 0.7959 time 25.59s
Final training  2467/4999 loss: 0.8043 time 25.53s
Final training  2468/4999 loss: 0.8013 time 25.27s
Final training  2469/4999 loss: 0.7891 time 25.52s
Final training  2470/4999 loss: 0.8150 time 25.65s
Final training  2471/4999 loss: 0.8109 time 25.48s
Final training  2472/4999 loss: 0.8252 time 25.43s
Final training  2473/4999 loss: 0.7958 time 25.49s
Final training  2474/4999 loss: 0.8090 time 25.45s
Final training  2475/4999 loss: 0.7776 time 25.35s
Final training  2476/4999 loss: 0.7994 time 25.13s
Final training  2477/4999 loss: 0.7922 time 25.17s
Final training  2478/4999 loss: 0.8030 time 25.15s
Final training  2479/4999 loss: 0.8011 time 25.21s
Final training  2480/4999 loss: 0.8251 time 25.43s
Final training  2481/4999 loss: 0.8170 time 25.57s
Final training  2482/4999 loss: 0.7989 time 25.55s
Final training  2483/4999 loss: 0.8073 time 25.33s
Final training  2484/4999 loss: 0.8159 time 25.02s
Final training  2485/4999 loss: 0.7846 time 24.94s
Final training  2486/4999 loss: 0.8130 time 25.27s
Final training  2487/4999 loss: 0.8015 time 24.91s
Final training  2488/4999 loss: 0.7932 time 24.83s
Final training  2489/4999 loss: 0.7891 time 25.18s
Final training  2490/4999 loss: 0.7964 time 25.33s
Final training  2491/4999 loss: 0.7733 time 25.33s
Final training  2492/4999 loss: 0.7993 time 25.36s
Final training  2493/4999 loss: 0.8063 time 25.43s
Final training  2494/4999 loss: 0.7721 time 25.40s
Final training  2495/4999 loss: 0.7913 time 25.42s
Final training  2496/4999 loss: 0.8018 time 25.52s
Final training  2497/4999 loss: 0.7750 time 25.66s
Final training  2498/4999 loss: 0.7977 time 25.53s
Final training  2499/4999 loss: 0.8120 time 25.66s
Dice accuracy for each class:  (tensor([0.9962, 0.8681, 0.9410, 0.9359, 0.7613, 0.7277, 0.9562, 0.7466, 0.9047,
        0.8324, 0.7523, 0.7629, 0.6812, 0.6769], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2499/4999 acc [ 0.825] time 194.94s
trigger times: 8
Final training  2500/4999 loss: 0.7889 time 25.63s
Final training  2501/4999 loss: 0.8024 time 25.54s
Final training  2502/4999 loss: 0.8384 time 25.46s
Final training  2503/4999 loss: 0.7972 time 25.28s
Final training  2504/4999 loss: 0.8123 time 25.42s
Final training  2505/4999 loss: 0.7975 time 25.62s
Final training  2506/4999 loss: 0.8065 time 25.66s
Final training  2507/4999 loss: 0.7834 time 25.37s
Final training  2508/4999 loss: 0.7740 time 24.78s
Final training  2509/4999 loss: 0.8198 time 25.29s
Final training  2510/4999 loss: 0.7790 time 25.24s
Final training  2511/4999 loss: 0.7871 time 25.51s
Final training  2512/4999 loss: 0.8139 time 25.63s
Final training  2513/4999 loss: 0.8133 time 25.67s
Final training  2514/4999 loss: 0.7939 time 25.28s
Final training  2515/4999 loss: 0.8136 time 25.67s
Final training  2516/4999 loss: 0.7958 time 25.05s
Final training  2517/4999 loss: 0.8266 time 25.62s
Final training  2518/4999 loss: 0.8091 time 25.62s
Final training  2519/4999 loss: 0.8201 time 25.56s
Final training  2520/4999 loss: 0.8127 time 25.71s
Final training  2521/4999 loss: 0.8288 time 25.47s
Final training  2522/4999 loss: 0.7998 time 25.61s
Final training  2523/4999 loss: 0.7907 time 25.14s
Final training  2524/4999 loss: 0.8066 time 25.41s
Final training  2525/4999 loss: 0.7959 time 25.20s
Final training  2526/4999 loss: 0.8054 time 24.76s
Final training  2527/4999 loss: 0.7715 time 25.57s
Final training  2528/4999 loss: 0.8023 time 25.60s
Final training  2529/4999 loss: 0.7942 time 25.57s
Final training  2530/4999 loss: 0.8103 time 25.59s
Final training  2531/4999 loss: 0.7812 time 25.57s
Final training  2532/4999 loss: 0.8125 time 25.64s
Final training  2533/4999 loss: 0.7849 time 25.44s
Final training  2534/4999 loss: 0.8015 time 25.62s
Final training  2535/4999 loss: 0.8045 time 25.43s
Final training  2536/4999 loss: 0.8011 time 25.32s
Final training  2537/4999 loss: 0.7829 time 25.33s
Final training  2538/4999 loss: 0.7669 time 25.37s
Final training  2539/4999 loss: 0.8010 time 25.02s
Final training  2540/4999 loss: 0.8024 time 25.38s
Final training  2541/4999 loss: 0.7746 time 25.25s
Final training  2542/4999 loss: 0.7931 time 25.30s
Final training  2543/4999 loss: 0.8113 time 25.75s
Final training  2544/4999 loss: 0.7993 time 25.34s
Final training  2545/4999 loss: 0.7976 time 25.42s
Final training  2546/4999 loss: 0.7905 time 25.48s
Final training  2547/4999 loss: 0.8025 time 25.17s
Final training  2548/4999 loss: 0.7882 time 25.44s
Final training  2549/4999 loss: 0.7961 time 25.16s
Final training  2550/4999 loss: 0.8195 time 25.31s
Final training  2551/4999 loss: 0.8099 time 25.38s
Final training  2552/4999 loss: 0.8040 time 25.42s
Final training  2553/4999 loss: 0.7935 time 25.46s
Final training  2554/4999 loss: 0.7788 time 25.29s
Final training  2555/4999 loss: 0.7844 time 25.36s
Final training  2556/4999 loss: 0.7989 time 25.65s
Final training  2557/4999 loss: 0.7618 time 25.57s
Final training  2558/4999 loss: 0.7907 time 25.50s
Final training  2559/4999 loss: 0.8063 time 25.06s
Final training  2560/4999 loss: 0.8067 time 25.46s
Final training  2561/4999 loss: 0.8004 time 25.49s
Final training  2562/4999 loss: 0.7922 time 25.44s
Final training  2563/4999 loss: 0.7939 time 25.51s
Final training  2564/4999 loss: 0.8014 time 25.47s
Final training  2565/4999 loss: 0.7806 time 25.46s
Final training  2566/4999 loss: 0.7886 time 25.24s
Final training  2567/4999 loss: 0.7862 time 25.49s
Final training  2568/4999 loss: 0.7872 time 25.36s
Final training  2569/4999 loss: 0.8338 time 25.43s
Final training  2570/4999 loss: 0.7806 time 25.58s
Final training  2571/4999 loss: 0.8154 time 25.17s
Final training  2572/4999 loss: 0.7787 time 25.40s
Final training  2573/4999 loss: 0.8021 time 25.61s
Final training  2574/4999 loss: 0.8136 time 25.48s
Final training  2575/4999 loss: 0.8143 time 25.45s
Final training  2576/4999 loss: 0.7647 time 25.53s
Final training  2577/4999 loss: 0.8051 time 25.29s
Final training  2578/4999 loss: 0.8004 time 25.21s
Final training  2579/4999 loss: 0.7725 time 25.53s
Final training  2580/4999 loss: 0.7983 time 25.31s
Final training  2581/4999 loss: 0.7991 time 25.10s
Final training  2582/4999 loss: 0.7707 time 25.16s
Final training  2583/4999 loss: 0.8226 time 25.14s
Final training  2584/4999 loss: 0.7923 time 24.94s
Final training  2585/4999 loss: 0.8041 time 25.39s
Final training  2586/4999 loss: 0.7898 time 25.07s
Final training  2587/4999 loss: 0.7898 time 25.06s
Final training  2588/4999 loss: 0.7955 time 24.94s
Final training  2589/4999 loss: 0.8230 time 25.26s
Final training  2590/4999 loss: 0.8139 time 25.47s
Final training  2591/4999 loss: 0.7959 time 25.33s
Final training  2592/4999 loss: 0.7885 time 25.15s
Final training  2593/4999 loss: 0.8171 time 25.25s
Final training  2594/4999 loss: 0.8069 time 25.56s
Final training  2595/4999 loss: 0.8458 time 25.52s
Final training  2596/4999 loss: 0.7995 time 25.94s
Final training  2597/4999 loss: 0.8286 time 25.23s
Final training  2598/4999 loss: 0.8068 time 25.42s
Final training  2599/4999 loss: 0.8264 time 25.47s
Dice accuracy for each class:  (tensor([0.9957, 0.7924, 0.9412, 0.9364, 0.7129, 0.7020, 0.9589, 0.7256, 0.8824,
        0.8293, 0.7413, 0.7811, 0.6694, 0.6504], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2599/4999 acc [ 0.809] time 194.41s
trigger times: 9
Final training  2600/4999 loss: 0.8140 time 25.41s
Final training  2601/4999 loss: 0.8192 time 24.92s
Final training  2602/4999 loss: 0.7835 time 25.43s
Final training  2603/4999 loss: 0.7842 time 25.32s
Final training  2604/4999 loss: 0.8227 time 25.43s
Final training  2605/4999 loss: 0.8362 time 25.58s
Final training  2606/4999 loss: 0.8145 time 25.94s
Final training  2607/4999 loss: 0.8495 time 25.45s
Final training  2608/4999 loss: 0.8217 time 25.38s
Final training  2609/4999 loss: 0.8114 time 25.45s
Final training  2610/4999 loss: 0.7759 time 25.45s
Final training  2611/4999 loss: 0.7935 time 25.27s
Final training  2612/4999 loss: 0.7998 time 25.59s
Final training  2613/4999 loss: 0.7890 time 25.30s
Final training  2614/4999 loss: 0.8072 time 25.56s
Final training  2615/4999 loss: 0.7939 time 25.20s
Final training  2616/4999 loss: 0.8236 time 25.62s
Final training  2617/4999 loss: 0.7961 time 25.36s
Final training  2618/4999 loss: 0.8269 time 25.39s
Final training  2619/4999 loss: 0.8076 time 25.19s
Final training  2620/4999 loss: 0.7869 time 25.16s
Final training  2621/4999 loss: 0.8147 time 25.61s
Final training  2622/4999 loss: 0.7767 time 25.49s
Final training  2623/4999 loss: 0.7968 time 25.74s
Final training  2624/4999 loss: 0.7727 time 25.55s
Final training  2625/4999 loss: 0.8041 time 25.37s
Final training  2626/4999 loss: 0.8038 time 25.33s
Final training  2627/4999 loss: 0.7882 time 25.58s
Final training  2628/4999 loss: 0.7841 time 25.66s
Final training  2629/4999 loss: 0.7648 time 25.24s
Final training  2630/4999 loss: 0.8140 time 25.52s
Final training  2631/4999 loss: 0.7773 time 25.55s
Final training  2632/4999 loss: 0.7784 time 25.15s
Final training  2633/4999 loss: 0.7943 time 25.40s
Final training  2634/4999 loss: 0.7895 time 25.35s
Final training  2635/4999 loss: 0.8018 time 25.39s
Final training  2636/4999 loss: 0.8128 time 25.43s
Final training  2637/4999 loss: 0.8089 time 24.89s
Final training  2638/4999 loss: 0.8020 time 25.63s
Final training  2639/4999 loss: 0.7907 time 25.47s
Final training  2640/4999 loss: 0.8006 time 25.50s
Final training  2641/4999 loss: 0.7859 time 25.33s
Final training  2642/4999 loss: 0.7824 time 25.28s
Final training  2643/4999 loss: 0.7870 time 25.19s
Final training  2644/4999 loss: 0.8119 time 25.27s
Final training  2645/4999 loss: 0.7925 time 25.43s
Final training  2646/4999 loss: 0.8021 time 25.24s
Final training  2647/4999 loss: 0.8272 time 25.17s
Final training  2648/4999 loss: 0.8087 time 25.29s
Final training  2649/4999 loss: 0.8239 time 25.08s
Final training  2650/4999 loss: 0.7863 time 25.27s
Final training  2651/4999 loss: 0.8042 time 25.04s
Final training  2652/4999 loss: 0.8101 time 25.38s
Final training  2653/4999 loss: 0.8046 time 25.50s
Final training  2654/4999 loss: 0.8113 time 25.20s
Final training  2655/4999 loss: 0.7732 time 25.40s
Final training  2656/4999 loss: 0.8075 time 25.24s
Final training  2657/4999 loss: 0.8060 time 25.22s
Final training  2658/4999 loss: 0.8068 time 25.42s
Final training  2659/4999 loss: 0.8330 time 25.51s
Final training  2660/4999 loss: 0.7782 time 25.30s
Final training  2661/4999 loss: 0.7979 time 25.49s
Final training  2662/4999 loss: 0.8155 time 25.30s
Final training  2663/4999 loss: 0.8119 time 24.84s
Final training  2664/4999 loss: 0.7911 time 25.42s
Final training  2665/4999 loss: 0.8032 time 25.65s
Final training  2666/4999 loss: 0.8016 time 25.52s
Final training  2667/4999 loss: 0.8038 time 25.36s
Final training  2668/4999 loss: 0.8174 time 25.46s
Final training  2669/4999 loss: 0.7949 time 25.64s
Final training  2670/4999 loss: 0.8081 time 25.60s
Final training  2671/4999 loss: 0.7881 time 25.58s
Final training  2672/4999 loss: 0.7923 time 25.55s
Final training  2673/4999 loss: 0.7997 time 25.68s
Final training  2674/4999 loss: 0.7791 time 24.91s
Final training  2675/4999 loss: 0.7891 time 25.61s
Final training  2676/4999 loss: 0.8169 time 25.43s
Final training  2677/4999 loss: 0.7986 time 25.50s
Final training  2678/4999 loss: 0.7812 time 25.65s
Final training  2679/4999 loss: 0.7943 time 25.37s
Final training  2680/4999 loss: 0.7819 time 25.26s
Final training  2681/4999 loss: 0.8061 time 25.37s
Final training  2682/4999 loss: 0.7903 time 25.20s
Final training  2683/4999 loss: 0.8051 time 25.42s
Final training  2684/4999 loss: 0.8052 time 24.97s
Final training  2685/4999 loss: 0.7851 time 24.50s
Final training  2686/4999 loss: 0.7847 time 25.37s
Final training  2687/4999 loss: 0.7933 time 25.57s
Final training  2688/4999 loss: 0.7898 time 25.46s
Final training  2689/4999 loss: 0.8286 time 25.48s
Final training  2690/4999 loss: 0.7944 time 25.64s
Final training  2691/4999 loss: 0.7841 time 25.41s
Final training  2692/4999 loss: 0.8000 time 25.50s
Final training  2693/4999 loss: 0.8080 time 25.39s
Final training  2694/4999 loss: 0.7976 time 25.35s
Final training  2695/4999 loss: 0.8113 time 25.22s
Final training  2696/4999 loss: 0.7664 time 25.40s
Final training  2697/4999 loss: 0.8024 time 25.80s
Final training  2698/4999 loss: 0.7920 time 25.37s
Final training  2699/4999 loss: 0.7845 time 25.60s
Dice accuracy for each class:  (tensor([0.9961, 0.8384, 0.9424, 0.9344, 0.7728, 0.7204, 0.9562, 0.7661, 0.9009,
        0.8445, 0.7434, 0.7824, 0.6786, 0.6334], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2699/4999 acc [ 0.822] time 193.60s
trigger times: 10
Final training  2700/4999 loss: 0.8048 time 25.10s
Final training  2701/4999 loss: 0.7847 time 25.57s
Final training  2702/4999 loss: 0.8000 time 25.36s
Final training  2703/4999 loss: 0.7954 time 25.58s
Final training  2704/4999 loss: 0.7903 time 25.64s
Final training  2705/4999 loss: 0.7873 time 25.46s
Final training  2706/4999 loss: 0.7874 time 25.21s
Final training  2707/4999 loss: 0.8037 time 25.71s
Final training  2708/4999 loss: 0.7837 time 25.67s
Final training  2709/4999 loss: 0.7801 time 25.61s
Final training  2710/4999 loss: 0.7948 time 25.45s
Final training  2711/4999 loss: 0.7945 time 25.42s
Final training  2712/4999 loss: 0.7666 time 25.63s
Final training  2713/4999 loss: 0.7879 time 25.59s
Final training  2714/4999 loss: 0.7814 time 25.28s
Final training  2715/4999 loss: 0.8007 time 25.68s
Final training  2716/4999 loss: 0.7891 time 25.28s
Final training  2717/4999 loss: 0.8052 time 25.27s
Final training  2718/4999 loss: 0.7783 time 25.60s
Final training  2719/4999 loss: 0.9011 time 25.44s
Final training  2720/4999 loss: 0.7823 time 25.46s
Final training  2721/4999 loss: 0.7991 time 25.64s
Final training  2722/4999 loss: 0.8150 time 25.53s
Final training  2723/4999 loss: 0.7970 time 25.26s
Final training  2724/4999 loss: 0.8015 time 25.53s
Final training  2725/4999 loss: 0.7864 time 25.25s
Final training  2726/4999 loss: 0.8080 time 25.67s
Final training  2727/4999 loss: 0.7885 time 25.42s
Final training  2728/4999 loss: 0.7963 time 25.25s
Final training  2729/4999 loss: 0.7927 time 25.01s
Final training  2730/4999 loss: 0.7751 time 25.60s
Final training  2731/4999 loss: 0.8040 time 25.79s
Final training  2732/4999 loss: 0.7906 time 25.58s
Final training  2733/4999 loss: 0.7979 time 25.72s
Final training  2734/4999 loss: 0.7938 time 25.77s
Final training  2735/4999 loss: 0.7920 time 25.18s
Final training  2736/4999 loss: 0.7806 time 25.49s
Final training  2737/4999 loss: 0.7960 time 25.58s
Final training  2738/4999 loss: 0.8076 time 25.32s
Final training  2739/4999 loss: 0.7997 time 25.52s
Final training  2740/4999 loss: 0.8166 time 25.57s
Final training  2741/4999 loss: 0.8036 time 25.45s
Final training  2742/4999 loss: 0.7763 time 25.19s
Final training  2743/4999 loss: 0.8086 time 25.55s
Final training  2744/4999 loss: 0.7919 time 25.55s
Final training  2745/4999 loss: 0.8042 time 25.23s
Final training  2746/4999 loss: 0.7862 time 25.14s
Final training  2747/4999 loss: 0.7993 time 25.44s
Final training  2748/4999 loss: 0.7818 time 25.62s
Final training  2749/4999 loss: 0.8292 time 25.63s
Final training  2750/4999 loss: 0.8079 time 25.56s
Final training  2751/4999 loss: 0.8104 time 25.40s
Final training  2752/4999 loss: 0.7986 time 25.30s
Final training  2753/4999 loss: 0.8163 time 25.32s
Final training  2754/4999 loss: 0.7931 time 25.25s
Final training  2755/4999 loss: 0.7957 time 25.40s
Final training  2756/4999 loss: 0.8049 time 25.46s
Final training  2757/4999 loss: 0.8048 time 25.23s
Final training  2758/4999 loss: 0.8033 time 25.28s
Final training  2759/4999 loss: 0.8001 time 25.67s
Final training  2760/4999 loss: 0.8195 time 25.39s
Final training  2761/4999 loss: 0.7684 time 25.23s
Final training  2762/4999 loss: 0.8030 time 25.73s
Final training  2763/4999 loss: 0.8067 time 26.06s
Final training  2764/4999 loss: 0.7778 time 25.32s
Final training  2765/4999 loss: 0.7973 time 25.51s
Final training  2766/4999 loss: 0.7958 time 25.33s
Final training  2767/4999 loss: 0.7907 time 25.56s
Final training  2768/4999 loss: 0.8235 time 25.28s
Final training  2769/4999 loss: 0.7802 time 25.53s
Final training  2770/4999 loss: 0.7884 time 25.33s
Final training  2771/4999 loss: 0.7750 time 24.80s
Final training  2772/4999 loss: 0.7849 time 25.28s
Final training  2773/4999 loss: 0.7708 time 25.61s
Final training  2774/4999 loss: 0.8017 time 25.59s
Final training  2775/4999 loss: 0.7891 time 25.13s
Final training  2776/4999 loss: 0.8139 time 25.21s
Final training  2777/4999 loss: 0.7689 time 25.19s
Final training  2778/4999 loss: 0.8120 time 25.05s
Final training  2779/4999 loss: 0.7917 time 25.20s
Final training  2780/4999 loss: 0.7668 time 25.19s
Final training  2781/4999 loss: 0.8057 time 25.17s
Final training  2782/4999 loss: 0.7902 time 24.93s
Final training  2783/4999 loss: 0.7904 time 24.54s
Final training  2784/4999 loss: 0.7947 time 25.06s
Final training  2785/4999 loss: 0.7777 time 25.57s
Final training  2786/4999 loss: 0.7862 time 25.52s
Final training  2787/4999 loss: 0.7730 time 25.46s
Final training  2788/4999 loss: 0.7904 time 25.39s
Final training  2789/4999 loss: 0.8055 time 25.29s
Final training  2790/4999 loss: 0.7691 time 25.23s
Final training  2791/4999 loss: 0.8063 time 25.09s
Final training  2792/4999 loss: 0.8096 time 25.35s
Final training  2793/4999 loss: 0.8078 time 25.30s
Final training  2794/4999 loss: 0.7970 time 24.98s
Final training  2795/4999 loss: 0.7875 time 24.95s
Final training  2796/4999 loss: 0.8033 time 25.70s
Final training  2797/4999 loss: 0.7807 time 25.23s
Final training  2798/4999 loss: 0.8003 time 25.40s
Final training  2799/4999 loss: 0.7821 time 25.51s
Dice accuracy for each class:  (tensor([0.9957, 0.8872, 0.9426, 0.9408, 0.7868, 0.7014, 0.9169, 0.7879, 0.9006,
        0.8424, 0.7558, 0.7686, 0.6667, 0.6448], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2799/4999 acc [ 0.824] time 193.06s
trigger times: 11
Final training  2800/4999 loss: 0.7888 time 25.39s
Final training  2801/4999 loss: 0.7964 time 25.47s
Final training  2802/4999 loss: 0.7722 time 25.42s
Final training  2803/4999 loss: 0.8045 time 25.34s
Final training  2804/4999 loss: 0.7753 time 25.54s
Final training  2805/4999 loss: 0.7825 time 25.44s
Final training  2806/4999 loss: 0.8238 time 25.39s
Final training  2807/4999 loss: 0.7699 time 25.02s
Final training  2808/4999 loss: 0.8007 time 25.49s
Final training  2809/4999 loss: 0.8095 time 25.55s
Final training  2810/4999 loss: 0.7860 time 25.40s
Final training  2811/4999 loss: 0.7778 time 25.57s
Final training  2812/4999 loss: 0.8064 time 25.38s
Final training  2813/4999 loss: 0.7508 time 25.39s
Final training  2814/4999 loss: 0.7811 time 25.30s
Final training  2815/4999 loss: 0.7923 time 25.42s
Final training  2816/4999 loss: 0.7905 time 25.37s
Final training  2817/4999 loss: 0.8045 time 25.11s
Final training  2818/4999 loss: 0.7656 time 24.95s
Final training  2819/4999 loss: 0.7689 time 25.61s
Final training  2820/4999 loss: 0.7948 time 25.29s
Final training  2821/4999 loss: 0.8133 time 25.53s
Final training  2822/4999 loss: 0.7682 time 25.32s
Final training  2823/4999 loss: 0.8104 time 25.15s
Final training  2824/4999 loss: 0.7979 time 25.12s
Final training  2825/4999 loss: 0.7919 time 25.34s
Final training  2826/4999 loss: 0.7911 time 25.57s
Final training  2827/4999 loss: 0.8090 time 25.49s
Final training  2828/4999 loss: 0.8054 time 25.69s
Final training  2829/4999 loss: 0.7769 time 25.26s
Final training  2830/4999 loss: 0.7998 time 25.51s
Final training  2831/4999 loss: 0.8130 time 25.21s
Final training  2832/4999 loss: 0.7957 time 25.24s
Final training  2833/4999 loss: 0.7715 time 25.47s
Final training  2834/4999 loss: 0.7855 time 25.59s
Final training  2835/4999 loss: 0.7788 time 25.07s
Final training  2836/4999 loss: 0.7772 time 24.38s
Final training  2837/4999 loss: 0.7919 time 25.43s
Final training  2838/4999 loss: 0.8124 time 25.43s
Final training  2839/4999 loss: 0.8077 time 24.87s
Final training  2840/4999 loss: 0.8023 time 25.28s
Final training  2841/4999 loss: 0.7874 time 25.24s
Final training  2842/4999 loss: 0.8135 time 25.62s
Final training  2843/4999 loss: 0.8066 time 25.50s
Final training  2844/4999 loss: 0.8043 time 25.19s
Final training  2845/4999 loss: 0.7938 time 25.34s
Final training  2846/4999 loss: 0.7739 time 25.43s
Final training  2847/4999 loss: 0.8040 time 25.56s
Final training  2848/4999 loss: 0.8202 time 25.40s
Final training  2849/4999 loss: 0.7896 time 25.48s
Final training  2850/4999 loss: 0.8017 time 25.43s
Final training  2851/4999 loss: 0.7877 time 25.34s
Final training  2852/4999 loss: 0.8015 time 25.39s
Final training  2853/4999 loss: 0.7762 time 25.52s
Final training  2854/4999 loss: 0.7935 time 25.49s
Final training  2855/4999 loss: 0.7727 time 25.37s
Final training  2856/4999 loss: 0.7908 time 25.55s
Final training  2857/4999 loss: 0.7969 time 25.44s
Final training  2858/4999 loss: 0.7781 time 25.44s
Final training  2859/4999 loss: 0.8008 time 25.84s
Final training  2860/4999 loss: 0.7960 time 25.41s
Final training  2861/4999 loss: 0.7831 time 25.65s
Final training  2862/4999 loss: 0.8016 time 25.37s
Final training  2863/4999 loss: 0.8060 time 25.13s
Final training  2864/4999 loss: 0.8156 time 25.44s
Final training  2865/4999 loss: 0.7918 time 25.37s
Final training  2866/4999 loss: 0.7949 time 25.40s
Final training  2867/4999 loss: 0.7766 time 25.16s
Final training  2868/4999 loss: 0.8162 time 25.83s
Final training  2869/4999 loss: 0.8030 time 25.62s
Final training  2870/4999 loss: 0.8025 time 25.24s
Final training  2871/4999 loss: 0.7996 time 25.46s
Final training  2872/4999 loss: 0.7993 time 25.52s
Final training  2873/4999 loss: 0.8113 time 25.87s
Final training  2874/4999 loss: 0.7968 time 25.59s
Final training  2875/4999 loss: 0.8169 time 25.55s
Final training  2876/4999 loss: 0.8116 time 25.69s
Final training  2877/4999 loss: 0.8168 time 25.36s
Final training  2878/4999 loss: 0.7925 time 25.28s
Final training  2879/4999 loss: 0.8034 time 25.27s
Final training  2880/4999 loss: 0.8046 time 25.42s
Final training  2881/4999 loss: 0.8150 time 24.98s
Final training  2882/4999 loss: 0.8060 time 25.07s
Final training  2883/4999 loss: 0.8024 time 25.45s
Final training  2884/4999 loss: 0.7941 time 24.99s
Final training  2885/4999 loss: 0.8010 time 25.08s
Final training  2886/4999 loss: 0.7925 time 25.42s
Final training  2887/4999 loss: 0.8039 time 25.55s
Final training  2888/4999 loss: 0.8351 time 25.44s
Final training  2889/4999 loss: 0.7969 time 25.40s
Final training  2890/4999 loss: 0.8071 time 25.19s
Final training  2891/4999 loss: 0.7990 time 25.23s
Final training  2892/4999 loss: 0.7998 time 25.55s
Final training  2893/4999 loss: 0.7842 time 25.23s
Final training  2894/4999 loss: 0.7972 time 25.53s
Final training  2895/4999 loss: 0.7798 time 25.33s
Final training  2896/4999 loss: 0.7737 time 25.71s
Final training  2897/4999 loss: 0.8033 time 25.68s
Final training  2898/4999 loss: 0.7853 time 25.39s
Final training  2899/4999 loss: 0.8096 time 25.56s
Dice accuracy for each class:  (tensor([0.9957, 0.9098, 0.9397, 0.9315, 0.7710, 0.7377, 0.8973, 0.8078, 0.9012,
        0.8392, 0.7441, 0.7651, 0.6863, 0.6731], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2899/4999 acc [ 0.829] time 194.59s
trigger times: 12
Final training  2900/4999 loss: 0.8090 time 25.13s
Final training  2901/4999 loss: 0.7816 time 25.68s
Final training  2902/4999 loss: 0.8086 time 25.05s
Final training  2903/4999 loss: 0.8189 time 25.65s
Final training  2904/4999 loss: 0.7908 time 25.42s
Final training  2905/4999 loss: 0.7929 time 25.52s
Final training  2906/4999 loss: 0.7937 time 25.59s
Final training  2907/4999 loss: 0.7769 time 25.45s
Final training  2908/4999 loss: 0.8154 time 25.58s
Final training  2909/4999 loss: 0.7793 time 25.56s
Final training  2910/4999 loss: 0.7812 time 25.75s
Final training  2911/4999 loss: 0.7854 time 25.58s
Final training  2912/4999 loss: 0.7907 time 25.52s
Final training  2913/4999 loss: 0.8053 time 25.70s
Final training  2914/4999 loss: 0.7938 time 25.32s
Final training  2915/4999 loss: 0.8025 time 25.37s
Final training  2916/4999 loss: 0.7707 time 25.68s
Final training  2917/4999 loss: 0.8078 time 25.23s
Final training  2918/4999 loss: 0.8134 time 25.56s
Final training  2919/4999 loss: 0.7778 time 25.76s
Final training  2920/4999 loss: 0.7836 time 25.31s
Final training  2921/4999 loss: 0.8081 time 25.34s
Final training  2922/4999 loss: 0.7841 time 26.09s
Final training  2923/4999 loss: 0.7886 time 25.58s
Final training  2924/4999 loss: 0.7837 time 25.41s
Final training  2925/4999 loss: 0.7845 time 25.10s
Final training  2926/4999 loss: 0.7843 time 25.44s
Final training  2927/4999 loss: 0.8011 time 25.48s
Final training  2928/4999 loss: 0.7958 time 25.12s
Final training  2929/4999 loss: 0.8087 time 25.51s
Final training  2930/4999 loss: 0.7977 time 25.07s
Final training  2931/4999 loss: 0.7952 time 25.29s
Final training  2932/4999 loss: 0.7817 time 24.92s
Final training  2933/4999 loss: 0.7874 time 25.25s
Final training  2934/4999 loss: 0.7901 time 25.36s
Final training  2935/4999 loss: 0.8144 time 25.63s
Final training  2936/4999 loss: 0.8018 time 25.47s
Final training  2937/4999 loss: 0.8049 time 25.52s
Final training  2938/4999 loss: 0.7906 time 25.70s
Final training  2939/4999 loss: 0.7956 time 25.23s
Final training  2940/4999 loss: 0.7947 time 25.63s
Final training  2941/4999 loss: 0.7862 time 25.50s
Final training  2942/4999 loss: 0.8099 time 25.45s
Final training  2943/4999 loss: 0.7816 time 25.98s
Final training  2944/4999 loss: 0.7858 time 25.81s
Final training  2945/4999 loss: 0.7991 time 25.71s
Final training  2946/4999 loss: 0.7818 time 25.29s
Final training  2947/4999 loss: 0.7951 time 25.60s
Final training  2948/4999 loss: 0.8069 time 25.40s
Final training  2949/4999 loss: 0.7799 time 25.60s
Final training  2950/4999 loss: 0.7979 time 25.51s
Final training  2951/4999 loss: 0.8013 time 25.24s
Final training  2952/4999 loss: 0.8006 time 25.68s
Final training  2953/4999 loss: 0.7913 time 25.57s
Final training  2954/4999 loss: 0.7922 time 25.49s
Final training  2955/4999 loss: 0.7966 time 25.62s
Final training  2956/4999 loss: 0.8013 time 25.63s
Final training  2957/4999 loss: 0.7979 time 25.15s
Final training  2958/4999 loss: 0.7785 time 25.17s
Final training  2959/4999 loss: 0.8013 time 25.77s
Final training  2960/4999 loss: 0.7971 time 25.29s
Final training  2961/4999 loss: 0.7802 time 25.36s
Final training  2962/4999 loss: 0.7995 time 25.78s
Final training  2963/4999 loss: 0.7707 time 25.41s
Final training  2964/4999 loss: 0.7885 time 25.41s
Final training  2965/4999 loss: 0.7787 time 25.43s
Final training  2966/4999 loss: 0.7734 time 25.27s
Final training  2967/4999 loss: 0.8229 time 25.35s
Final training  2968/4999 loss: 0.7902 time 25.41s
Final training  2969/4999 loss: 0.7920 time 25.55s
Final training  2970/4999 loss: 0.7615 time 25.13s
Final training  2971/4999 loss: 0.7609 time 25.45s
Final training  2972/4999 loss: 0.7977 time 25.28s
Final training  2973/4999 loss: 0.7961 time 25.18s
Final training  2974/4999 loss: 0.7789 time 25.41s
Final training  2975/4999 loss: 0.8008 time 25.35s
Final training  2976/4999 loss: 0.7861 time 25.38s
Final training  2977/4999 loss: 0.7910 time 25.31s
Final training  2978/4999 loss: 0.7706 time 25.16s
Final training  2979/4999 loss: 0.7806 time 25.46s
Final training  2980/4999 loss: 0.8066 time 25.10s
Final training  2981/4999 loss: 0.7796 time 25.38s
Final training  2982/4999 loss: 0.8007 time 25.14s
Final training  2983/4999 loss: 0.7833 time 25.19s
Final training  2984/4999 loss: 0.7904 time 25.33s
Final training  2985/4999 loss: 0.7955 time 25.60s
Final training  2986/4999 loss: 0.7785 time 25.46s
Final training  2987/4999 loss: 0.7980 time 25.76s
Final training  2988/4999 loss: 0.7842 time 25.54s
Final training  2989/4999 loss: 0.7779 time 25.24s
Final training  2990/4999 loss: 0.7947 time 25.52s
Final training  2991/4999 loss: 0.7746 time 25.48s
Final training  2992/4999 loss: 0.7825 time 25.59s
Final training  2993/4999 loss: 0.7829 time 24.98s
Final training  2994/4999 loss: 0.7789 time 25.25s
Final training  2995/4999 loss: 0.8103 time 25.26s
Final training  2996/4999 loss: 0.7937 time 25.36s
Final training  2997/4999 loss: 0.8083 time 25.37s
Final training  2998/4999 loss: 0.7822 time 25.74s
Final training  2999/4999 loss: 0.7632 time 25.49s
Dice accuracy for each class:  (tensor([0.9963, 0.8937, 0.9391, 0.9358, 0.7763, 0.7141, 0.9506, 0.7554, 0.8917,
        0.8397, 0.7483, 0.7550, 0.6843, 0.6757], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2999/4999 acc [ 0.826] time 194.96s
trigger times: 13
Final training  3000/4999 loss: 0.8036 time 25.11s
Final training  3001/4999 loss: 0.8080 time 25.46s
Final training  3002/4999 loss: 0.7712 time 25.76s
Final training  3003/4999 loss: 0.7821 time 25.63s
Final training  3004/4999 loss: 0.7931 time 25.54s
Final training  3005/4999 loss: 0.7474 time 25.59s
Final training  3006/4999 loss: 0.8087 time 25.75s
Final training  3007/4999 loss: 0.7775 time 25.43s
Final training  3008/4999 loss: 0.7990 time 25.42s
Final training  3009/4999 loss: 0.7953 time 25.60s
Final training  3010/4999 loss: 0.7871 time 25.52s
Final training  3011/4999 loss: 0.7970 time 25.32s
Final training  3012/4999 loss: 0.7903 time 25.53s
Final training  3013/4999 loss: 0.7957 time 25.71s
Final training  3014/4999 loss: 0.8050 time 25.55s
Final training  3015/4999 loss: 0.7904 time 25.43s
Final training  3016/4999 loss: 0.7889 time 25.72s
Final training  3017/4999 loss: 0.7720 time 25.07s
Final training  3018/4999 loss: 0.7714 time 25.49s
Final training  3019/4999 loss: 0.7859 time 25.37s
Final training  3020/4999 loss: 0.7861 time 25.44s
Final training  3021/4999 loss: 0.7970 time 25.50s
Final training  3022/4999 loss: 0.7800 time 25.43s
Final training  3023/4999 loss: 0.7950 time 25.48s
Final training  3024/4999 loss: 0.7686 time 25.64s
Final training  3025/4999 loss: 0.7968 time 25.68s
Final training  3026/4999 loss: 0.7834 time 25.56s
Final training  3027/4999 loss: 0.8162 time 25.34s
Final training  3028/4999 loss: 0.7955 time 25.43s
Final training  3029/4999 loss: 0.7769 time 25.43s
Final training  3030/4999 loss: 0.7766 time 25.69s
Final training  3031/4999 loss: 0.7831 time 25.42s
Final training  3032/4999 loss: 0.7871 time 25.80s
Final training  3033/4999 loss: 0.7924 time 25.58s
Final training  3034/4999 loss: 0.7892 time 25.46s
Final training  3035/4999 loss: 0.7748 time 25.45s
Final training  3036/4999 loss: 0.8145 time 25.44s
Final training  3037/4999 loss: 0.7960 time 25.54s
Final training  3038/4999 loss: 0.7927 time 25.47s
Final training  3039/4999 loss: 0.7900 time 25.52s
Final training  3040/4999 loss: 0.7717 time 25.19s
Final training  3041/4999 loss: 0.8004 time 25.38s
Final training  3042/4999 loss: 0.8118 time 25.26s
Final training  3043/4999 loss: 0.8015 time 25.35s
Final training  3044/4999 loss: 0.7945 time 25.31s
Final training  3045/4999 loss: 0.7857 time 25.40s
Final training  3046/4999 loss: 0.8036 time 25.16s
Final training  3047/4999 loss: 0.7812 time 25.19s
Final training  3048/4999 loss: 0.8109 time 25.52s
Final training  3049/4999 loss: 0.8085 time 25.53s
Final training  3050/4999 loss: 0.8253 time 25.76s
Final training  3051/4999 loss: 0.7982 time 25.53s
Final training  3052/4999 loss: 0.7920 time 25.16s
Final training  3053/4999 loss: 0.7935 time 25.68s
Final training  3054/4999 loss: 0.8136 time 25.48s
Final training  3055/4999 loss: 0.7968 time 25.95s
Final training  3056/4999 loss: 0.7962 time 25.74s
Final training  3057/4999 loss: 0.7771 time 25.74s
Final training  3058/4999 loss: 0.7819 time 25.80s
Final training  3059/4999 loss: 0.7966 time 25.85s
Final training  3060/4999 loss: 0.7748 time 25.52s
Final training  3061/4999 loss: 0.8158 time 25.57s
Final training  3062/4999 loss: 0.7845 time 25.59s
Final training  3063/4999 loss: 0.8241 time 25.83s
Final training  3064/4999 loss: 0.7673 time 25.55s
Final training  3065/4999 loss: 0.8179 time 25.37s
Final training  3066/4999 loss: 0.7927 time 25.47s
Final training  3067/4999 loss: 0.7834 time 25.72s
Final training  3068/4999 loss: 0.7763 time 25.38s
Final training  3069/4999 loss: 0.7925 time 25.58s
Final training  3070/4999 loss: 0.7909 time 25.57s
Final training  3071/4999 loss: 0.7824 time 25.70s
Final training  3072/4999 loss: 0.7502 time 25.55s
Final training  3073/4999 loss: 0.7817 time 25.33s
Final training  3074/4999 loss: 0.7897 time 25.43s
Final training  3075/4999 loss: 0.7941 time 25.09s
Final training  3076/4999 loss: 0.7800 time 25.41s
Final training  3077/4999 loss: 0.7954 time 25.46s
Final training  3078/4999 loss: 0.7937 time 25.09s
Final training  3079/4999 loss: 0.8091 time 25.09s
Final training  3080/4999 loss: 0.7682 time 25.77s
Final training  3081/4999 loss: 0.8162 time 25.64s
Final training  3082/4999 loss: 0.7768 time 25.51s
Final training  3083/4999 loss: 0.8004 time 25.64s
Final training  3084/4999 loss: 0.7895 time 25.51s
Final training  3085/4999 loss: 0.8166 time 25.57s
Final training  3086/4999 loss: 0.7859 time 25.38s
Final training  3087/4999 loss: 0.8075 time 25.04s
Final training  3088/4999 loss: 0.7871 time 25.62s
Final training  3089/4999 loss: 0.7750 time 25.25s
Final training  3090/4999 loss: 0.7908 time 25.58s
Final training  3091/4999 loss: 0.7984 time 25.46s
Final training  3092/4999 loss: 0.7891 time 25.60s
Final training  3093/4999 loss: 0.7826 time 25.49s
Final training  3094/4999 loss: 0.8096 time 25.45s
Final training  3095/4999 loss: 0.7912 time 25.35s
Final training  3096/4999 loss: 0.8058 time 25.39s
Final training  3097/4999 loss: 0.7665 time 25.43s
Final training  3098/4999 loss: 0.8148 time 25.64s
Final training  3099/4999 loss: 0.7854 time 25.66s
Dice accuracy for each class:  (tensor([0.9965, 0.9024, 0.9242, 0.9347, 0.7654, 0.7292, 0.9529, 0.7680, 0.8976,
        0.8554, 0.7467, 0.7983, 0.6962, 0.6720], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3099/4999 acc [ 0.832] time 193.03s
Reset trigger time to 0
new best (0.830441 --> 0.832000). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  3100/4999 loss: 0.7692 time 24.98s
Final training  3101/4999 loss: 0.7849 time 25.39s
Final training  3102/4999 loss: 0.7710 time 25.49s
Final training  3103/4999 loss: 0.7831 time 25.60s
Final training  3104/4999 loss: 0.7972 time 25.43s
Final training  3105/4999 loss: 0.7670 time 25.14s
Final training  3106/4999 loss: 0.7953 time 25.29s
Final training  3107/4999 loss: 0.7624 time 25.06s
Final training  3108/4999 loss: 0.7743 time 25.27s
Final training  3109/4999 loss: 0.7857 time 25.48s
Final training  3110/4999 loss: 0.7726 time 25.28s
Final training  3111/4999 loss: 0.8079 time 24.93s
Final training  3112/4999 loss: 0.7874 time 24.76s
Final training  3113/4999 loss: 0.7944 time 25.40s
Final training  3114/4999 loss: 0.8082 time 25.44s
Final training  3115/4999 loss: 0.8001 time 25.39s
Final training  3116/4999 loss: 0.7935 time 25.76s
Final training  3117/4999 loss: 0.7952 time 25.07s
Final training  3118/4999 loss: 0.8051 time 25.45s
Final training  3119/4999 loss: 0.9067 time 25.38s
Final training  3120/4999 loss: 0.8171 time 25.27s
Final training  3121/4999 loss: 0.8041 time 25.45s
Final training  3122/4999 loss: 0.8113 time 25.36s
Final training  3123/4999 loss: 0.7755 time 25.46s
Final training  3124/4999 loss: 0.7810 time 25.56s
Final training  3125/4999 loss: 0.7938 time 25.21s
Final training  3126/4999 loss: 0.7957 time 25.36s
Final training  3127/4999 loss: 0.7902 time 25.12s
Final training  3128/4999 loss: 0.8128 time 25.39s
Final training  3129/4999 loss: 0.7601 time 25.31s
Final training  3130/4999 loss: 0.8009 time 25.42s
Final training  3131/4999 loss: 0.8094 time 25.07s
Final training  3132/4999 loss: 0.7999 time 25.29s
Final training  3133/4999 loss: 0.7962 time 25.40s
Final training  3134/4999 loss: 0.7836 time 25.27s
Final training  3135/4999 loss: 0.7784 time 25.47s
Final training  3136/4999 loss: 0.8108 time 25.35s
Final training  3137/4999 loss: 0.7709 time 25.43s
Final training  3138/4999 loss: 0.7728 time 25.46s
Final training  3139/4999 loss: 0.7821 time 25.59s
Final training  3140/4999 loss: 0.8053 time 25.26s
Final training  3141/4999 loss: 0.8007 time 25.05s
Final training  3142/4999 loss: 0.7865 time 25.33s
Final training  3143/4999 loss: 0.7857 time 25.58s
Final training  3144/4999 loss: 0.7900 time 25.19s
Final training  3145/4999 loss: 0.8066 time 25.44s
Final training  3146/4999 loss: 0.7675 time 25.65s
Final training  3147/4999 loss: 0.7928 time 25.52s
Final training  3148/4999 loss: 0.8033 time 25.18s
Final training  3149/4999 loss: 0.7768 time 25.39s
Final training  3150/4999 loss: 0.7942 time 25.17s
Final training  3151/4999 loss: 0.7901 time 25.43s
Final training  3152/4999 loss: 0.7861 time 25.36s
Final training  3153/4999 loss: 0.7666 time 24.92s
Final training  3154/4999 loss: 0.7988 time 25.19s
Final training  3155/4999 loss: 0.8365 time 25.13s
Final training  3156/4999 loss: 0.7827 time 25.64s
Final training  3157/4999 loss: 0.7739 time 25.41s
Final training  3158/4999 loss: 0.7817 time 25.22s
Final training  3159/4999 loss: 0.7865 time 25.41s
Final training  3160/4999 loss: 0.8110 time 25.10s
Final training  3161/4999 loss: 0.8010 time 25.38s
Final training  3162/4999 loss: 0.7908 time 25.19s
Final training  3163/4999 loss: 0.7751 time 24.93s
Final training  3164/4999 loss: 0.8037 time 24.94s
Final training  3165/4999 loss: 0.7738 time 25.18s
Final training  3166/4999 loss: 0.8111 time 24.89s
Final training  3167/4999 loss: 0.7981 time 25.40s
Final training  3168/4999 loss: 0.7875 time 25.25s
Final training  3169/4999 loss: 0.8243 time 25.17s
Final training  3170/4999 loss: 0.8081 time 25.42s
Final training  3171/4999 loss: 0.7897 time 25.12s
Final training  3172/4999 loss: 0.8037 time 25.16s
Final training  3173/4999 loss: 0.7801 time 25.23s
Final training  3174/4999 loss: 0.7789 time 24.68s
Final training  3175/4999 loss: 0.7786 time 25.12s
Final training  3176/4999 loss: 0.8118 time 25.40s
Final training  3177/4999 loss: 0.7521 time 24.79s
Final training  3178/4999 loss: 0.8128 time 25.65s
Final training  3179/4999 loss: 0.7819 time 25.36s
Final training  3180/4999 loss: 0.7901 time 25.75s
Final training  3181/4999 loss: 0.7993 time 25.47s
Final training  3182/4999 loss: 0.7867 time 25.55s
Final training  3183/4999 loss: 0.7919 time 25.39s
Final training  3184/4999 loss: 0.7721 time 25.36s
Final training  3185/4999 loss: 0.7832 time 25.50s
Final training  3186/4999 loss: 0.7438 time 25.30s
Final training  3187/4999 loss: 0.8044 time 25.09s
Final training  3188/4999 loss: 0.7663 time 25.16s
Final training  3189/4999 loss: 0.7987 time 25.16s
Final training  3190/4999 loss: 0.7873 time 24.95s
Final training  3191/4999 loss: 0.7684 time 24.77s
Final training  3192/4999 loss: 0.7927 time 25.62s
Final training  3193/4999 loss: 0.8150 time 25.29s
Final training  3194/4999 loss: 0.7950 time 25.50s
Final training  3195/4999 loss: 0.7784 time 25.19s
Final training  3196/4999 loss: 0.7830 time 25.46s
Final training  3197/4999 loss: 0.7969 time 25.31s
Final training  3198/4999 loss: 0.7917 time 25.24s
Final training  3199/4999 loss: 0.7926 time 25.38s
Dice accuracy for each class:  (tensor([0.9950, 0.8992, 0.9437, 0.9430, 0.7456, 0.7145, 0.8886, 0.7450, 0.9041,
        0.8498, 0.7467, 0.7704, 0.6851, 0.6618], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3199/4999 acc [ 0.822] time 194.31s
trigger times: 1
Final training  3200/4999 loss: 0.7931 time 25.40s
Final training  3201/4999 loss: 0.7735 time 25.39s
Final training  3202/4999 loss: 0.7858 time 25.18s
Final training  3203/4999 loss: 0.7847 time 25.48s
Final training  3204/4999 loss: 0.7718 time 25.43s
Final training  3205/4999 loss: 0.8046 time 25.39s
Final training  3206/4999 loss: 0.7848 time 25.41s
Final training  3207/4999 loss: 0.8002 time 25.16s
Final training  3208/4999 loss: 0.7767 time 25.44s
Final training  3209/4999 loss: 0.7936 time 25.29s
Final training  3210/4999 loss: 0.7666 time 25.82s
Final training  3211/4999 loss: 0.8092 time 25.12s
Final training  3212/4999 loss: 0.7984 time 25.59s
Final training  3213/4999 loss: 0.7914 time 25.56s
Final training  3214/4999 loss: 0.7678 time 24.96s
Final training  3215/4999 loss: 0.7922 time 25.35s
Final training  3216/4999 loss: 0.7625 time 25.19s
Final training  3217/4999 loss: 0.7931 time 25.41s
Final training  3218/4999 loss: 0.7734 time 25.63s
Final training  3219/4999 loss: 0.8003 time 25.21s
Final training  3220/4999 loss: 0.7919 time 25.20s
Final training  3221/4999 loss: 0.7870 time 25.31s
Final training  3222/4999 loss: 0.7837 time 25.26s
Final training  3223/4999 loss: 0.7824 time 25.38s
Final training  3224/4999 loss: 0.7975 time 25.48s
Final training  3225/4999 loss: 0.7772 time 25.64s
Final training  3226/4999 loss: 0.7871 time 25.31s
Final training  3227/4999 loss: 0.7999 time 25.58s
Final training  3228/4999 loss: 0.7772 time 25.34s
Final training  3229/4999 loss: 0.7912 time 25.32s
Final training  3230/4999 loss: 0.7978 time 24.86s
Final training  3231/4999 loss: 0.7749 time 25.50s
Final training  3232/4999 loss: 0.8079 time 25.32s
Final training  3233/4999 loss: 0.8170 time 25.48s
Final training  3234/4999 loss: 0.7882 time 25.72s
Final training  3235/4999 loss: 0.8004 time 25.44s
Final training  3236/4999 loss: 0.7851 time 25.61s
Final training  3237/4999 loss: 0.7812 time 24.93s
Final training  3238/4999 loss: 0.7879 time 25.13s
Final training  3239/4999 loss: 0.7843 time 25.25s
Final training  3240/4999 loss: 0.7858 time 25.71s
Final training  3241/4999 loss: 0.7714 time 25.42s
Final training  3242/4999 loss: 0.7970 time 25.38s
Final training  3243/4999 loss: 0.7807 time 25.66s
Final training  3244/4999 loss: 0.7751 time 25.29s
Final training  3245/4999 loss: 0.7910 time 25.26s
Final training  3246/4999 loss: 0.7554 time 25.25s
Final training  3247/4999 loss: 0.7720 time 25.27s
Final training  3248/4999 loss: 0.7790 time 25.43s
Final training  3249/4999 loss: 0.7806 time 25.31s
Final training  3250/4999 loss: 0.7878 time 25.15s
Final training  3251/4999 loss: 0.7741 time 25.24s
Final training  3252/4999 loss: 0.7771 time 25.47s
Final training  3253/4999 loss: 0.7690 time 25.06s
Final training  3254/4999 loss: 0.7798 time 25.02s
Final training  3255/4999 loss: 0.7885 time 25.11s
Final training  3256/4999 loss: 0.7950 time 25.42s
Final training  3257/4999 loss: 0.7676 time 25.31s
Final training  3258/4999 loss: 0.7784 time 25.16s
Final training  3259/4999 loss: 0.8120 time 25.25s
Final training  3260/4999 loss: 0.7863 time 25.58s
Final training  3261/4999 loss: 0.7930 time 24.85s
Final training  3262/4999 loss: 0.7847 time 25.47s
Final training  3263/4999 loss: 0.7751 time 25.46s
Final training  3264/4999 loss: 0.7810 time 25.26s
Final training  3265/4999 loss: 0.7692 time 25.34s
Final training  3266/4999 loss: 0.7980 time 25.49s
Final training  3267/4999 loss: 0.7865 time 25.46s
Final training  3268/4999 loss: 0.7981 time 24.76s
Final training  3269/4999 loss: 0.7880 time 24.68s
Final training  3270/4999 loss: 0.7824 time 24.68s
Final training  3271/4999 loss: 0.7802 time 25.07s
Final training  3272/4999 loss: 0.8029 time 24.94s
Final training  3273/4999 loss: 0.8021 time 25.41s
Final training  3274/4999 loss: 0.8081 time 25.26s
Final training  3275/4999 loss: 0.7626 time 24.88s
Final training  3276/4999 loss: 0.7804 time 25.50s
Final training  3277/4999 loss: 0.7870 time 25.16s
Final training  3278/4999 loss: 0.7907 time 25.26s
Final training  3279/4999 loss: 0.7961 time 25.51s
Final training  3280/4999 loss: 0.7796 time 25.25s
Final training  3281/4999 loss: 0.7856 time 25.68s
Final training  3282/4999 loss: 0.7888 time 25.46s
Final training  3283/4999 loss: 0.7725 time 25.22s
Final training  3284/4999 loss: 0.7986 time 25.22s
Final training  3285/4999 loss: 0.7785 time 25.16s
Final training  3286/4999 loss: 0.7689 time 25.38s
Final training  3287/4999 loss: 0.7928 time 25.32s
Final training  3288/4999 loss: 0.7799 time 25.36s
Final training  3289/4999 loss: 0.7596 time 25.06s
Final training  3290/4999 loss: 0.7720 time 25.08s
Final training  3291/4999 loss: 0.7709 time 25.25s
Final training  3292/4999 loss: 0.7821 time 25.38s
Final training  3293/4999 loss: 0.7739 time 25.45s
Final training  3294/4999 loss: 0.7748 time 25.17s
Final training  3295/4999 loss: 0.7683 time 25.01s
Final training  3296/4999 loss: 0.7986 time 25.37s
Final training  3297/4999 loss: 0.7766 time 25.33s
Final training  3298/4999 loss: 0.7883 time 25.67s
Final training  3299/4999 loss: 0.7863 time 25.15s
Dice accuracy for each class:  (tensor([0.9963, 0.8989, 0.9438, 0.9435, 0.7802, 0.6966, 0.9496, 0.7439, 0.9047,
        0.8356, 0.7434, 0.7996, 0.6903, 0.6892], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3299/4999 acc [ 0.830] time 192.93s
trigger times: 2
Final training  3300/4999 loss: 0.7885 time 25.23s
Final training  3301/4999 loss: 0.7876 time 25.44s
Final training  3302/4999 loss: 0.7766 time 25.41s
Final training  3303/4999 loss: 0.7610 time 25.61s
Final training  3304/4999 loss: 0.7884 time 25.34s
Final training  3305/4999 loss: 0.7796 time 24.96s
Final training  3306/4999 loss: 0.7870 time 25.57s
Final training  3307/4999 loss: 0.8096 time 25.36s
Final training  3308/4999 loss: 0.7890 time 25.26s
Final training  3309/4999 loss: 0.7866 time 25.50s
Final training  3310/4999 loss: 0.7827 time 25.26s
Final training  3311/4999 loss: 0.7820 time 25.04s
Final training  3312/4999 loss: 0.8048 time 25.41s
Final training  3313/4999 loss: 0.8020 time 25.52s
Final training  3314/4999 loss: 0.7737 time 25.52s
Final training  3315/4999 loss: 0.8013 time 25.64s
Final training  3316/4999 loss: 0.7836 time 25.49s
Final training  3317/4999 loss: 0.7858 time 25.25s
Final training  3318/4999 loss: 0.7807 time 25.78s
Final training  3319/4999 loss: 0.7997 time 25.36s
Final training  3320/4999 loss: 0.8116 time 25.45s
Final training  3321/4999 loss: 0.7820 time 25.46s
Final training  3322/4999 loss: 0.7983 time 25.53s
Final training  3323/4999 loss: 0.7891 time 25.05s
Final training  3324/4999 loss: 0.7898 time 25.39s
Final training  3325/4999 loss: 0.7790 time 25.51s
Final training  3326/4999 loss: 0.7890 time 25.67s
Final training  3327/4999 loss: 0.7622 time 25.35s
Final training  3328/4999 loss: 0.8081 time 25.66s
Final training  3329/4999 loss: 0.7896 time 25.35s
Final training  3330/4999 loss: 0.7813 time 25.64s
Final training  3331/4999 loss: 0.7973 time 25.47s
Final training  3332/4999 loss: 0.7758 time 25.34s
Final training  3333/4999 loss: 0.7788 time 25.47s
Final training  3334/4999 loss: 0.7902 time 25.35s
Final training  3335/4999 loss: 0.8188 time 25.38s
Final training  3336/4999 loss: 0.8049 time 25.65s
Final training  3337/4999 loss: 0.7785 time 25.45s
Final training  3338/4999 loss: 0.7850 time 25.68s
Final training  3339/4999 loss: 0.7895 time 25.57s
Final training  3340/4999 loss: 0.7667 time 25.65s
Final training  3341/4999 loss: 0.7800 time 25.23s
Final training  3342/4999 loss: 0.7789 time 25.10s
Final training  3343/4999 loss: 0.7915 time 25.35s
Final training  3344/4999 loss: 0.7810 time 25.45s
Final training  3345/4999 loss: 0.7782 time 25.44s
Final training  3346/4999 loss: 0.7550 time 25.57s
Final training  3347/4999 loss: 0.8049 time 25.55s
Final training  3348/4999 loss: 0.8053 time 25.31s
Final training  3349/4999 loss: 0.7675 time 25.62s
Final training  3350/4999 loss: 0.7912 time 25.50s
Final training  3351/4999 loss: 0.7998 time 25.33s
Final training  3352/4999 loss: 0.7908 time 25.47s
Final training  3353/4999 loss: 0.7820 time 25.32s
Final training  3354/4999 loss: 0.7832 time 25.38s
Final training  3355/4999 loss: 0.7881 time 25.51s
Final training  3356/4999 loss: 0.7764 time 25.48s
Final training  3357/4999 loss: 0.7666 time 25.55s
Final training  3358/4999 loss: 0.7882 time 25.37s
Final training  3359/4999 loss: 0.7931 time 25.53s
Final training  3360/4999 loss: 0.7659 time 25.79s
Final training  3361/4999 loss: 0.7915 time 25.82s
Final training  3362/4999 loss: 0.7905 time 25.23s
Final training  3363/4999 loss: 0.7901 time 25.65s
Final training  3364/4999 loss: 0.7692 time 25.03s
Final training  3365/4999 loss: 0.7860 time 25.66s
Final training  3366/4999 loss: 0.7690 time 25.11s
Final training  3367/4999 loss: 0.7773 time 25.15s
Final training  3368/4999 loss: 0.7892 time 25.11s
Final training  3369/4999 loss: 0.7988 time 25.00s
Final training  3370/4999 loss: 0.7835 time 25.17s
Final training  3371/4999 loss: 0.7751 time 25.41s
Final training  3372/4999 loss: 0.7712 time 25.42s
Final training  3373/4999 loss: 0.7753 time 24.96s
Final training  3374/4999 loss: 0.7618 time 24.78s
Final training  3375/4999 loss: 0.7845 time 25.50s
Final training  3376/4999 loss: 0.7894 time 25.56s
Final training  3377/4999 loss: 0.7897 time 25.46s
Final training  3378/4999 loss: 0.7854 time 25.51s
Final training  3379/4999 loss: 0.7949 time 25.34s
Final training  3380/4999 loss: 0.7835 time 25.40s
Final training  3381/4999 loss: 0.7742 time 25.67s
Final training  3382/4999 loss: 0.7792 time 25.84s
Final training  3383/4999 loss: 0.8023 time 25.36s
Final training  3384/4999 loss: 0.7978 time 25.37s
Final training  3385/4999 loss: 0.7751 time 25.17s
Final training  3386/4999 loss: 0.7852 time 25.45s
Final training  3387/4999 loss: 0.7825 time 25.42s
Final training  3388/4999 loss: 0.7672 time 25.36s
Final training  3389/4999 loss: 0.7837 time 25.49s
Final training  3390/4999 loss: 0.7934 time 25.36s
Final training  3391/4999 loss: 0.7658 time 25.39s
Final training  3392/4999 loss: 0.7690 time 25.31s
Final training  3393/4999 loss: 0.7787 time 25.30s
Final training  3394/4999 loss: 0.7656 time 25.32s
Final training  3395/4999 loss: 0.7884 time 25.44s
Final training  3396/4999 loss: 0.7886 time 25.39s
Final training  3397/4999 loss: 0.7859 time 25.59s
Final training  3398/4999 loss: 0.7939 time 25.32s
Final training  3399/4999 loss: 0.8009 time 25.44s
Dice accuracy for each class:  (tensor([0.9964, 0.8906, 0.9441, 0.9420, 0.7785, 0.7083, 0.9541, 0.7330, 0.9072,
        0.8422, 0.7479, 0.7979, 0.6812, 0.6702], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3399/4999 acc [ 0.829] time 192.71s
trigger times: 3
Final training  3400/4999 loss: 0.7850 time 25.61s
Final training  3401/4999 loss: 0.8055 time 25.49s
Final training  3402/4999 loss: 0.7686 time 25.28s
Final training  3403/4999 loss: 0.8028 time 25.60s
Final training  3404/4999 loss: 0.7755 time 25.64s
Final training  3405/4999 loss: 0.7592 time 25.69s
Final training  3406/4999 loss: 0.7529 time 25.55s
Final training  3407/4999 loss: 0.8015 time 25.17s
Final training  3408/4999 loss: 0.7446 time 25.48s
Final training  3409/4999 loss: 0.7694 time 25.24s
Final training  3410/4999 loss: 0.7767 time 25.46s
Final training  3411/4999 loss: 0.7830 time 25.45s
Final training  3412/4999 loss: 0.7723 time 25.44s
Final training  3413/4999 loss: 0.7943 time 25.39s
Final training  3414/4999 loss: 0.7625 time 25.41s
Final training  3415/4999 loss: 0.7901 time 25.43s
Final training  3416/4999 loss: 0.7969 time 25.52s
Final training  3417/4999 loss: 0.7860 time 25.82s
Final training  3418/4999 loss: 0.7720 time 25.22s
Final training  3419/4999 loss: 0.7782 time 25.57s
Final training  3420/4999 loss: 0.7793 time 25.17s
Final training  3421/4999 loss: 0.7865 time 25.14s
Final training  3422/4999 loss: 0.7998 time 25.51s
Final training  3423/4999 loss: 0.8017 time 25.30s
Final training  3424/4999 loss: 0.7837 time 24.66s
Final training  3425/4999 loss: 0.7678 time 25.12s
Final training  3426/4999 loss: 0.7968 time 25.23s
Final training  3427/4999 loss: 0.7911 time 25.20s
Final training  3428/4999 loss: 0.7792 time 25.39s
Final training  3429/4999 loss: 0.7753 time 25.48s
Final training  3430/4999 loss: 0.7804 time 25.35s
Final training  3431/4999 loss: 0.8177 time 25.07s
Final training  3432/4999 loss: 0.7883 time 25.71s
Final training  3433/4999 loss: 0.7755 time 25.73s
Final training  3434/4999 loss: 0.7963 time 25.51s
Final training  3435/4999 loss: 0.7724 time 24.82s
Final training  3436/4999 loss: 0.7882 time 25.56s
Final training  3437/4999 loss: 0.7793 time 25.01s
Final training  3438/4999 loss: 0.7723 time 25.36s
Final training  3439/4999 loss: 0.7843 time 25.51s
Final training  3440/4999 loss: 0.7765 time 25.24s
Final training  3441/4999 loss: 0.7885 time 25.34s
Final training  3442/4999 loss: 0.7903 time 25.10s
Final training  3443/4999 loss: 0.7743 time 25.49s
Final training  3444/4999 loss: 0.7452 time 25.12s
Final training  3445/4999 loss: 0.7940 time 25.34s
Final training  3446/4999 loss: 0.7880 time 25.44s
Final training  3447/4999 loss: 0.7721 time 25.49s
Final training  3448/4999 loss: 0.7854 time 25.35s
Final training  3449/4999 loss: 0.7687 time 25.58s
Final training  3450/4999 loss: 0.7791 time 25.67s
Final training  3451/4999 loss: 0.7992 time 25.49s
Final training  3452/4999 loss: 0.7988 time 25.66s
Final training  3453/4999 loss: 0.7971 time 25.34s
Final training  3454/4999 loss: 0.8175 time 25.32s
Final training  3455/4999 loss: 0.7818 time 25.59s
Final training  3456/4999 loss: 0.7858 time 25.58s
Final training  3457/4999 loss: 0.7799 time 25.77s
Final training  3458/4999 loss: 0.8041 time 25.48s
Final training  3459/4999 loss: 0.7869 time 25.33s
Final training  3460/4999 loss: 0.7686 time 25.45s
Final training  3461/4999 loss: 0.7750 time 25.15s
Final training  3462/4999 loss: 0.7877 time 25.39s
Final training  3463/4999 loss: 0.8040 time 25.06s
Final training  3464/4999 loss: 0.7856 time 25.63s
Final training  3465/4999 loss: 0.8131 time 25.05s
Final training  3466/4999 loss: 0.7721 time 25.57s
Final training  3467/4999 loss: 0.7833 time 25.41s
Final training  3468/4999 loss: 0.7780 time 25.28s
Final training  3469/4999 loss: 0.7965 time 25.13s
Final training  3470/4999 loss: 0.8017 time 24.98s
Final training  3471/4999 loss: 0.7581 time 24.87s
Final training  3472/4999 loss: 0.7959 time 24.44s
Final training  3473/4999 loss: 0.7945 time 24.58s
Final training  3474/4999 loss: 0.7861 time 25.17s
Final training  3475/4999 loss: 0.8103 time 25.09s
Final training  3476/4999 loss: 0.7853 time 25.26s
Final training  3477/4999 loss: 0.7906 time 25.42s
Final training  3478/4999 loss: 0.7768 time 25.19s
Final training  3479/4999 loss: 0.7929 time 25.42s
Final training  3480/4999 loss: 0.7937 time 25.40s
Final training  3481/4999 loss: 0.7885 time 25.35s
Final training  3482/4999 loss: 0.7706 time 25.37s
Final training  3483/4999 loss: 0.7830 time 25.43s
Final training  3484/4999 loss: 0.7823 time 25.51s
Final training  3485/4999 loss: 0.7562 time 24.95s
Final training  3486/4999 loss: 0.7884 time 25.32s
Final training  3487/4999 loss: 0.7770 time 25.36s
Final training  3488/4999 loss: 0.8047 time 25.36s
Final training  3489/4999 loss: 0.7687 time 25.46s
Final training  3490/4999 loss: 0.7865 time 25.25s
Final training  3491/4999 loss: 0.8038 time 25.31s
Final training  3492/4999 loss: 0.8014 time 25.21s
Final training  3493/4999 loss: 0.7640 time 25.54s
Final training  3494/4999 loss: 0.7856 time 25.49s
Final training  3495/4999 loss: 0.7580 time 25.52s
Final training  3496/4999 loss: 0.7829 time 25.60s
Final training  3497/4999 loss: 0.7931 time 25.04s
Final training  3498/4999 loss: 0.8093 time 25.10s
Final training  3499/4999 loss: 0.7691 time 25.56s
Dice accuracy for each class:  (tensor([0.9967, 0.9240, 0.9431, 0.9434, 0.7718, 0.7183, 0.9638, 0.7377, 0.9030,
        0.8464, 0.7546, 0.7834, 0.6794, 0.6480], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3499/4999 acc [ 0.830] time 192.84s
trigger times: 4
Final training  3500/4999 loss: 0.7882 time 25.37s
Final training  3501/4999 loss: 0.8029 time 25.17s
Final training  3502/4999 loss: 0.7856 time 25.51s
Final training  3503/4999 loss: 0.7927 time 25.29s
Final training  3504/4999 loss: 0.7767 time 25.52s
Final training  3505/4999 loss: 0.7967 time 25.22s
Final training  3506/4999 loss: 0.8000 time 25.24s
Final training  3507/4999 loss: 0.7798 time 25.49s
Final training  3508/4999 loss: 0.7616 time 25.46s
Final training  3509/4999 loss: 0.7874 time 25.61s
Final training  3510/4999 loss: 0.7669 time 25.39s
Final training  3511/4999 loss: 0.7753 time 25.44s
Final training  3512/4999 loss: 0.7803 time 25.57s
Final training  3513/4999 loss: 0.8017 time 25.38s
Final training  3514/4999 loss: 0.7789 time 25.57s
Final training  3515/4999 loss: 0.7808 time 25.54s
Final training  3516/4999 loss: 0.7774 time 25.41s
Final training  3517/4999 loss: 0.7708 time 25.31s
Final training  3518/4999 loss: 0.7798 time 25.54s
Final training  3519/4999 loss: 0.7738 time 25.72s
Final training  3520/4999 loss: 0.7826 time 25.32s
Final training  3521/4999 loss: 0.7942 time 25.46s
Final training  3522/4999 loss: 0.7903 time 25.13s
Final training  3523/4999 loss: 0.7630 time 25.30s
Final training  3524/4999 loss: 0.7801 time 25.33s
Final training  3525/4999 loss: 0.7717 time 25.43s
Final training  3526/4999 loss: 0.7898 time 25.22s
Final training  3527/4999 loss: 0.7872 time 25.17s
Final training  3528/4999 loss: 0.7701 time 25.57s
Final training  3529/4999 loss: 0.8109 time 25.10s
Final training  3530/4999 loss: 0.7869 time 25.15s
Final training  3531/4999 loss: 0.7862 time 24.95s
Final training  3532/4999 loss: 0.7674 time 24.50s
Final training  3533/4999 loss: 0.7702 time 24.90s
Final training  3534/4999 loss: 0.8063 time 24.98s
Final training  3535/4999 loss: 0.7742 time 25.68s
Final training  3536/4999 loss: 0.7958 time 25.39s
Final training  3537/4999 loss: 0.7927 time 25.23s
Final training  3538/4999 loss: 0.7582 time 25.45s
Final training  3539/4999 loss: 0.7687 time 25.68s
Final training  3540/4999 loss: 0.7687 time 25.29s
Final training  3541/4999 loss: 0.7836 time 25.38s
Final training  3542/4999 loss: 0.7774 time 25.56s
Final training  3543/4999 loss: 0.7872 time 25.23s
Final training  3544/4999 loss: 0.7837 time 25.63s
Final training  3545/4999 loss: 0.7753 time 25.21s
Final training  3546/4999 loss: 0.7785 time 25.23s
Final training  3547/4999 loss: 0.7720 time 25.42s
Final training  3548/4999 loss: 0.7827 time 25.02s
Final training  3549/4999 loss: 0.7985 time 25.50s
Final training  3550/4999 loss: 0.7999 time 25.20s
Final training  3551/4999 loss: 0.7467 time 25.60s
Final training  3552/4999 loss: 0.8021 time 25.41s
Final training  3553/4999 loss: 0.7891 time 25.17s
Final training  3554/4999 loss: 0.7844 time 25.49s
Final training  3555/4999 loss: 0.7796 time 25.11s
Final training  3556/4999 loss: 0.7709 time 25.52s
Final training  3557/4999 loss: 0.7699 time 25.32s
Final training  3558/4999 loss: 0.7847 time 25.43s
Final training  3559/4999 loss: 0.7791 time 25.27s
Final training  3560/4999 loss: 0.7764 time 25.86s
Final training  3561/4999 loss: 0.7865 time 25.72s
Final training  3562/4999 loss: 0.8020 time 25.31s
Final training  3563/4999 loss: 0.7775 time 25.39s
Final training  3564/4999 loss: 0.7534 time 25.49s
Final training  3565/4999 loss: 0.7893 time 25.06s
Final training  3566/4999 loss: 0.7705 time 25.32s
Final training  3567/4999 loss: 0.7883 time 25.32s
Final training  3568/4999 loss: 0.7771 time 25.34s
Final training  3569/4999 loss: 0.7538 time 24.96s
Final training  3570/4999 loss: 0.7867 time 25.00s
Final training  3571/4999 loss: 0.7879 time 25.13s
Final training  3572/4999 loss: 0.7687 time 25.47s
Final training  3573/4999 loss: 0.7611 time 25.46s
Final training  3574/4999 loss: 0.7949 time 25.13s
Final training  3575/4999 loss: 0.7952 time 25.58s
Final training  3576/4999 loss: 0.7807 time 25.41s
Final training  3577/4999 loss: 0.7886 time 25.71s
Final training  3578/4999 loss: 0.7864 time 25.51s
Final training  3579/4999 loss: 0.7772 time 25.28s
Final training  3580/4999 loss: 0.7910 time 25.81s
Final training  3581/4999 loss: 0.7961 time 24.69s
Final training  3582/4999 loss: 0.8098 time 25.60s
Final training  3583/4999 loss: 0.7844 time 25.46s
Final training  3584/4999 loss: 0.7824 time 25.63s
Final training  3585/4999 loss: 0.7845 time 25.28s
Final training  3586/4999 loss: 0.7850 time 25.26s
Final training  3587/4999 loss: 0.7677 time 25.41s
Final training  3588/4999 loss: 0.8028 time 25.21s
Final training  3589/4999 loss: 0.7921 time 25.46s
Final training  3590/4999 loss: 0.7736 time 25.05s
Final training  3591/4999 loss: 0.7531 time 25.22s
Final training  3592/4999 loss: 0.7739 time 25.46s
Final training  3593/4999 loss: 0.7773 time 25.23s
Final training  3594/4999 loss: 0.7997 time 25.17s
Final training  3595/4999 loss: 0.7838 time 24.86s
Final training  3596/4999 loss: 0.7764 time 25.36s
Final training  3597/4999 loss: 0.7784 time 25.82s
Final training  3598/4999 loss: 0.7968 time 25.38s
Final training  3599/4999 loss: 0.7732 time 25.20s
Dice accuracy for each class:  (tensor([0.9964, 0.8880, 0.9431, 0.9437, 0.7752, 0.7234, 0.9489, 0.7819, 0.9025,
        0.8300, 0.7468, 0.7812, 0.6757, 0.6456], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3599/4999 acc [ 0.828] time 194.21s
trigger times: 5
Final training  3600/4999 loss: 0.8029 time 25.89s
Final training  3601/4999 loss: 0.7560 time 25.25s
Final training  3602/4999 loss: 0.7952 time 25.05s
Final training  3603/4999 loss: 0.7727 time 25.74s
Final training  3604/4999 loss: 0.7970 time 25.25s
Final training  3605/4999 loss: 0.7882 time 25.35s
Final training  3606/4999 loss: 0.7701 time 25.44s
Final training  3607/4999 loss: 0.7740 time 25.47s
Final training  3608/4999 loss: 0.7758 time 25.36s
Final training  3609/4999 loss: 0.7520 time 25.31s
Final training  3610/4999 loss: 0.7840 time 25.52s
Final training  3611/4999 loss: 0.7795 time 25.29s
Final training  3612/4999 loss: 0.7610 time 25.03s
Final training  3613/4999 loss: 0.7415 time 25.37s
Final training  3614/4999 loss: 0.7933 time 25.37s
Final training  3615/4999 loss: 0.8031 time 25.34s
Final training  3616/4999 loss: 0.8030 time 25.13s
Final training  3617/4999 loss: 0.7613 time 25.47s
Final training  3618/4999 loss: 0.7754 time 25.52s
Final training  3619/4999 loss: 0.7776 time 25.64s
Final training  3620/4999 loss: 0.7997 time 25.37s
Final training  3621/4999 loss: 0.7931 time 25.33s
Final training  3622/4999 loss: 0.7841 time 25.48s
Final training  3623/4999 loss: 0.7645 time 25.32s
Final training  3624/4999 loss: 0.7913 time 25.94s
Final training  3625/4999 loss: 0.7726 time 25.49s
Final training  3626/4999 loss: 0.7790 time 25.16s
Final training  3627/4999 loss: 0.7768 time 25.10s
Final training  3628/4999 loss: 0.7779 time 25.15s
Final training  3629/4999 loss: 0.7714 time 25.09s
Final training  3630/4999 loss: 0.7800 time 25.10s
Final training  3631/4999 loss: 0.7941 time 25.36s
Final training  3632/4999 loss: 0.7803 time 25.30s
Final training  3633/4999 loss: 0.7739 time 25.26s
Final training  3634/4999 loss: 0.7937 time 25.23s
Final training  3635/4999 loss: 0.7718 time 25.12s
Final training  3636/4999 loss: 0.7921 time 24.83s
Final training  3637/4999 loss: 0.7960 time 24.90s
Final training  3638/4999 loss: 0.8134 time 25.57s
Final training  3639/4999 loss: 0.7949 time 25.29s
Final training  3640/4999 loss: 0.8075 time 25.61s
Final training  3641/4999 loss: 0.7996 time 25.32s
Final training  3642/4999 loss: 0.7887 time 25.35s
Final training  3643/4999 loss: 0.7606 time 25.27s
Final training  3644/4999 loss: 0.7949 time 25.19s
Final training  3645/4999 loss: 0.7722 time 24.87s
Final training  3646/4999 loss: 0.7778 time 24.94s
Final training  3647/4999 loss: 0.7633 time 24.86s
Final training  3648/4999 loss: 0.7773 time 25.10s
Final training  3649/4999 loss: 0.7956 time 25.16s
Final training  3650/4999 loss: 0.8166 time 25.25s
Final training  3651/4999 loss: 0.8013 time 25.51s
Final training  3652/4999 loss: 0.7765 time 25.25s
Final training  3653/4999 loss: 0.7595 time 25.41s
Final training  3654/4999 loss: 0.7888 time 25.65s
Final training  3655/4999 loss: 0.7799 time 25.50s
Final training  3656/4999 loss: 0.7858 time 25.45s
Final training  3657/4999 loss: 0.7598 time 25.43s
Final training  3658/4999 loss: 0.7992 time 25.01s
Final training  3659/4999 loss: 0.7778 time 25.10s
Final training  3660/4999 loss: 0.7760 time 25.65s
Final training  3661/4999 loss: 0.7670 time 25.16s
Final training  3662/4999 loss: 0.7558 time 25.41s
Final training  3663/4999 loss: 0.7666 time 25.52s
Final training  3664/4999 loss: 0.7720 time 25.13s
Final training  3665/4999 loss: 0.7813 time 25.61s
Final training  3666/4999 loss: 0.7771 time 24.96s
Final training  3667/4999 loss: 0.7913 time 25.64s
Final training  3668/4999 loss: 0.7937 time 24.72s
Final training  3669/4999 loss: 0.7758 time 24.68s
Final training  3670/4999 loss: 0.7784 time 25.38s
Final training  3671/4999 loss: 0.7729 time 25.57s
Final training  3672/4999 loss: 0.7875 time 25.28s
Final training  3673/4999 loss: 0.7836 time 25.27s
Final training  3674/4999 loss: 0.7835 time 25.49s
Final training  3675/4999 loss: 0.7521 time 25.10s
Final training  3676/4999 loss: 0.7669 time 25.56s
Final training  3677/4999 loss: 0.7847 time 25.32s
Final training  3678/4999 loss: 0.7863 time 25.40s
Final training  3679/4999 loss: 0.7716 time 25.46s
Final training  3680/4999 loss: 0.7713 time 25.29s
Final training  3681/4999 loss: 0.8038 time 25.75s
Final training  3682/4999 loss: 0.7929 time 25.25s
Final training  3683/4999 loss: 0.7946 time 25.66s
Final training  3684/4999 loss: 0.8066 time 25.28s
Final training  3685/4999 loss: 0.7812 time 25.61s
Final training  3686/4999 loss: 0.7804 time 25.45s
Final training  3687/4999 loss: 0.7728 time 25.37s
Final training  3688/4999 loss: 0.7855 time 25.33s
Final training  3689/4999 loss: 0.7855 time 25.52s
Final training  3690/4999 loss: 0.7694 time 25.93s
Final training  3691/4999 loss: 0.7800 time 25.41s
Final training  3692/4999 loss: 0.7935 time 25.38s
Final training  3693/4999 loss: 0.7826 time 25.54s
Final training  3694/4999 loss: 0.7737 time 25.18s
Final training  3695/4999 loss: 0.7950 time 25.35s
Final training  3696/4999 loss: 0.7992 time 25.56s
Final training  3697/4999 loss: 0.7586 time 25.22s
Final training  3698/4999 loss: 0.7791 time 25.36s
Final training  3699/4999 loss: 0.7674 time 25.49s
Dice accuracy for each class:  (tensor([0.9965, 0.9015, 0.9426, 0.9421, 0.7663, 0.7159, 0.9523, 0.7792, 0.8979,
        0.8349, 0.7529, 0.8003, 0.6825, 0.6619], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3699/4999 acc [ 0.831] time 193.26s
trigger times: 6
Final training  3700/4999 loss: 0.7774 time 24.92s
Final training  3701/4999 loss: 0.7906 time 25.25s
Final training  3702/4999 loss: 0.7831 time 25.16s
Final training  3703/4999 loss: 0.7876 time 25.17s
Final training  3704/4999 loss: 0.8009 time 25.34s
Final training  3705/4999 loss: 0.7958 time 25.27s
Final training  3706/4999 loss: 0.7882 time 25.26s
Final training  3707/4999 loss: 0.7923 time 25.13s
Final training  3708/4999 loss: 0.7920 time 25.52s
Final training  3709/4999 loss: 0.7688 time 25.44s
Final training  3710/4999 loss: 0.7775 time 25.30s
Final training  3711/4999 loss: 0.7696 time 25.33s
Final training  3712/4999 loss: 0.7568 time 25.61s
Final training  3713/4999 loss: 0.8031 time 25.36s
Final training  3714/4999 loss: 0.7648 time 25.28s
Final training  3715/4999 loss: 0.7803 time 25.08s
Final training  3716/4999 loss: 0.7972 time 25.27s
Final training  3717/4999 loss: 0.7902 time 25.51s
Final training  3718/4999 loss: 0.7920 time 25.43s
Final training  3719/4999 loss: 0.7586 time 25.48s
Final training  3720/4999 loss: 0.7698 time 25.37s
Final training  3721/4999 loss: 0.7702 time 25.33s
Final training  3722/4999 loss: 0.8068 time 25.59s
Final training  3723/4999 loss: 0.7969 time 25.45s
Final training  3724/4999 loss: 0.7627 time 25.63s
Final training  3725/4999 loss: 0.7950 time 25.46s
Final training  3726/4999 loss: 0.8138 time 25.15s
Final training  3727/4999 loss: 0.7860 time 25.44s
Final training  3728/4999 loss: 0.7708 time 25.45s
Final training  3729/4999 loss: 0.7898 time 25.39s
Final training  3730/4999 loss: 0.7928 time 25.42s
Final training  3731/4999 loss: 0.7880 time 25.37s
Final training  3732/4999 loss: 0.7815 time 25.73s
Final training  3733/4999 loss: 0.7673 time 25.19s
Final training  3734/4999 loss: 0.7731 time 25.27s
Final training  3735/4999 loss: 0.7813 time 25.31s
Final training  3736/4999 loss: 0.7971 time 25.26s
Final training  3737/4999 loss: 0.7973 time 25.58s
Final training  3738/4999 loss: 0.7763 time 25.25s
Final training  3739/4999 loss: 0.7891 time 25.59s
Final training  3740/4999 loss: 0.7945 time 25.76s
Final training  3741/4999 loss: 0.7753 time 25.53s
Final training  3742/4999 loss: 0.7579 time 25.80s
Final training  3743/4999 loss: 0.7654 time 25.47s
Final training  3744/4999 loss: 0.7738 time 25.61s
Final training  3745/4999 loss: 0.7776 time 24.91s
Final training  3746/4999 loss: 0.7659 time 25.26s
Final training  3747/4999 loss: 0.7778 time 25.44s
Final training  3748/4999 loss: 0.7987 time 25.40s
Final training  3749/4999 loss: 0.7693 time 25.07s
Final training  3750/4999 loss: 0.7943 time 25.18s
Final training  3751/4999 loss: 0.7973 time 24.82s
Final training  3752/4999 loss: 0.7818 time 24.86s
Final training  3753/4999 loss: 0.7674 time 25.45s
Final training  3754/4999 loss: 0.7825 time 25.53s
Final training  3755/4999 loss: 0.7542 time 25.55s
Final training  3756/4999 loss: 0.7789 time 25.64s
Final training  3757/4999 loss: 0.7861 time 25.56s
Final training  3758/4999 loss: 0.7653 time 25.47s
Final training  3759/4999 loss: 0.7964 time 25.41s
Final training  3760/4999 loss: 0.7666 time 25.43s
Final training  3761/4999 loss: 0.7789 time 25.32s
Final training  3762/4999 loss: 0.7875 time 25.29s
Final training  3763/4999 loss: 0.7881 time 25.36s
Final training  3764/4999 loss: 0.7888 time 24.85s
Final training  3765/4999 loss: 0.7547 time 25.23s
Final training  3766/4999 loss: 0.7879 time 25.37s
Final training  3767/4999 loss: 0.8005 time 24.86s
Final training  3768/4999 loss: 0.7759 time 25.02s
Final training  3769/4999 loss: 0.8086 time 25.49s
Final training  3770/4999 loss: 0.7679 time 25.45s
Final training  3771/4999 loss: 0.7906 time 25.26s
Final training  3772/4999 loss: 0.8034 time 25.52s
Final training  3773/4999 loss: 0.7740 time 25.18s
Final training  3774/4999 loss: 0.7901 time 25.44s
Final training  3775/4999 loss: 0.7782 time 25.30s
Final training  3776/4999 loss: 0.7566 time 25.55s
Final training  3777/4999 loss: 0.7811 time 25.32s
Final training  3778/4999 loss: 0.7756 time 25.29s
Final training  3779/4999 loss: 0.7888 time 25.28s
Final training  3780/4999 loss: 0.7812 time 25.39s
Final training  3781/4999 loss: 0.8009 time 25.35s
Final training  3782/4999 loss: 0.8098 time 25.57s
Final training  3783/4999 loss: 0.7448 time 25.00s
Final training  3784/4999 loss: 0.7781 time 25.36s
Final training  3785/4999 loss: 0.7408 time 25.54s
Final training  3786/4999 loss: 0.7898 time 25.76s
Final training  3787/4999 loss: 0.7970 time 25.45s
Final training  3788/4999 loss: 0.7924 time 25.31s
Final training  3789/4999 loss: 0.7463 time 25.41s
Final training  3790/4999 loss: 0.7411 time 24.84s
Final training  3791/4999 loss: 0.7823 time 25.37s
Final training  3792/4999 loss: 0.7884 time 25.34s
Final training  3793/4999 loss: 0.7836 time 25.28s
Final training  3794/4999 loss: 0.7680 time 25.32s
Final training  3795/4999 loss: 0.7670 time 25.51s
Final training  3796/4999 loss: 0.7676 time 25.42s
Final training  3797/4999 loss: 0.7895 time 25.50s
Final training  3798/4999 loss: 0.7745 time 24.67s
Final training  3799/4999 loss: 0.7621 time 25.02s
Dice accuracy for each class:  (tensor([0.9962, 0.9121, 0.9433, 0.9435, 0.7773, 0.7185, 0.9320, 0.7747, 0.9063,
        0.8473, 0.7526, 0.7935, 0.6817, 0.6577], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3799/4999 acc [ 0.832] time 195.12s
trigger times: 7
Final training  3800/4999 loss: 0.7802 time 25.31s
Final training  3801/4999 loss: 0.7730 time 25.09s
Final training  3802/4999 loss: 0.7918 time 25.46s
Final training  3803/4999 loss: 0.7877 time 25.37s
Final training  3804/4999 loss: 0.7839 time 25.52s
Final training  3805/4999 loss: 0.7921 time 25.70s
Final training  3806/4999 loss: 0.7701 time 24.94s
Final training  3807/4999 loss: 0.7945 time 24.89s
Final training  3808/4999 loss: 0.7960 time 25.23s
Final training  3809/4999 loss: 0.7633 time 25.25s
Final training  3810/4999 loss: 0.7628 time 25.16s
Final training  3811/4999 loss: 0.7686 time 25.29s
Final training  3812/4999 loss: 0.7864 time 25.24s
Final training  3813/4999 loss: 0.7646 time 25.08s
Final training  3814/4999 loss: 0.7996 time 25.21s
Final training  3815/4999 loss: 0.7837 time 25.39s
Final training  3816/4999 loss: 0.8083 time 25.16s
Final training  3817/4999 loss: 0.7973 time 25.77s
Final training  3818/4999 loss: 0.7892 time 25.19s
Final training  3819/4999 loss: 0.7703 time 25.17s
Final training  3820/4999 loss: 0.7832 time 25.48s
Final training  3821/4999 loss: 0.7825 time 25.12s
Final training  3822/4999 loss: 0.7915 time 25.09s
Final training  3823/4999 loss: 0.7832 time 25.11s
Final training  3824/4999 loss: 0.7677 time 25.35s
Final training  3825/4999 loss: 0.7653 time 25.21s
Final training  3826/4999 loss: 0.7893 time 25.34s
Final training  3827/4999 loss: 0.7577 time 25.47s
Final training  3828/4999 loss: 0.7823 time 25.40s
Final training  3829/4999 loss: 0.7844 time 25.60s
Final training  3830/4999 loss: 0.7793 time 25.59s
Final training  3831/4999 loss: 0.7851 time 25.48s
Final training  3832/4999 loss: 0.7868 time 25.04s
Final training  3833/4999 loss: 0.7923 time 25.13s
Final training  3834/4999 loss: 0.7707 time 25.48s
Final training  3835/4999 loss: 0.7987 time 25.32s
Final training  3836/4999 loss: 0.7881 time 25.60s
Final training  3837/4999 loss: 0.8048 time 25.44s
Final training  3838/4999 loss: 0.7608 time 25.08s
Final training  3839/4999 loss: 0.7991 time 25.58s
Final training  3840/4999 loss: 0.7838 time 25.46s
Final training  3841/4999 loss: 0.7946 time 24.96s
Final training  3842/4999 loss: 0.7745 time 25.61s
Final training  3843/4999 loss: 0.7920 time 25.56s
Final training  3844/4999 loss: 0.7896 time 25.16s
Final training  3845/4999 loss: 0.8091 time 25.25s
Final training  3846/4999 loss: 0.7583 time 25.32s
Final training  3847/4999 loss: 0.7813 time 25.17s
Final training  3848/4999 loss: 0.7700 time 25.60s
Final training  3849/4999 loss: 0.7793 time 25.35s
Final training  3850/4999 loss: 0.7933 time 25.12s
Final training  3851/4999 loss: 0.7759 time 25.35s
Final training  3852/4999 loss: 0.7749 time 25.14s
Final training  3853/4999 loss: 0.7797 time 25.21s
Final training  3854/4999 loss: 0.7858 time 24.92s
Final training  3855/4999 loss: 0.7824 time 25.00s
Final training  3856/4999 loss: 0.7859 time 25.13s
Final training  3857/4999 loss: 0.7647 time 25.05s
Final training  3858/4999 loss: 0.7666 time 24.66s
Final training  3859/4999 loss: 0.7738 time 24.76s
Final training  3860/4999 loss: 0.7740 time 24.72s
Final training  3861/4999 loss: 0.7851 time 24.96s
Final training  3862/4999 loss: 0.7983 time 25.34s
Final training  3863/4999 loss: 0.7822 time 25.14s
Final training  3864/4999 loss: 0.7663 time 25.36s
Final training  3865/4999 loss: 0.7693 time 25.16s
Final training  3866/4999 loss: 0.7874 time 24.98s
Final training  3867/4999 loss: 0.7855 time 25.61s
Final training  3868/4999 loss: 0.7889 time 25.55s
Final training  3869/4999 loss: 0.7926 time 25.46s
Final training  3870/4999 loss: 0.7793 time 25.20s
Final training  3871/4999 loss: 0.7682 time 25.63s
Final training  3872/4999 loss: 0.8107 time 25.63s
Final training  3873/4999 loss: 0.7739 time 25.52s
Final training  3874/4999 loss: 0.7591 time 25.70s
Final training  3875/4999 loss: 0.7538 time 25.43s
Final training  3876/4999 loss: 0.7805 time 25.24s
Final training  3877/4999 loss: 0.7850 time 25.43s
Final training  3878/4999 loss: 0.7858 time 25.22s
Final training  3879/4999 loss: 0.8068 time 25.24s
Final training  3880/4999 loss: 0.7860 time 25.23s
Final training  3881/4999 loss: 0.8057 time 25.73s
Final training  3882/4999 loss: 0.7896 time 25.45s
Final training  3883/4999 loss: 0.7907 time 26.08s
Final training  3884/4999 loss: 0.7810 time 25.69s
Final training  3885/4999 loss: 0.7929 time 25.46s
Final training  3886/4999 loss: 0.7858 time 25.53s
Final training  3887/4999 loss: 0.7633 time 25.58s
Final training  3888/4999 loss: 0.7762 time 25.62s
Final training  3889/4999 loss: 0.7622 time 25.31s
Final training  3890/4999 loss: 0.7761 time 25.30s
Final training  3891/4999 loss: 0.7865 time 25.11s
Final training  3892/4999 loss: 0.7639 time 25.15s
Final training  3893/4999 loss: 0.7778 time 25.44s
Final training  3894/4999 loss: 0.7638 time 24.73s
Final training  3895/4999 loss: 0.7765 time 25.49s
Final training  3896/4999 loss: 0.7965 time 25.20s
Final training  3897/4999 loss: 0.7759 time 25.14s
Final training  3898/4999 loss: 0.7708 time 25.16s
Final training  3899/4999 loss: 0.7709 time 25.24s
Dice accuracy for each class:  (tensor([0.9961, 0.9012, 0.9426, 0.9404, 0.7624, 0.7196, 0.9294, 0.7945, 0.9058,
        0.8498, 0.7532, 0.7867, 0.6812, 0.6715], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3899/4999 acc [ 0.832] time 194.51s
trigger times: 8
Final training  3900/4999 loss: 0.7742 time 25.39s
Final training  3901/4999 loss: 0.7937 time 25.50s
Final training  3902/4999 loss: 0.7872 time 25.44s
Final training  3903/4999 loss: 0.7664 time 25.50s
Final training  3904/4999 loss: 0.7691 time 25.67s
Final training  3905/4999 loss: 0.7735 time 25.32s
Final training  3906/4999 loss: 0.7777 time 25.63s
Final training  3907/4999 loss: 0.7707 time 25.48s
Final training  3908/4999 loss: 0.7676 time 25.54s
Final training  3909/4999 loss: 0.7754 time 25.52s
Final training  3910/4999 loss: 0.7671 time 25.18s
Final training  3911/4999 loss: 0.7831 time 25.41s
Final training  3912/4999 loss: 0.8035 time 25.56s
Final training  3913/4999 loss: 0.7735 time 25.45s
Final training  3914/4999 loss: 0.8023 time 25.36s
Final training  3915/4999 loss: 0.7693 time 25.54s
Final training  3916/4999 loss: 0.7852 time 25.37s
Final training  3917/4999 loss: 0.7674 time 25.43s
Final training  3918/4999 loss: 0.7867 time 25.30s
Final training  3919/4999 loss: 0.8064 time 25.41s
Final training  3920/4999 loss: 0.7700 time 25.27s
Final training  3921/4999 loss: 0.7643 time 25.68s
Final training  3922/4999 loss: 0.7826 time 25.25s
Final training  3923/4999 loss: 0.7916 time 25.09s
Final training  3924/4999 loss: 0.7856 time 25.30s
Final training  3925/4999 loss: 0.7951 time 25.54s
Final training  3926/4999 loss: 0.7914 time 25.49s
Final training  3927/4999 loss: 0.8015 time 25.43s
Final training  3928/4999 loss: 0.7891 time 25.37s
Final training  3929/4999 loss: 0.7928 time 25.36s
Final training  3930/4999 loss: 0.7847 time 25.36s
Final training  3931/4999 loss: 0.7793 time 25.40s
Final training  3932/4999 loss: 0.7757 time 25.51s
Final training  3933/4999 loss: 0.7977 time 25.30s
Final training  3934/4999 loss: 0.7584 time 25.13s
Final training  3935/4999 loss: 0.7551 time 25.40s
Final training  3936/4999 loss: 0.7742 time 24.95s
Final training  3937/4999 loss: 0.7848 time 25.10s
Final training  3938/4999 loss: 0.7935 time 24.86s
Final training  3939/4999 loss: 0.7806 time 25.02s
Final training  3940/4999 loss: 0.7753 time 24.91s
Final training  3941/4999 loss: 0.7668 time 25.53s
Final training  3942/4999 loss: 0.7748 time 25.34s
Final training  3943/4999 loss: 0.7834 time 25.18s
Final training  3944/4999 loss: 0.7817 time 25.49s
Final training  3945/4999 loss: 0.7937 time 25.78s
Final training  3946/4999 loss: 0.7720 time 25.64s
Final training  3947/4999 loss: 0.7687 time 25.27s
Final training  3948/4999 loss: 0.8018 time 25.10s
Final training  3949/4999 loss: 0.7436 time 25.73s
Final training  3950/4999 loss: 0.7559 time 25.42s
Final training  3951/4999 loss: 0.7578 time 25.66s
Final training  3952/4999 loss: 0.7480 time 25.86s
Final training  3953/4999 loss: 0.7850 time 25.84s
Final training  3954/4999 loss: 0.7711 time 25.45s
Final training  3955/4999 loss: 0.7861 time 25.39s
Final training  3956/4999 loss: 0.7806 time 25.56s
Final training  3957/4999 loss: 0.7653 time 25.35s
Final training  3958/4999 loss: 0.7829 time 25.49s
Final training  3959/4999 loss: 0.7800 time 25.15s
Final training  3960/4999 loss: 0.7760 time 25.30s
Final training  3961/4999 loss: 0.7827 time 25.12s
Final training  3962/4999 loss: 0.7640 time 25.26s
Final training  3963/4999 loss: 0.7646 time 25.01s
Final training  3964/4999 loss: 0.7674 time 25.13s
Final training  3965/4999 loss: 0.7727 time 25.46s
Final training  3966/4999 loss: 0.7555 time 25.93s
Final training  3967/4999 loss: 0.7558 time 25.84s
Final training  3968/4999 loss: 0.7872 time 25.77s
Final training  3969/4999 loss: 0.7796 time 25.12s
Final training  3970/4999 loss: 0.7650 time 25.29s
Final training  3971/4999 loss: 0.7925 time 25.27s
Final training  3972/4999 loss: 0.7843 time 25.51s
Final training  3973/4999 loss: 0.7909 time 25.47s
Final training  3974/4999 loss: 0.7733 time 25.18s
Final training  3975/4999 loss: 0.7867 time 25.42s
Final training  3976/4999 loss: 0.7816 time 25.05s
Final training  3977/4999 loss: 0.7934 time 25.54s
Final training  3978/4999 loss: 0.7789 time 25.60s
Final training  3979/4999 loss: 0.7769 time 25.67s
Final training  3980/4999 loss: 0.7709 time 25.43s
Final training  3981/4999 loss: 0.8124 time 25.37s
Final training  3982/4999 loss: 0.7596 time 25.80s
Final training  3983/4999 loss: 0.8121 time 25.19s
Final training  3984/4999 loss: 0.7644 time 25.21s
Final training  3985/4999 loss: 0.7980 time 25.37s
Final training  3986/4999 loss: 0.7610 time 25.57s
Final training  3987/4999 loss: 0.7948 time 25.33s
Final training  3988/4999 loss: 0.7699 time 25.56s
Final training  3989/4999 loss: 0.7463 time 25.55s
Final training  3990/4999 loss: 0.7582 time 25.35s
Final training  3991/4999 loss: 0.7970 time 25.54s
Final training  3992/4999 loss: 0.7507 time 25.45s
Final training  3993/4999 loss: 0.7717 time 25.56s
Final training  3994/4999 loss: 0.7705 time 25.20s
Final training  3995/4999 loss: 0.7734 time 25.31s
Final training  3996/4999 loss: 0.7776 time 25.55s
Final training  3997/4999 loss: 0.7632 time 25.56s
Final training  3998/4999 loss: 0.7634 time 25.46s
Final training  3999/4999 loss: 0.7884 time 25.58s
Dice accuracy for each class:  (tensor([0.9959, 0.9061, 0.9439, 0.9405, 0.7726, 0.7239, 0.9155, 0.7908, 0.9068,
        0.8556, 0.7567, 0.8144, 0.6865, 0.6621], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3999/4999 acc [ 0.834] time 194.44s
Reset trigger time to 0
new best (0.832000 --> 0.834179). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsf/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  4000/4999 loss: 0.7836 time 25.26s
Final training  4001/4999 loss: 0.7886 time 25.75s
Final training  4002/4999 loss: 0.7700 time 25.42s
Final training  4003/4999 loss: 0.7708 time 25.84s
Final training  4004/4999 loss: 0.7814 time 25.93s
Final training  4005/4999 loss: 0.7520 time 25.57s
Final training  4006/4999 loss: 0.7590 time 25.60s
Final training  4007/4999 loss: 0.7684 time 25.52s
Final training  4008/4999 loss: 0.7895 time 25.58s
Final training  4009/4999 loss: 0.7720 time 25.70s
Final training  4010/4999 loss: 0.7718 time 25.64s
Final training  4011/4999 loss: 0.8006 time 25.62s
Final training  4012/4999 loss: 0.8023 time 25.39s
Final training  4013/4999 loss: 0.7659 time 25.22s
Final training  4014/4999 loss: 0.7793 time 25.59s
Final training  4015/4999 loss: 0.7997 time 25.24s
Final training  4016/4999 loss: 0.7557 time 25.38s
Final training  4017/4999 loss: 0.7715 time 25.44s
Final training  4018/4999 loss: 0.7523 time 25.58s
Final training  4019/4999 loss: 0.7844 time 26.06s
Final training  4020/4999 loss: 0.7659 time 25.64s
Final training  4021/4999 loss: 0.7596 time 25.55s
Final training  4022/4999 loss: 0.7861 time 25.20s
Final training  4023/4999 loss: 0.7854 time 25.59s
Final training  4024/4999 loss: 0.7787 time 25.35s
Final training  4025/4999 loss: 0.8030 time 25.36s
Final training  4026/4999 loss: 0.7940 time 25.62s
Final training  4027/4999 loss: 0.7712 time 25.45s
Final training  4028/4999 loss: 0.7693 time 25.70s
Final training  4029/4999 loss: 0.7590 time 25.54s
Final training  4030/4999 loss: 0.7549 time 25.64s
Final training  4031/4999 loss: 0.7673 time 25.06s
Final training  4032/4999 loss: 0.7937 time 25.63s
Final training  4033/4999 loss: 0.7684 time 25.83s
Final training  4034/4999 loss: 0.7890 time 25.36s
Final training  4035/4999 loss: 0.7678 time 25.88s
Final training  4036/4999 loss: 0.7742 time 25.88s
Final training  4037/4999 loss: 0.7527 time 25.54s
Final training  4038/4999 loss: 0.7866 time 25.51s
Final training  4039/4999 loss: 0.7597 time 25.71s
Final training  4040/4999 loss: 0.7907 time 26.08s
Final training  4041/4999 loss: 0.7709 time 25.83s
Final training  4042/4999 loss: 0.7709 time 25.81s
Final training  4043/4999 loss: 0.7954 time 25.75s
Final training  4044/4999 loss: 0.7840 time 25.60s
Final training  4045/4999 loss: 0.7505 time 25.74s
Final training  4046/4999 loss: 0.7592 time 25.64s
Final training  4047/4999 loss: 0.7912 time 25.39s
Final training  4048/4999 loss: 0.7790 time 25.65s
Final training  4049/4999 loss: 0.7938 time 25.90s
Final training  4050/4999 loss: 0.7626 time 25.62s
Final training  4051/4999 loss: 0.7541 time 25.74s
Final training  4052/4999 loss: 0.7811 time 25.79s
Final training  4053/4999 loss: 0.7837 time 25.53s
Final training  4054/4999 loss: 0.7771 time 25.55s
Final training  4055/4999 loss: 0.7676 time 25.38s
Final training  4056/4999 loss: 0.7972 time 25.70s
Final training  4057/4999 loss: 0.7256 time 25.33s
Final training  4058/4999 loss: 0.7641 time 25.47s
Final training  4059/4999 loss: 0.7739 time 25.29s
Final training  4060/4999 loss: 0.7910 time 24.91s
Final training  4061/4999 loss: 0.7817 time 25.42s
Final training  4062/4999 loss: 0.7759 time 25.02s
Final training  4063/4999 loss: 0.8181 time 25.51s
Final training  4064/4999 loss: 0.7776 time 25.70s
Final training  4065/4999 loss: 0.7847 time 25.67s
Final training  4066/4999 loss: 0.7741 time 25.83s
Final training  4067/4999 loss: 0.7858 time 25.37s
Final training  4068/4999 loss: 0.7994 time 25.64s
Final training  4069/4999 loss: 0.7867 time 25.38s
Final training  4070/4999 loss: 0.8046 time 25.18s
Final training  4071/4999 loss: 0.7904 time 25.42s
Final training  4072/4999 loss: 0.7809 time 26.15s
Final training  4073/4999 loss: 0.7902 time 25.41s
Final training  4074/4999 loss: 0.7744 time 25.42s
Final training  4075/4999 loss: 0.7904 time 25.54s
Final training  4076/4999 loss: 0.7896 time 25.38s
Final training  4077/4999 loss: 0.7733 time 25.39s
Final training  4078/4999 loss: 0.7598 time 25.22s
Final training  4079/4999 loss: 0.7666 time 25.36s
Final training  4080/4999 loss: 0.7612 time 25.32s
Final training  4081/4999 loss: 0.7681 time 25.20s
Final training  4082/4999 loss: 0.7801 time 25.73s
Final training  4083/4999 loss: 0.7530 time 25.54s
Final training  4084/4999 loss: 0.7850 time 25.61s
Final training  4085/4999 loss: 0.8073 time 25.67s
Final training  4086/4999 loss: 0.7838 time 25.45s
Final training  4087/4999 loss: 0.7640 time 25.44s
Final training  4088/4999 loss: 0.7877 time 25.10s
Final training  4089/4999 loss: 0.7947 time 25.63s
Final training  4090/4999 loss: 0.7796 time 25.59s
Final training  4091/4999 loss: 0.7743 time 25.50s
Final training  4092/4999 loss: 0.7801 time 25.85s
Final training  4093/4999 loss: 0.7903 time 25.51s
Final training  4094/4999 loss: 0.7585 time 25.59s
Final training  4095/4999 loss: 0.7932 time 25.39s
Final training  4096/4999 loss: 0.7682 time 25.13s
Final training  4097/4999 loss: 0.7853 time 25.32s
Final training  4098/4999 loss: 0.7893 time 25.40s
Final training  4099/4999 loss: 0.7605 time 25.19s
Dice accuracy for each class:  (tensor([0.9959, 0.9073, 0.9431, 0.9413, 0.7614, 0.7242, 0.9207, 0.7815, 0.9042,
        0.8500, 0.7516, 0.7957, 0.6826, 0.6705], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4099/4999 acc [ 0.831] time 194.20s
trigger times: 1
Final training  4100/4999 loss: 0.7983 time 25.74s
Final training  4101/4999 loss: 0.7666 time 25.76s
Final training  4102/4999 loss: 0.7877 time 25.38s
Final training  4103/4999 loss: 0.7731 time 25.57s
Final training  4104/4999 loss: 0.7766 time 24.89s
Final training  4105/4999 loss: 0.7808 time 25.62s
Final training  4106/4999 loss: 0.7935 time 25.39s
Final training  4107/4999 loss: 0.7820 time 25.34s
Final training  4108/4999 loss: 0.7837 time 25.54s
Final training  4109/4999 loss: 0.7673 time 25.29s
Final training  4110/4999 loss: 0.7677 time 25.63s
Final training  4111/4999 loss: 0.7936 time 25.38s
Final training  4112/4999 loss: 0.7576 time 25.24s
Final training  4113/4999 loss: 0.7685 time 25.53s
Final training  4114/4999 loss: 0.7794 time 25.35s
Final training  4115/4999 loss: 0.7771 time 25.70s
Final training  4116/4999 loss: 0.7739 time 25.55s
Final training  4117/4999 loss: 0.7956 time 25.67s
Final training  4118/4999 loss: 0.7705 time 25.36s
Final training  4119/4999 loss: 0.7912 time 25.56s
Final training  4120/4999 loss: 0.7911 time 25.14s
Final training  4121/4999 loss: 0.7741 time 25.51s
Final training  4122/4999 loss: 0.7889 time 25.70s
Final training  4123/4999 loss: 0.7544 time 25.50s
Final training  4124/4999 loss: 0.7888 time 25.56s
Final training  4125/4999 loss: 0.7868 time 25.69s
Final training  4126/4999 loss: 0.7951 time 25.86s
Final training  4127/4999 loss: 0.7708 time 25.40s
Final training  4128/4999 loss: 0.7852 time 25.44s
Final training  4129/4999 loss: 0.7704 time 25.26s
Final training  4130/4999 loss: 0.7656 time 25.59s
Final training  4131/4999 loss: 0.7988 time 25.50s
Final training  4132/4999 loss: 0.7797 time 25.65s
Final training  4133/4999 loss: 0.7871 time 25.22s
Final training  4134/4999 loss: 0.7385 time 25.26s
Final training  4135/4999 loss: 0.7813 time 25.08s
Final training  4136/4999 loss: 0.7779 time 25.63s
Final training  4137/4999 loss: 0.7840 time 25.53s
Final training  4138/4999 loss: 0.7683 time 25.65s
Final training  4139/4999 loss: 0.7812 time 25.31s
Final training  4140/4999 loss: 0.7996 time 25.44s
Final training  4141/4999 loss: 0.8052 time 25.49s
Final training  4142/4999 loss: 0.7735 time 25.54s
Final training  4143/4999 loss: 0.7473 time 25.34s
Final training  4144/4999 loss: 0.7711 time 25.67s
Final training  4145/4999 loss: 0.7865 time 25.80s
Final training  4146/4999 loss: 0.7515 time 25.37s
Final training  4147/4999 loss: 0.7683 time 25.53s
Final training  4148/4999 loss: 0.7722 time 25.70s
Final training  4149/4999 loss: 0.7848 time 25.38s
Final training  4150/4999 loss: 0.7890 time 25.79s
Final training  4151/4999 loss: 0.7774 time 25.45s
Final training  4152/4999 loss: 0.7761 time 25.65s
Final training  4153/4999 loss: 0.7595 time 25.43s
Final training  4154/4999 loss: 0.7834 time 25.41s
Final training  4155/4999 loss: 0.7958 time 25.23s
Final training  4156/4999 loss: 0.7733 time 25.53s
Final training  4157/4999 loss: 0.7710 time 24.97s
Final training  4158/4999 loss: 0.7849 time 24.96s
Final training  4159/4999 loss: 0.7793 time 25.28s
Final training  4160/4999 loss: 0.7911 time 25.20s
Final training  4161/4999 loss: 0.7952 time 25.30s
Final training  4162/4999 loss: 0.7619 time 25.60s
Final training  4163/4999 loss: 0.7733 time 25.45s
Final training  4164/4999 loss: 0.7742 time 25.76s
Final training  4165/4999 loss: 0.7882 time 25.48s
Final training  4166/4999 loss: 0.7520 time 25.83s
Final training  4167/4999 loss: 0.7752 time 25.42s
Final training  4168/4999 loss: 0.7756 time 25.52s
Final training  4169/4999 loss: 0.7957 time 25.62s
Final training  4170/4999 loss: 0.7526 time 25.68s
Final training  4171/4999 loss: 0.7481 time 25.53s
Final training  4172/4999 loss: 0.7939 time 25.64s
Final training  4173/4999 loss: 0.7807 time 25.23s
Final training  4174/4999 loss: 0.7656 time 25.58s
Final training  4175/4999 loss: 0.7939 time 25.34s
Final training  4176/4999 loss: 0.7394 time 25.24s
Final training  4177/4999 loss: 0.7753 time 25.52s
Final training  4178/4999 loss: 0.7499 time 25.49s
Final training  4179/4999 loss: 0.7822 time 26.12s
Final training  4180/4999 loss: 0.7994 time 25.67s
Final training  4181/4999 loss: 0.7861 time 25.45s
Final training  4182/4999 loss: 0.7676 time 25.58s
Final training  4183/4999 loss: 0.7579 time 26.02s
Final training  4184/4999 loss: 0.7741 time 25.35s
Final training  4185/4999 loss: 0.7716 time 25.83s
Final training  4186/4999 loss: 0.7770 time 25.39s
Final training  4187/4999 loss: 0.7792 time 25.47s
Final training  4188/4999 loss: 0.7475 time 25.41s
Final training  4189/4999 loss: 0.7799 time 25.37s
Final training  4190/4999 loss: 0.7965 time 25.65s
Final training  4191/4999 loss: 0.7642 time 25.15s
Final training  4192/4999 loss: 0.7917 time 25.61s
Final training  4193/4999 loss: 0.7561 time 25.50s
Final training  4194/4999 loss: 0.7856 time 25.81s
Final training  4195/4999 loss: 0.7780 time 25.61s
Final training  4196/4999 loss: 0.7387 time 25.34s
Final training  4197/4999 loss: 0.7727 time 25.06s
Final training  4198/4999 loss: 0.7767 time 25.42s
Final training  4199/4999 loss: 0.7922 time 25.50s
Dice accuracy for each class:  (tensor([0.9960, 0.9141, 0.9439, 0.9430, 0.7574, 0.7264, 0.9222, 0.7804, 0.9047,
        0.8421, 0.7468, 0.7854, 0.6784, 0.6756], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4199/4999 acc [ 0.830] time 194.52s
trigger times: 2
Final training  4200/4999 loss: 0.8029 time 25.41s
Final training  4201/4999 loss: 0.7734 time 25.27s
Final training  4202/4999 loss: 0.7943 time 25.20s
Final training  4203/4999 loss: 0.7561 time 25.19s
Final training  4204/4999 loss: 0.7638 time 25.01s
Final training  4205/4999 loss: 0.7777 time 24.95s
Final training  4206/4999 loss: 0.8007 time 25.46s
Final training  4207/4999 loss: 0.7990 time 25.16s
Final training  4208/4999 loss: 0.7636 time 25.61s
Final training  4209/4999 loss: 0.7705 time 25.35s
Final training  4210/4999 loss: 0.7977 time 25.32s
Final training  4211/4999 loss: 0.7722 time 25.48s
Final training  4212/4999 loss: 0.7850 time 25.36s
Final training  4213/4999 loss: 0.7656 time 25.62s
Final training  4214/4999 loss: 0.7709 time 25.44s
Final training  4215/4999 loss: 0.7537 time 25.18s
Final training  4216/4999 loss: 0.7818 time 25.39s
Final training  4217/4999 loss: 0.7901 time 25.10s
Final training  4218/4999 loss: 0.7821 time 25.27s
Final training  4219/4999 loss: 0.7768 time 25.19s
Final training  4220/4999 loss: 0.7788 time 25.47s
Final training  4221/4999 loss: 0.7709 time 25.11s
Final training  4222/4999 loss: 0.7634 time 25.27s
Final training  4223/4999 loss: 0.7739 time 24.93s
Final training  4224/4999 loss: 0.7776 time 25.18s
Final training  4225/4999 loss: 0.7723 time 25.30s
Final training  4226/4999 loss: 0.7809 time 25.23s
Final training  4227/4999 loss: 0.7872 time 25.24s
Final training  4228/4999 loss: 0.7961 time 25.21s
Final training  4229/4999 loss: 0.7661 time 25.04s
Final training  4230/4999 loss: 0.7930 time 25.29s
Final training  4231/4999 loss: 0.7653 time 25.36s
Final training  4232/4999 loss: 0.7948 time 25.63s
Final training  4233/4999 loss: 0.7650 time 25.37s
Final training  4234/4999 loss: 0.7706 time 25.60s
Final training  4235/4999 loss: 0.7767 time 25.19s
Final training  4236/4999 loss: 0.7808 time 25.36s
Final training  4237/4999 loss: 0.7902 time 25.17s
Final training  4238/4999 loss: 0.7816 time 25.15s
Final training  4239/4999 loss: 0.7548 time 24.91s
Final training  4240/4999 loss: 0.7609 time 25.40s
Final training  4241/4999 loss: 0.7630 time 25.43s
Final training  4242/4999 loss: 0.7719 time 25.17s
Final training  4243/4999 loss: 0.7524 time 25.31s
Final training  4244/4999 loss: 0.7816 time 25.27s
Final training  4245/4999 loss: 0.7809 time 25.29s
Final training  4246/4999 loss: 0.7704 time 25.20s
Final training  4247/4999 loss: 0.7991 time 25.03s
Final training  4248/4999 loss: 0.7637 time 25.06s
Final training  4249/4999 loss: 0.7719 time 25.23s
Final training  4250/4999 loss: 0.8041 time 25.26s
Final training  4251/4999 loss: 0.7720 time 25.12s
Final training  4252/4999 loss: 0.7572 time 25.10s
Final training  4253/4999 loss: 0.7872 time 24.88s
Final training  4254/4999 loss: 0.7770 time 24.98s
Final training  4255/4999 loss: 0.7800 time 25.19s
Final training  4256/4999 loss: 0.7683 time 24.82s
Final training  4257/4999 loss: 0.7780 time 25.04s
Final training  4258/4999 loss: 0.7777 time 25.23s
Final training  4259/4999 loss: 0.7449 time 24.84s
Final training  4260/4999 loss: 0.7901 time 25.08s
Final training  4261/4999 loss: 0.7765 time 25.47s
Final training  4262/4999 loss: 0.7665 time 25.16s
Final training  4263/4999 loss: 0.7775 time 25.46s
Final training  4264/4999 loss: 0.8023 time 25.39s
Final training  4265/4999 loss: 0.7849 time 25.15s
Final training  4266/4999 loss: 0.7724 time 25.33s
Final training  4267/4999 loss: 0.8158 time 25.36s
Final training  4268/4999 loss: 0.7705 time 25.50s
Final training  4269/4999 loss: 0.7608 time 25.35s
Final training  4270/4999 loss: 0.7717 time 25.32s
Final training  4271/4999 loss: 0.7937 time 25.40s
Final training  4272/4999 loss: 0.7930 time 25.25s
Final training  4273/4999 loss: 0.7860 time 25.64s
Final training  4274/4999 loss: 0.7988 time 25.35s
Final training  4275/4999 loss: 0.8030 time 25.22s
Final training  4276/4999 loss: 0.7743 time 25.08s
Final training  4277/4999 loss: 0.7611 time 25.22s
Final training  4278/4999 loss: 0.7635 time 25.25s
Final training  4279/4999 loss: 0.7290 time 25.32s
Final training  4280/4999 loss: 0.7612 time 25.56s
Final training  4281/4999 loss: 0.7586 time 25.47s
Final training  4282/4999 loss: 0.7779 time 25.44s
Final training  4283/4999 loss: 0.7706 time 25.26s
Final training  4284/4999 loss: 0.7867 time 25.60s
Final training  4285/4999 loss: 0.7850 time 25.34s
Final training  4286/4999 loss: 0.7662 time 25.17s
Final training  4287/4999 loss: 0.7717 time 25.34s
Final training  4288/4999 loss: 0.7911 time 25.37s
Final training  4289/4999 loss: 0.7854 time 25.46s
Final training  4290/4999 loss: 0.7501 time 25.39s
Final training  4291/4999 loss: 0.7831 time 25.40s
Final training  4292/4999 loss: 0.8185 time 25.38s
Final training  4293/4999 loss: 0.7881 time 25.30s
Final training  4294/4999 loss: 0.7887 time 25.67s
Final training  4295/4999 loss: 0.7835 time 25.46s
Final training  4296/4999 loss: 0.8157 time 25.32s
Final training  4297/4999 loss: 0.7834 time 25.21s
Final training  4298/4999 loss: 0.7563 time 25.16s
Final training  4299/4999 loss: 0.7960 time 25.08s
Dice accuracy for each class:  (tensor([0.9959, 0.9028, 0.9437, 0.9444, 0.7692, 0.7156, 0.9184, 0.7780, 0.9056,
        0.8480, 0.7570, 0.8034, 0.6899, 0.6815], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4299/4999 acc [ 0.833] time 192.83s
trigger times: 3
Final training  4300/4999 loss: 0.7686 time 25.49s
Final training  4301/4999 loss: 0.7661 time 25.15s
Final training  4302/4999 loss: 0.7811 time 25.45s
Final training  4303/4999 loss: 0.7877 time 25.09s
Final training  4304/4999 loss: 0.7567 time 25.29s
Final training  4305/4999 loss: 0.7709 time 25.18s
Final training  4306/4999 loss: 0.7861 time 25.39s
Final training  4307/4999 loss: 0.7718 time 25.23s
Final training  4308/4999 loss: 0.7598 time 25.58s
Final training  4309/4999 loss: 0.7865 time 25.04s
Final training  4310/4999 loss: 0.7788 time 25.10s
Final training  4311/4999 loss: 0.7621 time 25.19s
Final training  4312/4999 loss: 0.7655 time 25.01s
Final training  4313/4999 loss: 0.7716 time 25.45s
Final training  4314/4999 loss: 0.7792 time 25.17s
Final training  4315/4999 loss: 0.7955 time 25.15s
Final training  4316/4999 loss: 0.7554 time 25.34s
Final training  4317/4999 loss: 0.8036 time 25.22s
Final training  4318/4999 loss: 0.7733 time 25.30s
Final training  4319/4999 loss: 0.7427 time 25.15s
Final training  4320/4999 loss: 0.7707 time 25.21s
Final training  4321/4999 loss: 0.7512 time 25.25s
Final training  4322/4999 loss: 0.7848 time 25.46s
Final training  4323/4999 loss: 0.7820 time 25.26s
Final training  4324/4999 loss: 0.7960 time 25.36s
Final training  4325/4999 loss: 0.7752 time 25.18s
Final training  4326/4999 loss: 0.7797 time 25.23s
Final training  4327/4999 loss: 0.7572 time 25.07s
Final training  4328/4999 loss: 0.8022 time 25.18s
Final training  4329/4999 loss: 0.7679 time 25.08s
Final training  4330/4999 loss: 0.7522 time 25.00s
Final training  4331/4999 loss: 0.7866 time 25.21s
Final training  4332/4999 loss: 0.7928 time 25.04s
Final training  4333/4999 loss: 0.7537 time 24.99s
Final training  4334/4999 loss: 0.7577 time 25.27s
Final training  4335/4999 loss: 0.7382 time 25.10s
Final training  4336/4999 loss: 0.7685 time 25.31s
Final training  4337/4999 loss: 0.7842 time 25.17s
Final training  4338/4999 loss: 0.7879 time 25.16s
Final training  4339/4999 loss: 0.7610 time 25.14s
Final training  4340/4999 loss: 0.7602 time 25.22s
Final training  4341/4999 loss: 0.7929 time 25.18s
Final training  4342/4999 loss: 0.7646 time 25.06s
Final training  4343/4999 loss: 0.7706 time 25.22s
Final training  4344/4999 loss: 0.7885 time 25.29s
Final training  4345/4999 loss: 0.7907 time 25.48s
Final training  4346/4999 loss: 0.7717 time 25.24s
Final training  4347/4999 loss: 0.7806 time 25.16s
Final training  4348/4999 loss: 0.7640 time 25.30s
Final training  4349/4999 loss: 0.7881 time 25.10s
Final training  4350/4999 loss: 0.7844 time 25.13s
Final training  4351/4999 loss: 0.7875 time 25.31s
Final training  4352/4999 loss: 0.7842 time 24.97s
Final training  4353/4999 loss: 0.7823 time 25.23s
Final training  4354/4999 loss: 0.7682 time 24.76s
Final training  4355/4999 loss: 0.7718 time 24.89s
Final training  4356/4999 loss: 0.7701 time 24.80s
Final training  4357/4999 loss: 0.7455 time 24.76s
Final training  4358/4999 loss: 0.7868 time 24.96s
Final training  4359/4999 loss: 0.7904 time 25.13s
Final training  4360/4999 loss: 0.7719 time 25.13s
Final training  4361/4999 loss: 0.7885 time 25.51s
Final training  4362/4999 loss: 0.7699 time 25.10s
Final training  4363/4999 loss: 0.8026 time 25.25s
Final training  4364/4999 loss: 0.7701 time 25.31s
Final training  4365/4999 loss: 0.7895 time 25.01s
Final training  4366/4999 loss: 0.7947 time 25.05s
Final training  4367/4999 loss: 0.7582 time 25.09s
Final training  4368/4999 loss: 0.7749 time 25.11s
Final training  4369/4999 loss: 0.7732 time 25.31s
Final training  4370/4999 loss: 0.7894 time 25.19s
Final training  4371/4999 loss: 0.7792 time 24.95s
Final training  4372/4999 loss: 0.7709 time 25.04s
Final training  4373/4999 loss: 0.7765 time 25.08s
Final training  4374/4999 loss: 0.7880 time 25.34s
Final training  4375/4999 loss: 0.7481 time 25.36s
Final training  4376/4999 loss: 0.7822 time 25.21s
Final training  4377/4999 loss: 0.7836 time 25.27s
Final training  4378/4999 loss: 0.7996 time 25.47s
Final training  4379/4999 loss: 0.7802 time 25.19s
Final training  4380/4999 loss: 0.7833 time 25.13s
Final training  4381/4999 loss: 0.7543 time 25.13s
Final training  4382/4999 loss: 0.7601 time 24.88s
Final training  4383/4999 loss: 0.7870 time 25.36s
Final training  4384/4999 loss: 0.7851 time 25.25s
Final training  4385/4999 loss: 0.7604 time 25.18s
Final training  4386/4999 loss: 0.7914 time 25.38s
Final training  4387/4999 loss: 0.7903 time 25.44s
Final training  4388/4999 loss: 0.7883 time 24.90s
Final training  4389/4999 loss: 0.7820 time 25.08s
Final training  4390/4999 loss: 0.7941 time 25.31s
Final training  4391/4999 loss: 0.8015 time 25.26s
Final training  4392/4999 loss: 0.7584 time 25.37s
Final training  4393/4999 loss: 0.7623 time 25.44s
Final training  4394/4999 loss: 0.8092 time 25.28s
Final training  4395/4999 loss: 0.7612 time 25.57s
Final training  4396/4999 loss: 0.7701 time 24.99s
Final training  4397/4999 loss: 0.7794 time 25.32s
Final training  4398/4999 loss: 0.7830 time 25.23s
Final training  4399/4999 loss: 0.7943 time 25.33s
Dice accuracy for each class:  (tensor([0.9959, 0.9085, 0.9442, 0.9433, 0.7620, 0.7237, 0.9164, 0.7860, 0.9061,
        0.8495, 0.7559, 0.7990, 0.6769, 0.6814], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4399/4999 acc [ 0.833] time 194.70s
trigger times: 4
Final training  4400/4999 loss: 0.7971 time 25.20s
Final training  4401/4999 loss: 0.7774 time 25.28s
Final training  4402/4999 loss: 0.7744 time 25.40s
Final training  4403/4999 loss: 0.7661 time 24.92s
Final training  4404/4999 loss: 0.7772 time 25.17s
Final training  4405/4999 loss: 0.7808 time 25.03s
Final training  4406/4999 loss: 0.7542 time 25.17s
Final training  4407/4999 loss: 0.7449 time 25.22s
Final training  4408/4999 loss: 0.7745 time 25.39s
Final training  4409/4999 loss: 0.7375 time 25.42s
Final training  4410/4999 loss: 0.7723 time 25.27s
Final training  4411/4999 loss: 0.7835 time 25.36s
Final training  4412/4999 loss: 0.7765 time 25.52s
Final training  4413/4999 loss: 0.7566 time 25.19s
Final training  4414/4999 loss: 0.7644 time 25.51s
Final training  4415/4999 loss: 0.7966 time 25.22s
Final training  4416/4999 loss: 0.7899 time 25.22s
Final training  4417/4999 loss: 0.7542 time 25.23s
Final training  4418/4999 loss: 0.7711 time 25.33s
Final training  4419/4999 loss: 0.7989 time 25.11s
Final training  4420/4999 loss: 0.7619 time 25.45s
Final training  4421/4999 loss: 0.7858 time 25.17s
Final training  4422/4999 loss: 0.7785 time 25.44s
Final training  4423/4999 loss: 0.8107 time 25.26s
Final training  4424/4999 loss: 0.7828 time 25.16s
Final training  4425/4999 loss: 0.7886 time 25.31s
Final training  4426/4999 loss: 0.7505 time 25.40s
Final training  4427/4999 loss: 0.7806 time 25.16s
Final training  4428/4999 loss: 0.7739 time 25.44s
Final training  4429/4999 loss: 0.7622 time 25.21s
Final training  4430/4999 loss: 0.7541 time 25.25s
Final training  4431/4999 loss: 0.7742 time 25.37s
Final training  4432/4999 loss: 0.7755 time 25.28s
Final training  4433/4999 loss: 0.7604 time 25.41s
Final training  4434/4999 loss: 0.7706 time 25.30s
Final training  4435/4999 loss: 0.7453 time 25.24s
Final training  4436/4999 loss: 0.8004 time 25.20s
Final training  4437/4999 loss: 0.7752 time 25.21s
Final training  4438/4999 loss: 0.7832 time 25.12s
Final training  4439/4999 loss: 0.7584 time 25.10s
Final training  4440/4999 loss: 0.7587 time 25.05s
Final training  4441/4999 loss: 0.7689 time 25.32s
Final training  4442/4999 loss: 0.7765 time 25.39s
Final training  4443/4999 loss: 0.7617 time 24.98s
Final training  4444/4999 loss: 0.7896 time 25.46s
Final training  4445/4999 loss: 0.7969 time 25.23s
Final training  4446/4999 loss: 0.7960 time 25.21s
Final training  4447/4999 loss: 0.7807 time 25.26s
Final training  4448/4999 loss: 0.7758 time 25.17s
Final training  4449/4999 loss: 0.7778 time 25.38s
Final training  4450/4999 loss: 0.7477 time 24.96s
Final training  4451/4999 loss: 0.7658 time 24.95s
Final training  4452/4999 loss: 0.7865 time 25.53s
Final training  4453/4999 loss: 0.7758 time 25.21s
Final training  4454/4999 loss: 0.7897 time 25.16s
Final training  4455/4999 loss: 0.7806 time 25.26s
Final training  4456/4999 loss: 0.7842 time 24.85s
Final training  4457/4999 loss: 0.7680 time 25.07s
Final training  4458/4999 loss: 0.7964 time 25.30s
Final training  4459/4999 loss: 0.7762 time 25.25s
Final training  4460/4999 loss: 0.7737 time 25.14s
Final training  4461/4999 loss: 0.7684 time 25.23s
Final training  4462/4999 loss: 0.7498 time 24.95s
Final training  4463/4999 loss: 0.7354 time 25.32s
Final training  4464/4999 loss: 0.7698 time 25.43s
Final training  4465/4999 loss: 0.7645 time 25.19s
Final training  4466/4999 loss: 0.7615 time 25.15s
Final training  4467/4999 loss: 0.7862 time 25.12s
Final training  4468/4999 loss: 0.7846 time 25.21s
Final training  4469/4999 loss: 0.7637 time 25.32s
Final training  4470/4999 loss: 0.7719 time 25.24s
Final training  4471/4999 loss: 0.7604 time 25.33s
Final training  4472/4999 loss: 0.7701 time 25.29s
Final training  4473/4999 loss: 0.7433 time 25.34s
Final training  4474/4999 loss: 0.7861 time 25.02s
Final training  4475/4999 loss: 0.7571 time 25.58s
Final training  4476/4999 loss: 0.7755 time 25.17s
Final training  4477/4999 loss: 0.7763 time 25.28s
Final training  4478/4999 loss: 0.7716 time 25.53s
Final training  4479/4999 loss: 0.7737 time 25.08s
Final training  4480/4999 loss: 0.7773 time 25.18s
Final training  4481/4999 loss: 0.7930 time 25.15s
Final training  4482/4999 loss: 0.7813 time 24.99s
Final training  4483/4999 loss: 0.7650 time 25.08s
Final training  4484/4999 loss: 0.7912 time 25.23s
Final training  4485/4999 loss: 0.7740 time 25.83s
Final training  4486/4999 loss: 0.7944 time 25.45s
Final training  4487/4999 loss: 0.7777 time 26.27s
Final training  4488/4999 loss: 0.8087 time 26.21s
Final training  4489/4999 loss: 0.7762 time 25.88s
Final training  4490/4999 loss: 0.7718 time 25.79s
Final training  4491/4999 loss: 0.7950 time 25.95s
Final training  4492/4999 loss: 0.7555 time 26.33s
Final training  4493/4999 loss: 0.7860 time 25.78s
Final training  4494/4999 loss: 0.7708 time 25.68s
Final training  4495/4999 loss: 0.7902 time 25.96s
Final training  4496/4999 loss: 0.7876 time 25.80s
Final training  4497/4999 loss: 0.7694 time 25.82s
Final training  4498/4999 loss: 0.7581 time 25.92s
Final training  4499/4999 loss: 0.7629 time 25.69s
Dice accuracy for each class:  (tensor([0.9958, 0.9136, 0.9442, 0.9426, 0.7778, 0.7303, 0.9145, 0.7697, 0.9067,
        0.8490, 0.7554, 0.8015, 0.6789, 0.6768], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4499/4999 acc [ 0.833] time 195.16s
trigger times: 5
Final training  4500/4999 loss: 0.7661 time 25.74s
Final training  4501/4999 loss: 0.7916 time 25.66s
Final training  4502/4999 loss: 0.7969 time 25.55s
Final training  4503/4999 loss: 0.7853 time 25.74s
Final training  4504/4999 loss: 0.7550 time 25.56s
Final training  4505/4999 loss: 0.7853 time 26.12s
Final training  4506/4999 loss: 0.7647 time 25.47s
Final training  4507/4999 loss: 0.7884 time 25.76s
Final training  4508/4999 loss: 0.7826 time 25.82s
Final training  4509/4999 loss: 0.7718 time 25.43s
Final training  4510/4999 loss: 0.7626 time 25.95s
Final training  4511/4999 loss: 0.7925 time 25.69s
Final training  4512/4999 loss: 0.7620 time 25.50s
Final training  4513/4999 loss: 0.7630 time 25.90s
Final training  4514/4999 loss: 0.7745 time 25.75s
Final training  4515/4999 loss: 0.7897 time 25.57s
Final training  4516/4999 loss: 0.7666 time 25.59s
Final training  4517/4999 loss: 0.7658 time 25.79s
Final training  4518/4999 loss: 0.7650 time 25.78s
Final training  4519/4999 loss: 0.7869 time 25.88s
Final training  4520/4999 loss: 0.7764 time 25.84s
Final training  4521/4999 loss: 0.7493 time 25.80s
Final training  4522/4999 loss: 0.7743 time 25.89s
Final training  4523/4999 loss: 0.7666 time 25.61s
Final training  4524/4999 loss: 0.7686 time 25.87s
Final training  4525/4999 loss: 0.8156 time 25.73s
Final training  4526/4999 loss: 0.7458 time 26.01s
Final training  4527/4999 loss: 0.7936 time 26.00s
Final training  4528/4999 loss: 0.7677 time 25.94s
Final training  4529/4999 loss: 0.7574 time 25.95s
Final training  4530/4999 loss: 0.7932 time 25.67s
Final training  4531/4999 loss: 0.7827 time 26.05s
Final training  4532/4999 loss: 0.7687 time 25.91s
Final training  4533/4999 loss: 0.7700 time 25.82s
Final training  4534/4999 loss: 0.7857 time 25.52s
Final training  4535/4999 loss: 0.7931 time 25.46s
Final training  4536/4999 loss: 0.7547 time 25.66s
Final training  4537/4999 loss: 0.7913 time 25.61s
Final training  4538/4999 loss: 0.7919 time 25.64s
Final training  4539/4999 loss: 0.7771 time 25.68s
Final training  4540/4999 loss: 0.7953 time 25.89s
Final training  4541/4999 loss: 0.7829 time 25.67s
Final training  4542/4999 loss: 0.7759 time 25.69s
Final training  4543/4999 loss: 0.7755 time 25.88s
Final training  4544/4999 loss: 0.7854 time 25.47s
Final training  4545/4999 loss: 0.7737 time 25.61s
Final training  4546/4999 loss: 0.7820 time 25.27s
Final training  4547/4999 loss: 0.7860 time 25.87s
Final training  4548/4999 loss: 0.7879 time 25.75s
Final training  4549/4999 loss: 0.7706 time 25.57s
Final training  4550/4999 loss: 0.7503 time 25.48s
Final training  4551/4999 loss: 0.7900 time 25.11s
Final training  4552/4999 loss: 0.7807 time 25.17s
Final training  4553/4999 loss: 0.7773 time 25.51s
Final training  4554/4999 loss: 0.7547 time 25.31s
Final training  4555/4999 loss: 0.7769 time 25.40s
Final training  4556/4999 loss: 0.7771 time 25.49s
Final training  4557/4999 loss: 0.7753 time 25.89s
Final training  4558/4999 loss: 0.7735 time 25.75s
Final training  4559/4999 loss: 0.7585 time 25.49s
Final training  4560/4999 loss: 0.7628 time 25.45s
Final training  4561/4999 loss: 0.7819 time 25.47s
Final training  4562/4999 loss: 0.7658 time 25.42s
Final training  4563/4999 loss: 0.7792 time 25.31s
Final training  4564/4999 loss: 0.7772 time 25.57s
Final training  4565/4999 loss: 0.7873 time 25.06s
Final training  4566/4999 loss: 0.7689 time 25.55s
Final training  4567/4999 loss: 0.7748 time 25.56s
Final training  4568/4999 loss: 0.7672 time 25.36s
Final training  4569/4999 loss: 0.7714 time 25.29s
Final training  4570/4999 loss: 0.7605 time 24.97s
Final training  4571/4999 loss: 0.7770 time 25.29s
Final training  4572/4999 loss: 0.7833 time 25.53s
Final training  4573/4999 loss: 0.7818 time 25.70s
Final training  4574/4999 loss: 0.7754 time 25.38s
Final training  4575/4999 loss: 0.7839 time 25.30s
Final training  4576/4999 loss: 0.7721 time 25.66s
Final training  4577/4999 loss: 0.7801 time 25.74s
Final training  4578/4999 loss: 0.7843 time 25.60s
Final training  4579/4999 loss: 0.7944 time 25.74s
Final training  4580/4999 loss: 0.7914 time 25.67s
Final training  4581/4999 loss: 0.7712 time 25.34s
Final training  4582/4999 loss: 0.7835 time 25.87s
Final training  4583/4999 loss: 0.7708 time 25.63s
Final training  4584/4999 loss: 0.7827 time 25.35s
Final training  4585/4999 loss: 0.7585 time 25.72s
Final training  4586/4999 loss: 0.7503 time 25.38s
Final training  4587/4999 loss: 0.7766 time 25.70s
Final training  4588/4999 loss: 0.7773 time 25.79s
Final training  4589/4999 loss: 0.7713 time 25.72s
Final training  4590/4999 loss: 0.7685 time 25.39s
Final training  4591/4999 loss: 0.7863 time 25.13s
Final training  4592/4999 loss: 0.7883 time 25.38s
Final training  4593/4999 loss: 0.7631 time 25.46s
Final training  4594/4999 loss: 0.7970 time 25.73s
Final training  4595/4999 loss: 0.7815 time 25.53s
Final training  4596/4999 loss: 0.7969 time 25.55s
Final training  4597/4999 loss: 0.7892 time 25.55s
Final training  4598/4999 loss: 0.7543 time 25.51s
Final training  4599/4999 loss: 0.7760 time 25.67s
Dice accuracy for each class:  (tensor([0.9960, 0.9084, 0.9441, 0.9437, 0.7783, 0.7181, 0.9212, 0.7808, 0.9067,
        0.8513, 0.7558, 0.8043, 0.6792, 0.6807], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4599/4999 acc [ 0.834] time 193.67s
trigger times: 6
Final training  4600/4999 loss: 0.7825 time 25.12s
Final training  4601/4999 loss: 0.7928 time 25.66s
Final training  4602/4999 loss: 0.7808 time 25.68s
Final training  4603/4999 loss: 0.7749 time 25.51s
Final training  4604/4999 loss: 0.7731 time 25.64s
Final training  4605/4999 loss: 0.7924 time 25.56s
Final training  4606/4999 loss: 0.7877 time 25.80s
Final training  4607/4999 loss: 0.7496 time 25.63s
Final training  4608/4999 loss: 0.7770 time 25.58s
Final training  4609/4999 loss: 0.7499 time 25.63s
Final training  4610/4999 loss: 0.7919 time 25.67s
Final training  4611/4999 loss: 0.7736 time 25.44s
Final training  4612/4999 loss: 0.7690 time 25.85s
Final training  4613/4999 loss: 0.7580 time 25.78s
Final training  4614/4999 loss: 0.7812 time 25.88s
Final training  4615/4999 loss: 0.7890 time 25.82s
Final training  4616/4999 loss: 0.7672 time 25.41s
Final training  4617/4999 loss: 0.7585 time 25.55s
Final training  4618/4999 loss: 0.7885 time 25.86s
Final training  4619/4999 loss: 0.7582 time 25.30s
Final training  4620/4999 loss: 0.7943 time 25.70s
Final training  4621/4999 loss: 0.7834 time 25.39s
Final training  4622/4999 loss: 0.7845 time 25.65s
Final training  4623/4999 loss: 0.7847 time 25.71s
Final training  4624/4999 loss: 0.7783 time 25.85s
Final training  4625/4999 loss: 0.7751 time 25.78s
Final training  4626/4999 loss: 0.7715 time 25.85s
Final training  4627/4999 loss: 0.7628 time 25.70s
Final training  4628/4999 loss: 0.8055 time 25.54s
Final training  4629/4999 loss: 0.7916 time 25.54s
Final training  4630/4999 loss: 0.7538 time 25.42s
Final training  4631/4999 loss: 0.7807 time 25.71s
Final training  4632/4999 loss: 0.7828 time 25.89s
Final training  4633/4999 loss: 0.7950 time 26.09s
Final training  4634/4999 loss: 0.7904 time 26.68s
Final training  4635/4999 loss: 0.7980 time 25.91s
Final training  4636/4999 loss: 0.7924 time 26.82s
Final training  4637/4999 loss: 0.7955 time 26.29s
Final training  4638/4999 loss: 0.7781 time 26.45s
Final training  4639/4999 loss: 0.7589 time 26.07s
Final training  4640/4999 loss: 0.7826 time 26.27s
Final training  4641/4999 loss: 0.7707 time 26.27s
Final training  4642/4999 loss: 0.7612 time 26.13s
Final training  4643/4999 loss: 0.7716 time 26.02s
Final training  4644/4999 loss: 0.7794 time 26.08s
Final training  4645/4999 loss: 0.7675 time 26.34s
Final training  4646/4999 loss: 0.7627 time 25.85s
Final training  4647/4999 loss: 0.7882 time 25.77s
Final training  4648/4999 loss: 0.7898 time 25.89s
Final training  4649/4999 loss: 0.7564 time 25.65s
Final training  4650/4999 loss: 0.7699 time 25.42s
Final training  4651/4999 loss: 0.7824 time 25.65s
Final training  4652/4999 loss: 0.7923 time 26.13s
Final training  4653/4999 loss: 0.7874 time 26.20s
Final training  4654/4999 loss: 0.7620 time 26.16s
Final training  4655/4999 loss: 0.7799 time 26.37s
Final training  4656/4999 loss: 0.7471 time 25.91s
Final training  4657/4999 loss: 0.7490 time 26.36s
Final training  4658/4999 loss: 0.7966 time 26.42s
Final training  4659/4999 loss: 0.7737 time 26.55s
Final training  4660/4999 loss: 0.7690 time 26.59s
Final training  4661/4999 loss: 0.8164 time 26.45s
Final training  4662/4999 loss: 0.7782 time 26.48s
Final training  4663/4999 loss: 0.7908 time 26.42s
Final training  4664/4999 loss: 0.7722 time 26.15s
Final training  4665/4999 loss: 0.7524 time 26.22s
Final training  4666/4999 loss: 0.7559 time 26.13s
Final training  4667/4999 loss: 0.7596 time 26.34s
Final training  4668/4999 loss: 0.7837 time 26.48s
Final training  4669/4999 loss: 0.7987 time 26.46s
Final training  4670/4999 loss: 0.7856 time 26.36s
Final training  4671/4999 loss: 0.7561 time 26.46s
Final training  4672/4999 loss: 0.7667 time 26.25s
Final training  4673/4999 loss: 0.7682 time 25.99s
Final training  4674/4999 loss: 0.7686 time 26.14s
Final training  4675/4999 loss: 0.8120 time 26.00s
Final training  4676/4999 loss: 0.7534 time 26.27s
Final training  4677/4999 loss: 0.7818 time 26.28s
Final training  4678/4999 loss: 0.7769 time 26.20s
Final training  4679/4999 loss: 0.7754 time 26.69s
Final training  4680/4999 loss: 0.7784 time 26.21s
Final training  4681/4999 loss: 0.7707 time 26.44s
Final training  4682/4999 loss: 0.7548 time 25.73s
Final training  4683/4999 loss: 0.7779 time 26.21s
Final training  4684/4999 loss: 0.7665 time 26.36s
Final training  4685/4999 loss: 0.7613 time 26.34s
Final training  4686/4999 loss: 0.7944 time 26.52s
Final training  4687/4999 loss: 0.7775 time 26.55s
Final training  4688/4999 loss: 0.8119 time 26.59s
Final training  4689/4999 loss: 0.7626 time 26.15s
Final training  4690/4999 loss: 0.7899 time 26.13s
Final training  4691/4999 loss: 0.7984 time 26.31s
Final training  4692/4999 loss: 0.7897 time 26.65s
Final training  4693/4999 loss: 0.7776 time 26.72s
Final training  4694/4999 loss: 0.7783 time 26.96s
Final training  4695/4999 loss: 0.7763 time 26.50s
Final training  4696/4999 loss: 0.7520 time 26.43s
Final training  4697/4999 loss: 0.7611 time 26.33s
Final training  4698/4999 loss: 0.7661 time 25.99s
Final training  4699/4999 loss: 0.7657 time 26.18s
Dice accuracy for each class:  (tensor([0.9959, 0.9058, 0.9440, 0.9432, 0.7770, 0.7199, 0.9177, 0.7740, 0.9056,
        0.8474, 0.7555, 0.7993, 0.6797, 0.6729], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4699/4999 acc [ 0.832] time 195.13s
trigger times: 7
Final training  4700/4999 loss: 0.7638 time 25.83s
Final training  4701/4999 loss: 0.7351 time 25.97s
Final training  4702/4999 loss: 0.7974 time 25.98s
Final training  4703/4999 loss: 0.7878 time 26.65s
Final training  4704/4999 loss: 0.7866 time 26.47s
Final training  4705/4999 loss: 0.7789 time 26.01s
Final training  4706/4999 loss: 0.7723 time 26.47s
Final training  4707/4999 loss: 0.7737 time 26.19s
Final training  4708/4999 loss: 0.7662 time 26.42s
Final training  4709/4999 loss: 0.7633 time 26.45s
Final training  4710/4999 loss: 0.7891 time 26.48s
Final training  4711/4999 loss: 0.7923 time 26.34s
Final training  4712/4999 loss: 0.7810 time 26.26s
Final training  4713/4999 loss: 0.7942 time 26.01s
Final training  4714/4999 loss: 0.8046 time 26.10s
Final training  4715/4999 loss: 0.7900 time 26.12s
Final training  4716/4999 loss: 0.7624 time 25.92s
Final training  4717/4999 loss: 0.7559 time 26.42s
Final training  4718/4999 loss: 0.7740 time 25.75s
Final training  4719/4999 loss: 0.7605 time 25.83s
Final training  4720/4999 loss: 0.7687 time 25.91s
Final training  4721/4999 loss: 0.7918 time 25.87s
Final training  4722/4999 loss: 0.7728 time 26.32s
Final training  4723/4999 loss: 0.7642 time 25.92s
Final training  4724/4999 loss: 0.7711 time 26.10s
Final training  4725/4999 loss: 0.7791 time 26.15s
Final training  4726/4999 loss: 0.7649 time 26.09s
Final training  4727/4999 loss: 0.7449 time 25.92s
Final training  4728/4999 loss: 0.7838 time 26.40s
Final training  4729/4999 loss: 0.7852 time 26.37s
Final training  4730/4999 loss: 0.7821 time 25.69s
Final training  4731/4999 loss: 0.7654 time 26.05s
Final training  4732/4999 loss: 0.7773 time 26.07s
Final training  4733/4999 loss: 0.7847 time 25.98s
Final training  4734/4999 loss: 0.7975 time 26.45s
Final training  4735/4999 loss: 0.7811 time 26.40s
Final training  4736/4999 loss: 0.7975 time 26.00s
Final training  4737/4999 loss: 0.7802 time 26.25s
Final training  4738/4999 loss: 0.7900 time 26.00s
Final training  4739/4999 loss: 0.7794 time 26.02s
Final training  4740/4999 loss: 0.7691 time 26.79s
Final training  4741/4999 loss: 0.7959 time 26.55s
Final training  4742/4999 loss: 0.7898 time 25.95s
Final training  4743/4999 loss: 0.7813 time 26.45s
Final training  4744/4999 loss: 0.7519 time 26.36s
Final training  4745/4999 loss: 0.7706 time 26.57s
Final training  4746/4999 loss: 0.7716 time 25.83s
Final training  4747/4999 loss: 0.7714 time 26.45s
Final training  4748/4999 loss: 0.7772 time 25.80s
Final training  4749/4999 loss: 0.7724 time 26.06s
Final training  4750/4999 loss: 0.7757 time 26.41s
Final training  4751/4999 loss: 0.7764 time 26.20s
Final training  4752/4999 loss: 0.7560 time 26.49s
Final training  4753/4999 loss: 0.7769 time 26.10s
Final training  4754/4999 loss: 0.7788 time 25.84s
Final training  4755/4999 loss: 0.7742 time 26.34s
Final training  4756/4999 loss: 0.7608 time 26.13s
Final training  4757/4999 loss: 0.7891 time 26.32s
Final training  4758/4999 loss: 0.7558 time 26.67s
Final training  4759/4999 loss: 0.7774 time 26.33s
Final training  4760/4999 loss: 0.7647 time 26.28s
Final training  4761/4999 loss: 0.7751 time 26.72s
Final training  4762/4999 loss: 0.7638 time 26.43s
Final training  4763/4999 loss: 0.7671 time 26.15s
Final training  4764/4999 loss: 0.7797 time 25.85s
Final training  4765/4999 loss: 0.7911 time 25.90s
Final training  4766/4999 loss: 0.7702 time 26.00s
Final training  4767/4999 loss: 0.7681 time 26.32s
Final training  4768/4999 loss: 0.7908 time 26.26s
Final training  4769/4999 loss: 0.7687 time 26.08s
Final training  4770/4999 loss: 0.7865 time 26.64s
Final training  4771/4999 loss: 0.7747 time 26.11s
Final training  4772/4999 loss: 0.7986 time 26.11s
Final training  4773/4999 loss: 0.7822 time 25.24s
Final training  4774/4999 loss: 0.7781 time 25.70s
Final training  4775/4999 loss: 0.7609 time 25.98s
Final training  4776/4999 loss: 0.7360 time 25.89s
Final training  4777/4999 loss: 0.7682 time 26.45s
Final training  4778/4999 loss: 0.7960 time 26.36s
Final training  4779/4999 loss: 0.7737 time 26.58s
Final training  4780/4999 loss: 0.7678 time 26.57s
Final training  4781/4999 loss: 0.7774 time 26.55s
Final training  4782/4999 loss: 0.7640 time 26.36s
Final training  4783/4999 loss: 0.7605 time 26.19s
Final training  4784/4999 loss: 0.7801 time 26.71s
Final training  4785/4999 loss: 0.7994 time 26.18s
Final training  4786/4999 loss: 0.7557 time 26.48s
Final training  4787/4999 loss: 0.7876 time 26.27s
Final training  4788/4999 loss: 0.7706 time 26.41s
Final training  4789/4999 loss: 0.7859 time 26.42s
Final training  4790/4999 loss: 0.7774 time 26.75s
Final training  4791/4999 loss: 0.7638 time 26.10s
Final training  4792/4999 loss: 0.7812 time 26.10s
Final training  4793/4999 loss: 0.7511 time 26.42s
Final training  4794/4999 loss: 0.7875 time 26.01s
Final training  4795/4999 loss: 0.7461 time 26.21s
Final training  4796/4999 loss: 0.7731 time 25.95s
Final training  4797/4999 loss: 0.7570 time 25.92s
Final training  4798/4999 loss: 0.7683 time 25.89s
Final training  4799/4999 loss: 0.7574 time 26.44s
Dice accuracy for each class:  (tensor([0.9958, 0.9044, 0.9443, 0.9432, 0.7734, 0.7168, 0.9172, 0.7719, 0.9060,
        0.8510, 0.7551, 0.8002, 0.6797, 0.6721], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4799/4999 acc [ 0.831] time 196.54s
trigger times: 8
Final training  4800/4999 loss: 0.7712 time 26.27s
Final training  4801/4999 loss: 0.7858 time 26.68s
Final training  4802/4999 loss: 0.7855 time 26.52s
Final training  4803/4999 loss: 0.7666 time 26.30s
Final training  4804/4999 loss: 0.7818 time 26.69s
Final training  4805/4999 loss: 0.7963 time 26.63s
Final training  4806/4999 loss: 0.7940 time 26.19s
Final training  4807/4999 loss: 0.7875 time 26.71s
Final training  4808/4999 loss: 0.7560 time 26.32s
Final training  4809/4999 loss: 0.7704 time 26.27s
Final training  4810/4999 loss: 0.7812 time 26.09s
Final training  4811/4999 loss: 0.7780 time 26.40s
Final training  4812/4999 loss: 0.7667 time 26.50s
Final training  4813/4999 loss: 0.7737 time 26.27s
Final training  4814/4999 loss: 0.7761 time 26.46s
Final training  4815/4999 loss: 0.7871 time 26.42s
Final training  4816/4999 loss: 0.7971 time 26.34s
Final training  4817/4999 loss: 0.7830 time 26.10s
Final training  4818/4999 loss: 0.7602 time 26.20s
Final training  4819/4999 loss: 0.7755 time 26.13s
Final training  4820/4999 loss: 0.7814 time 26.19s
Final training  4821/4999 loss: 0.7838 time 26.16s
Final training  4822/4999 loss: 0.7729 time 26.36s
Final training  4823/4999 loss: 0.7780 time 26.55s
Final training  4824/4999 loss: 0.7727 time 26.05s
Final training  4825/4999 loss: 0.8095 time 26.28s
Final training  4826/4999 loss: 0.7851 time 25.99s
Final training  4827/4999 loss: 0.7942 time 26.54s
Final training  4828/4999 loss: 0.7882 time 26.25s
Final training  4829/4999 loss: 0.7808 time 26.28s
Final training  4830/4999 loss: 0.7735 time 25.99s
Final training  4831/4999 loss: 0.7953 time 25.91s
Final training  4832/4999 loss: 0.7906 time 26.31s
Final training  4833/4999 loss: 0.7643 time 26.24s
Final training  4834/4999 loss: 0.7813 time 26.59s
Final training  4835/4999 loss: 0.7786 time 26.52s
Final training  4836/4999 loss: 0.7694 time 26.02s
Final training  4837/4999 loss: 0.7834 time 26.89s
Final training  4838/4999 loss: 0.7618 time 26.70s
Final training  4839/4999 loss: 0.7609 time 26.35s
Final training  4840/4999 loss: 0.7353 time 26.42s
Final training  4841/4999 loss: 0.7624 time 26.35s
Final training  4842/4999 loss: 0.7844 time 26.19s
Final training  4843/4999 loss: 0.7801 time 26.19s
Final training  4844/4999 loss: 0.7744 time 26.01s
Final training  4845/4999 loss: 0.7989 time 25.91s
Final training  4846/4999 loss: 0.7566 time 26.00s
Final training  4847/4999 loss: 0.8019 time 25.95s
Final training  4848/4999 loss: 0.7862 time 26.14s
Final training  4849/4999 loss: 0.7645 time 26.23s
Final training  4850/4999 loss: 0.7690 time 26.17s
Final training  4851/4999 loss: 0.7481 time 25.93s
Final training  4852/4999 loss: 0.7793 time 26.22s
Final training  4853/4999 loss: 0.7761 time 26.44s
Final training  4854/4999 loss: 0.7747 time 26.51s
Final training  4855/4999 loss: 0.7955 time 26.16s
Final training  4856/4999 loss: 0.7829 time 26.41s
Final training  4857/4999 loss: 0.7918 time 26.19s
Final training  4858/4999 loss: 0.7509 time 26.44s
Final training  4859/4999 loss: 0.7835 time 26.04s
Final training  4860/4999 loss: 0.8061 time 26.49s
Final training  4861/4999 loss: 0.7987 time 26.38s
Final training  4862/4999 loss: 0.7851 time 26.29s
Final training  4863/4999 loss: 0.7652 time 26.61s
Final training  4864/4999 loss: 0.7768 time 26.05s
Final training  4865/4999 loss: 0.7727 time 26.58s
Final training  4866/4999 loss: 0.7839 time 26.62s
Final training  4867/4999 loss: 0.7420 time 26.44s
Final training  4868/4999 loss: 0.7626 time 26.71s
Final training  4869/4999 loss: 0.7700 time 26.81s
Final training  4870/4999 loss: 0.7913 time 26.44s
Final training  4871/4999 loss: 0.7952 time 26.53s
Final training  4872/4999 loss: 0.7860 time 27.08s
Final training  4873/4999 loss: 0.7945 time 26.56s
Final training  4874/4999 loss: 0.7912 time 26.18s
Final training  4875/4999 loss: 0.7386 time 26.97s
Final training  4876/4999 loss: 0.7612 time 26.69s
Final training  4877/4999 loss: 0.7630 time 26.75s
Final training  4878/4999 loss: 0.7682 time 26.40s
Final training  4879/4999 loss: 0.7537 time 26.38s
Final training  4880/4999 loss: 0.7830 time 26.35s
Final training  4881/4999 loss: 0.7609 time 26.69s
Final training  4882/4999 loss: 0.7485 time 26.41s
Final training  4883/4999 loss: 0.7617 time 26.14s
Final training  4884/4999 loss: 0.8040 time 26.19s
Final training  4885/4999 loss: 0.7436 time 26.24s
Final training  4886/4999 loss: 0.7764 time 26.26s
Final training  4887/4999 loss: 0.7569 time 26.40s
Final training  4888/4999 loss: 0.7920 time 26.33s
Final training  4889/4999 loss: 0.7956 time 26.49s
Final training  4890/4999 loss: 0.7827 time 26.22s
Final training  4891/4999 loss: 0.7729 time 26.09s
Final training  4892/4999 loss: 0.7494 time 26.63s
Final training  4893/4999 loss: 0.7686 time 26.42s
Final training  4894/4999 loss: 0.7691 time 26.47s
Final training  4895/4999 loss: 0.8029 time 26.28s
Final training  4896/4999 loss: 0.7665 time 26.28s
Final training  4897/4999 loss: 0.7868 time 26.61s
Final training  4898/4999 loss: 0.7904 time 26.48s
Final training  4899/4999 loss: 0.7677 time 26.34s
Dice accuracy for each class:  (tensor([0.9959, 0.9030, 0.9443, 0.9433, 0.7713, 0.7168, 0.9186, 0.7732, 0.9060,
        0.8489, 0.7555, 0.7977, 0.6797, 0.6763], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4899/4999 acc [ 0.831] time 196.56s
trigger times: 9
Final training  4900/4999 loss: 0.7951 time 26.91s
Final training  4901/4999 loss: 0.7852 time 26.72s
Final training  4902/4999 loss: 0.7739 time 26.03s
Final training  4903/4999 loss: 0.7840 time 26.38s
Final training  4904/4999 loss: 0.7906 time 26.56s
Final training  4905/4999 loss: 0.8002 time 26.05s
Final training  4906/4999 loss: 0.7512 time 26.33s
Final training  4907/4999 loss: 0.7845 time 26.76s
Final training  4908/4999 loss: 0.7792 time 26.54s
Final training  4909/4999 loss: 0.7736 time 26.76s
Final training  4910/4999 loss: 0.7825 time 26.62s
Final training  4911/4999 loss: 0.7356 time 26.45s
Final training  4912/4999 loss: 0.7872 time 26.09s
Final training  4913/4999 loss: 0.7781 time 26.32s
Final training  4914/4999 loss: 0.7688 time 26.15s
Final training  4915/4999 loss: 0.7772 time 26.34s
Final training  4916/4999 loss: 0.7605 time 26.52s
Final training  4917/4999 loss: 0.7563 time 26.26s
Final training  4918/4999 loss: 0.7580 time 26.21s
Final training  4919/4999 loss: 0.7770 time 26.37s
Final training  4920/4999 loss: 0.7695 time 26.42s
Final training  4921/4999 loss: 0.7717 time 26.34s
Final training  4922/4999 loss: 0.7389 time 26.18s
Final training  4923/4999 loss: 0.8038 time 26.13s
Final training  4924/4999 loss: 0.7821 time 26.61s
Final training  4925/4999 loss: 0.7868 time 26.35s
Final training  4926/4999 loss: 0.7653 time 26.42s
Final training  4927/4999 loss: 0.7787 time 26.58s
Final training  4928/4999 loss: 0.7919 time 26.64s
Final training  4929/4999 loss: 0.7632 time 26.48s
Final training  4930/4999 loss: 0.8019 time 26.23s
Final training  4931/4999 loss: 0.7667 time 26.72s
Final training  4932/4999 loss: 0.7866 time 26.25s
Final training  4933/4999 loss: 0.7577 time 25.93s
Final training  4934/4999 loss: 0.7564 time 25.97s
Final training  4935/4999 loss: 0.7647 time 25.68s
Final training  4936/4999 loss: 0.7711 time 26.04s
Final training  4937/4999 loss: 0.7528 time 26.44s
Final training  4938/4999 loss: 0.7785 time 26.16s
Final training  4939/4999 loss: 0.7948 time 26.39s
Final training  4940/4999 loss: 0.7581 time 26.64s
Final training  4941/4999 loss: 0.7855 time 26.29s
Final training  4942/4999 loss: 0.7681 time 26.30s
Final training  4943/4999 loss: 0.8080 time 25.66s
Final training  4944/4999 loss: 0.7631 time 26.22s
Final training  4945/4999 loss: 0.7843 time 25.71s
Final training  4946/4999 loss: 0.7824 time 26.01s
Final training  4947/4999 loss: 0.7756 time 25.94s
Final training  4948/4999 loss: 0.7713 time 26.20s
Final training  4949/4999 loss: 0.7838 time 26.07s
Final training  4950/4999 loss: 0.7807 time 25.62s
Final training  4951/4999 loss: 0.7870 time 26.04s
Final training  4952/4999 loss: 0.7927 time 25.69s
Final training  4953/4999 loss: 0.7609 time 25.58s
Final training  4954/4999 loss: 0.7739 time 25.81s
Final training  4955/4999 loss: 0.7622 time 25.85s
Final training  4956/4999 loss: 0.7665 time 25.48s
Final training  4957/4999 loss: 0.7975 time 25.23s
Final training  4958/4999 loss: 0.8077 time 25.55s
Final training  4959/4999 loss: 0.7826 time 25.58s
Final training  4960/4999 loss: 0.7827 time 25.78s
Final training  4961/4999 loss: 0.7928 time 25.52s
Final training  4962/4999 loss: 0.7867 time 25.68s
Final training  4963/4999 loss: 0.7862 time 25.52s
Final training  4964/4999 loss: 0.7935 time 25.83s
Final training  4965/4999 loss: 0.7684 time 25.46s
Final training  4966/4999 loss: 0.7753 time 25.62s
Final training  4967/4999 loss: 0.7981 time 25.68s
Final training  4968/4999 loss: 0.7898 time 25.67s
Final training  4969/4999 loss: 0.7605 time 25.77s
Final training  4970/4999 loss: 0.7749 time 25.77s
Final training  4971/4999 loss: 0.7660 time 25.53s
Final training  4972/4999 loss: 0.7756 time 25.63s
Final training  4973/4999 loss: 0.7744 time 25.40s
Final training  4974/4999 loss: 0.7787 time 25.35s
Final training  4975/4999 loss: 0.7908 time 25.80s
Final training  4976/4999 loss: 0.7559 time 25.59s
Final training  4977/4999 loss: 0.7828 time 25.49s
Final training  4978/4999 loss: 0.7750 time 25.53s
Final training  4979/4999 loss: 0.7616 time 25.53s
Final training  4980/4999 loss: 0.7803 time 25.68s
Final training  4981/4999 loss: 0.7642 time 25.72s
Final training  4982/4999 loss: 0.7853 time 25.69s
Final training  4983/4999 loss: 0.7355 time 25.81s
Final training  4984/4999 loss: 0.7643 time 26.02s
Final training  4985/4999 loss: 0.7912 time 25.54s
Final training  4986/4999 loss: 0.8009 time 25.63s
Final training  4987/4999 loss: 0.7581 time 25.50s
Final training  4988/4999 loss: 0.7841 time 25.64s
Final training  4989/4999 loss: 0.7558 time 25.10s
Final training  4990/4999 loss: 0.7801 time 25.67s
Final training  4991/4999 loss: 0.7934 time 25.92s
Final training  4992/4999 loss: 0.7735 time 25.64s
Final training  4993/4999 loss: 0.7783 time 25.55s
Final training  4994/4999 loss: 0.7868 time 25.61s
Final training  4995/4999 loss: 0.7928 time 25.75s
Final training  4996/4999 loss: 0.7810 time 25.69s
Final training  4997/4999 loss: 0.7623 time 25.40s
Final training  4998/4999 loss: 0.7698 time 25.79s
Final training  4999/4999 loss: 0.7620 time 25.91s
Dice accuracy for each class:  (tensor([0.9959, 0.9029, 0.9443, 0.9433, 0.7711, 0.7169, 0.9187, 0.7739, 0.9060,
        0.8494, 0.7555, 0.7980, 0.6792, 0.6756], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4999/4999 acc [ 0.831] time 195.22s
trigger times: 10
Training Finished !, Best Accuracy:  0.8341792225837708


Training Finished !, Best mean Validation Accuracy:  0.8341792225837708
