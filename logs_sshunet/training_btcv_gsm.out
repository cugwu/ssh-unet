------------------------------TRAINING OF MODEL 1------------------------------
You are using the following device: cuda
Batch size is: 2 epochs 5000
--------------------Folder 0-------------------

Cross Entropy Dice Loss
Total parameters count 6513854
Filters: [32, 64, 128, 256, 320],
Kernels: [[1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3]]
Strides: [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]]
GateDynUNet(
  (input_block): GateUnetResBlock(
    (conv1): Convolution(
      (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (conv2): Convolution(
      (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
    (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (shift): Gsm(
      (conv3D): Conv3d(32, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
      (tanh): Tanh()
      (bn): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (relu): LeakyReLU(negative_slope=0.1)
    )
    (conv3): Convolution(
      (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  )
  (downsamples): ModuleList(
    (0): GateUnetResBlock(
      (conv1): Convolution(
        (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv2): Convolution(
        (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
      (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (shift): Gsm(
        (conv3D): Conv3d(64, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
        (tanh): Tanh()
        (bn): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (relu): LeakyReLU(negative_slope=0.1)
      )
      (conv3): Convolution(
        (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (1): GateUnetResBlock(
      (conv1): Convolution(
        (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv2): Convolution(
        (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
      (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (shift): Gsm(
        (conv3D): Conv3d(128, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
        (tanh): Tanh()
        (bn): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (relu): LeakyReLU(negative_slope=0.1)
      )
      (conv3): Convolution(
        (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (2): GateUnetResBlock(
      (conv1): Convolution(
        (conv): Conv3d(128, 256, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv2): Convolution(
        (conv): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
      (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (shift): Gsm(
        (conv3D): Conv3d(256, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
        (tanh): Tanh()
        (bn): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (relu): LeakyReLU(negative_slope=0.1)
      )
      (conv3): Convolution(
        (conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
  )
  (bottleneck): GateUnetResBlock(
    (conv1): Convolution(
      (conv): Conv3d(256, 320, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (conv2): Convolution(
      (conv): Conv3d(320, 320, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
    (norm1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (norm2): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (shift): Gsm(
      (conv3D): Conv3d(320, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
      (tanh): Tanh()
      (bn): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (relu): LeakyReLU(negative_slope=0.1)
    )
    (conv3): Convolution(
      (conv): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (norm3): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  )
  (upsamples): ModuleList(
    (0): GateUnetUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv_block): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(512, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (shift): Gsm(
          (conv3D): Conv3d(256, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
          (tanh): Tanh()
          (bn): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (relu): LeakyReLU(negative_slope=0.1)
        )
        (conv3): Convolution(
          (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (1): GateUnetUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv_block): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (shift): Gsm(
          (conv3D): Conv3d(128, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
          (tanh): Tanh()
          (bn): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (relu): LeakyReLU(negative_slope=0.1)
        )
        (conv3): Convolution(
          (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (2): GateUnetUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv_block): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (shift): Gsm(
          (conv3D): Conv3d(64, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
          (tanh): Tanh()
          (bn): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (relu): LeakyReLU(negative_slope=0.1)
        )
        (conv3): Convolution(
          (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (3): GateUnetUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv_block): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
  )
  (output_block): GateUnetOutBlock(
    (conv1): Convolution(
      (conv): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
    (act): LeakyReLU(negative_slope=0.01)
    (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (conv): Convolution(
      (conv): Conv3d(32, 14, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      (adn): ADN(
        (D): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (skip_layers): DynUNetSkipLayer(
    (downsample): GateUnetResBlock(
      (conv1): Convolution(
        (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv2): Convolution(
        (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
      (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (shift): Gsm(
        (conv3D): Conv3d(32, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
        (tanh): Tanh()
        (bn): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (relu): LeakyReLU(negative_slope=0.1)
      )
      (conv3): Convolution(
        (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (next_layer): DynUNetSkipLayer(
      (downsample): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (shift): Gsm(
          (conv3D): Conv3d(64, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
          (tanh): Tanh()
          (bn): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (relu): LeakyReLU(negative_slope=0.1)
        )
        (conv3): Convolution(
          (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
      (next_layer): DynUNetSkipLayer(
        (downsample): GateUnetResBlock(
          (conv1): Convolution(
            (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (conv2): Convolution(
            (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (shift): Gsm(
            (conv3D): Conv3d(128, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
            (tanh): Tanh()
            (bn): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (relu): LeakyReLU(negative_slope=0.1)
          )
          (conv3): Convolution(
            (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
        (next_layer): DynUNetSkipLayer(
          (downsample): GateUnetResBlock(
            (conv1): Convolution(
              (conv): Conv3d(128, 256, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (conv2): Convolution(
              (conv): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
            (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (shift): Gsm(
              (conv3D): Conv3d(256, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
              (tanh): Tanh()
              (bn): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (relu): LeakyReLU(negative_slope=0.1)
            )
            (conv3): Convolution(
              (conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (next_layer): GateUnetResBlock(
            (conv1): Convolution(
              (conv): Conv3d(256, 320, kernel_size=(1, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (conv2): Convolution(
              (conv): Conv3d(320, 320, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
            (norm1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (norm2): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (shift): Gsm(
              (conv3D): Conv3d(320, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
              (tanh): Tanh()
              (bn): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (relu): LeakyReLU(negative_slope=0.1)
            )
            (conv3): Convolution(
              (conv): Conv3d(256, 320, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (upsample): GateUnetUpBlock(
            (transp_conv): Convolution(
              (conv): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (conv_block): GateUnetResBlock(
              (conv1): Convolution(
                (conv): Conv3d(512, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
                (adn): ADN(
                  (D): Dropout(p=0.0, inplace=False)
                )
              )
              (conv2): Convolution(
                (conv): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
                (adn): ADN(
                  (D): Dropout(p=0.0, inplace=False)
                )
              )
              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
              (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (shift): Gsm(
                (conv3D): Conv3d(256, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
                (tanh): Tanh()
                (bn): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (relu): LeakyReLU(negative_slope=0.1)
              )
              (conv3): Convolution(
                (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (adn): ADN(
                  (D): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            )
          )
        )
        (upsample): GateUnetUpBlock(
          (transp_conv): Convolution(
            (conv): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (conv_block): GateUnetResBlock(
            (conv1): Convolution(
              (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (conv2): Convolution(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
            (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (shift): Gsm(
              (conv3D): Conv3d(128, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
              (tanh): Tanh()
              (bn): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (relu): LeakyReLU(negative_slope=0.1)
            )
            (conv3): Convolution(
              (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
              (adn): ADN(
                (D): Dropout(p=0.0, inplace=False)
              )
            )
            (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (upsample): GateUnetUpBlock(
        (transp_conv): Convolution(
          (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv_block): GateUnetResBlock(
          (conv1): Convolution(
            (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (conv2): Convolution(
            (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          (shift): Gsm(
            (conv3D): Conv3d(64, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=2)
            (tanh): Tanh()
            (bn): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (relu): LeakyReLU(negative_slope=0.1)
          )
          (conv3): Convolution(
            (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
            (adn): ADN(
              (D): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        )
      )
    )
    (upsample): GateUnetUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))
        (adn): ADN(
          (D): Dropout(p=0.0, inplace=False)
        )
      )
      (conv_block): GateUnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (adn): ADN(
            (D): Dropout(p=0.0, inplace=False)
          )
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
  )
)
Writing Tensorboard logs to  /data/vision_group/medical/btcv/results_gsm/logs
Final training  0/4999 loss: 4.6372 time 24.02s
Final training  1/4999 loss: 3.9867 time 24.03s
Final training  2/4999 loss: 3.6043 time 24.02s
Final training  3/4999 loss: 3.4345 time 24.18s
Final training  4/4999 loss: 3.2992 time 23.93s
Final training  5/4999 loss: 3.1630 time 23.76s
Final training  6/4999 loss: 3.0700 time 23.85s
Final training  7/4999 loss: 3.0074 time 23.92s
Final training  8/4999 loss: 2.9327 time 23.39s
Final training  9/4999 loss: 2.8737 time 23.88s
Final training  10/4999 loss: 2.7876 time 23.94s
Final training  11/4999 loss: 2.7725 time 24.05s
Final training  12/4999 loss: 2.6640 time 23.97s
Final training  13/4999 loss: 2.6494 time 23.46s
Final training  14/4999 loss: 2.5923 time 24.01s
Final training  15/4999 loss: 2.5250 time 24.10s
Final training  16/4999 loss: 2.5528 time 23.25s
Final training  17/4999 loss: 2.4494 time 23.99s
Final training  18/4999 loss: 2.3661 time 23.72s
Final training  19/4999 loss: 2.3558 time 23.89s
Final training  20/4999 loss: 2.3294 time 23.60s
Final training  21/4999 loss: 2.3035 time 24.08s
Final training  22/4999 loss: 2.2654 time 23.91s
Final training  23/4999 loss: 2.2125 time 24.16s
Final training  24/4999 loss: 2.2181 time 24.08s
Final training  25/4999 loss: 2.1214 time 24.18s
Final training  26/4999 loss: 2.1381 time 24.08s
Final training  27/4999 loss: 2.1061 time 23.87s
Final training  28/4999 loss: 2.1262 time 24.07s
Final training  29/4999 loss: 2.0252 time 23.93s
Final training  30/4999 loss: 2.0043 time 23.85s
Final training  31/4999 loss: 1.9725 time 24.24s
Final training  32/4999 loss: 1.9938 time 24.05s
Final training  33/4999 loss: 1.9509 time 24.38s
Final training  34/4999 loss: 1.9245 time 23.81s
Final training  35/4999 loss: 1.9342 time 24.10s
Final training  36/4999 loss: 1.8726 time 24.27s
Final training  37/4999 loss: 1.8387 time 24.39s
Final training  38/4999 loss: 1.8278 time 23.95s
Final training  39/4999 loss: 1.7950 time 24.05s
Final training  40/4999 loss: 1.7528 time 24.17s
Final training  41/4999 loss: 1.7946 time 24.06s
Final training  42/4999 loss: 1.8699 time 24.26s
Final training  43/4999 loss: 1.7787 time 23.93s
Final training  44/4999 loss: 1.7206 time 24.30s
Final training  45/4999 loss: 1.7284 time 24.23s
Final training  46/4999 loss: 1.7158 time 23.90s
Final training  47/4999 loss: 1.7179 time 24.06s
Final training  48/4999 loss: 1.7008 time 24.18s
Final training  49/4999 loss: 1.6515 time 24.14s
Final training  50/4999 loss: 1.7182 time 24.39s
Final training  51/4999 loss: 1.7032 time 24.29s
Final training  52/4999 loss: 1.6666 time 24.41s
Final training  53/4999 loss: 1.6740 time 24.11s
Final training  54/4999 loss: 1.6717 time 24.49s
Final training  55/4999 loss: 1.6486 time 24.10s
Final training  56/4999 loss: 1.6597 time 24.43s
Final training  57/4999 loss: 1.6328 time 24.22s
Final training  58/4999 loss: 1.6696 time 24.52s
Final training  59/4999 loss: 1.6720 time 24.24s
Final training  60/4999 loss: 1.5882 time 24.06s
Final training  61/4999 loss: 1.6874 time 24.51s
Final training  62/4999 loss: 1.6107 time 24.37s
Final training  63/4999 loss: 1.5878 time 23.78s
Final training  64/4999 loss: 1.6602 time 24.22s
Final training  65/4999 loss: 1.5886 time 24.34s
Final training  66/4999 loss: 1.5027 time 24.31s
Final training  67/4999 loss: 1.5339 time 24.45s
Final training  68/4999 loss: 1.5829 time 24.42s
Final training  69/4999 loss: 1.6414 time 24.23s
Final training  70/4999 loss: 1.6399 time 24.31s
Final training  71/4999 loss: 1.5472 time 24.39s
Final training  72/4999 loss: 1.5190 time 24.23s
Final training  73/4999 loss: 1.4970 time 23.78s
Final training  74/4999 loss: 1.4925 time 24.28s
Final training  75/4999 loss: 1.5060 time 24.04s
Final training  76/4999 loss: 1.5574 time 23.75s
Final training  77/4999 loss: 1.5545 time 24.38s
Final training  78/4999 loss: 1.6403 time 24.10s
Final training  79/4999 loss: 1.5871 time 24.14s
Final training  80/4999 loss: 1.4637 time 24.16s
Final training  81/4999 loss: 1.5436 time 24.26s
Final training  82/4999 loss: 1.4733 time 24.29s
Final training  83/4999 loss: 1.5149 time 24.38s
Final training  84/4999 loss: 1.5604 time 24.58s
Final training  85/4999 loss: 1.5209 time 24.53s
Final training  86/4999 loss: 1.5704 time 24.25s
Final training  87/4999 loss: 1.4810 time 24.31s
Final training  88/4999 loss: 1.5081 time 24.13s
Final training  89/4999 loss: 1.4451 time 24.28s
Final training  90/4999 loss: 1.5036 time 24.14s
Final training  91/4999 loss: 1.4724 time 24.35s
Final training  92/4999 loss: 1.5591 time 24.38s
Final training  93/4999 loss: 1.4638 time 24.21s
Final training  94/4999 loss: 1.4996 time 24.23s
Final training  95/4999 loss: 1.4592 time 24.40s
Final training  96/4999 loss: 1.5358 time 24.60s
Final training  97/4999 loss: 1.5424 time 24.59s
Final training  98/4999 loss: 1.4378 time 24.65s
Final training  99/4999 loss: 1.4893 time 24.50s
Dice accuracy for each class:  (tensor([0.9864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7622, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  99/4999 acc [ 0.126] time 184.95s
Reset trigger time to 0
new best (0.000000 --> 0.126435). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  100/4999 loss: 1.5085 time 24.18s
Final training  101/4999 loss: 1.3999 time 24.32s
Final training  102/4999 loss: 1.4285 time 24.22s
Final training  103/4999 loss: 1.4115 time 24.09s
Final training  104/4999 loss: 1.5457 time 24.15s
Final training  105/4999 loss: 1.5324 time 24.21s
Final training  106/4999 loss: 1.5024 time 24.24s
Final training  107/4999 loss: 1.4517 time 23.99s
Final training  108/4999 loss: 1.4287 time 24.21s
Final training  109/4999 loss: 1.4204 time 24.18s
Final training  110/4999 loss: 1.5072 time 24.01s
Final training  111/4999 loss: 1.4596 time 24.04s
Final training  112/4999 loss: 1.5146 time 24.30s
Final training  113/4999 loss: 1.5511 time 24.20s
Final training  114/4999 loss: 1.4154 time 24.23s
Final training  115/4999 loss: 1.4486 time 24.13s
Final training  116/4999 loss: 1.4651 time 24.31s
Final training  117/4999 loss: 1.4478 time 24.26s
Final training  118/4999 loss: 1.3832 time 24.06s
Final training  119/4999 loss: 1.4628 time 24.22s
Final training  120/4999 loss: 1.4657 time 24.42s
Final training  121/4999 loss: 1.5241 time 24.48s
Final training  122/4999 loss: 1.4309 time 24.48s
Final training  123/4999 loss: 1.4171 time 24.55s
Final training  124/4999 loss: 1.4514 time 24.41s
Final training  125/4999 loss: 1.4132 time 24.19s
Final training  126/4999 loss: 1.4172 time 24.23s
Final training  127/4999 loss: 1.4644 time 24.18s
Final training  128/4999 loss: 1.4198 time 24.33s
Final training  129/4999 loss: 1.4314 time 24.46s
Final training  130/4999 loss: 1.4514 time 24.41s
Final training  131/4999 loss: 1.4197 time 24.21s
Final training  132/4999 loss: 1.5093 time 24.11s
Final training  133/4999 loss: 1.4215 time 24.36s
Final training  134/4999 loss: 1.4491 time 23.66s
Final training  135/4999 loss: 1.3599 time 24.07s
Final training  136/4999 loss: 1.5245 time 24.01s
Final training  137/4999 loss: 1.4920 time 24.22s
Final training  138/4999 loss: 1.4289 time 24.16s
Final training  139/4999 loss: 1.4619 time 24.05s
Final training  140/4999 loss: 1.4087 time 24.36s
Final training  141/4999 loss: 1.4123 time 24.69s
Final training  142/4999 loss: 1.4181 time 24.49s
Final training  143/4999 loss: 1.4935 time 24.44s
Final training  144/4999 loss: 1.3816 time 24.31s
Final training  145/4999 loss: 1.3519 time 24.23s
Final training  146/4999 loss: 1.4081 time 24.74s
Final training  147/4999 loss: 1.4131 time 24.28s
Final training  148/4999 loss: 1.4418 time 24.46s
Final training  149/4999 loss: 1.3715 time 24.47s
Final training  150/4999 loss: 1.4029 time 24.42s
Final training  151/4999 loss: 1.4115 time 24.55s
Final training  152/4999 loss: 1.3844 time 24.46s
Final training  153/4999 loss: 1.4248 time 24.56s
Final training  154/4999 loss: 1.4046 time 24.47s
Final training  155/4999 loss: 1.3286 time 24.67s
Final training  156/4999 loss: 1.4639 time 24.08s
Final training  157/4999 loss: 1.4272 time 24.47s
Final training  158/4999 loss: 1.3502 time 24.47s
Final training  159/4999 loss: 1.3515 time 24.43s
Final training  160/4999 loss: 1.3801 time 24.43s
Final training  161/4999 loss: 1.3450 time 24.41s
Final training  162/4999 loss: 1.4222 time 24.57s
Final training  163/4999 loss: 1.3513 time 24.67s
Final training  164/4999 loss: 1.3905 time 24.74s
Final training  165/4999 loss: 1.3398 time 24.63s
Final training  166/4999 loss: 1.3646 time 24.35s
Final training  167/4999 loss: 1.3687 time 24.55s
Final training  168/4999 loss: 1.3141 time 24.48s
Final training  169/4999 loss: 1.4047 time 24.45s
Final training  170/4999 loss: 1.3888 time 24.38s
Final training  171/4999 loss: 1.3526 time 24.49s
Final training  172/4999 loss: 1.3250 time 24.43s
Final training  173/4999 loss: 1.3689 time 24.59s
Final training  174/4999 loss: 1.3782 time 24.53s
Final training  175/4999 loss: 1.3213 time 24.82s
Final training  176/4999 loss: 1.4007 time 24.72s
Final training  177/4999 loss: 1.2995 time 24.70s
Final training  178/4999 loss: 1.3250 time 24.51s
Final training  179/4999 loss: 1.3830 time 24.84s
Final training  180/4999 loss: 1.3122 time 24.65s
Final training  181/4999 loss: 1.3751 time 24.69s
Final training  182/4999 loss: 1.4047 time 24.48s
Final training  183/4999 loss: 1.3781 time 24.54s
Final training  184/4999 loss: 1.3194 time 24.50s
Final training  185/4999 loss: 1.3641 time 24.70s
Final training  186/4999 loss: 1.4022 time 24.54s
Final training  187/4999 loss: 1.3243 time 24.84s
Final training  188/4999 loss: 1.3616 time 24.75s
Final training  189/4999 loss: 1.3774 time 24.26s
Final training  190/4999 loss: 1.3478 time 24.57s
Final training  191/4999 loss: 1.3087 time 24.85s
Final training  192/4999 loss: 1.3427 time 24.52s
Final training  193/4999 loss: 1.3168 time 24.61s
Final training  194/4999 loss: 1.2671 time 24.69s
Final training  195/4999 loss: 1.3481 time 24.39s
Final training  196/4999 loss: 1.2852 time 24.65s
Final training  197/4999 loss: 1.2602 time 24.66s
Final training  198/4999 loss: 1.3703 time 24.47s
Final training  199/4999 loss: 1.3234 time 24.74s
Dice accuracy for each class:  (tensor([0.9867, 0.1938, 0.3791, 0.1294, 0.0000, 0.0000, 0.7703, 0.0013, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  199/4999 acc [ 0.178] time 183.26s
Reset trigger time to 0
new best (0.126435 --> 0.177647). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  200/4999 loss: 1.3353 time 24.56s
Final training  201/4999 loss: 1.3906 time 24.63s
Final training  202/4999 loss: 1.3484 time 24.66s
Final training  203/4999 loss: 1.3980 time 24.63s
Final training  204/4999 loss: 1.3425 time 24.37s
Final training  205/4999 loss: 1.2997 time 24.67s
Final training  206/4999 loss: 1.3418 time 24.28s
Final training  207/4999 loss: 1.3065 time 24.48s
Final training  208/4999 loss: 1.2607 time 24.44s
Final training  209/4999 loss: 1.3205 time 23.98s
Final training  210/4999 loss: 1.2934 time 24.55s
Final training  211/4999 loss: 1.3062 time 24.37s
Final training  212/4999 loss: 1.2713 time 24.63s
Final training  213/4999 loss: 1.2686 time 24.86s
Final training  214/4999 loss: 1.3334 time 24.44s
Final training  215/4999 loss: 1.3951 time 24.75s
Final training  216/4999 loss: 1.2713 time 24.54s
Final training  217/4999 loss: 1.2616 time 24.59s
Final training  218/4999 loss: 1.2829 time 24.69s
Final training  219/4999 loss: 1.2824 time 24.57s
Final training  220/4999 loss: 1.2403 time 24.56s
Final training  221/4999 loss: 1.3490 time 24.67s
Final training  222/4999 loss: 1.2736 time 24.71s
Final training  223/4999 loss: 1.3738 time 24.79s
Final training  224/4999 loss: 1.3065 time 24.41s
Final training  225/4999 loss: 1.2932 time 24.54s
Final training  226/4999 loss: 1.2662 time 24.82s
Final training  227/4999 loss: 1.3153 time 24.61s
Final training  228/4999 loss: 1.2592 time 24.75s
Final training  229/4999 loss: 1.2284 time 24.78s
Final training  230/4999 loss: 1.3252 time 24.60s
Final training  231/4999 loss: 1.3135 time 24.58s
Final training  232/4999 loss: 1.3698 time 24.32s
Final training  233/4999 loss: 1.3045 time 24.40s
Final training  234/4999 loss: 1.2949 time 24.62s
Final training  235/4999 loss: 1.2858 time 24.88s
Final training  236/4999 loss: 1.2049 time 24.84s
Final training  237/4999 loss: 1.2810 time 24.47s
Final training  238/4999 loss: 1.2598 time 24.75s
Final training  239/4999 loss: 1.2264 time 24.12s
Final training  240/4999 loss: 1.2121 time 24.38s
Final training  241/4999 loss: 1.3134 time 23.91s
Final training  242/4999 loss: 1.2440 time 24.34s
Final training  243/4999 loss: 1.2484 time 24.38s
Final training  244/4999 loss: 1.3011 time 24.50s
Final training  245/4999 loss: 1.3625 time 24.86s
Final training  246/4999 loss: 1.3073 time 24.60s
Final training  247/4999 loss: 1.2682 time 24.72s
Final training  248/4999 loss: 1.2945 time 24.73s
Final training  249/4999 loss: 1.2522 time 24.86s
Final training  250/4999 loss: 1.3008 time 24.78s
Final training  251/4999 loss: 1.3007 time 24.53s
Final training  252/4999 loss: 1.3198 time 24.60s
Final training  253/4999 loss: 1.3369 time 24.52s
Final training  254/4999 loss: 1.1883 time 24.72s
Final training  255/4999 loss: 1.2481 time 24.52s
Final training  256/4999 loss: 1.2702 time 24.71s
Final training  257/4999 loss: 1.2724 time 24.63s
Final training  258/4999 loss: 1.3666 time 24.73s
Final training  259/4999 loss: 1.2765 time 24.46s
Final training  260/4999 loss: 1.3054 time 24.50s
Final training  261/4999 loss: 1.2104 time 24.49s
Final training  262/4999 loss: 1.3484 time 24.73s
Final training  263/4999 loss: 1.3243 time 24.63s
Final training  264/4999 loss: 1.2930 time 24.64s
Final training  265/4999 loss: 1.1883 time 24.58s
Final training  266/4999 loss: 1.2313 time 24.73s
Final training  267/4999 loss: 1.2762 time 23.67s
Final training  268/4999 loss: 1.2639 time 24.40s
Final training  269/4999 loss: 1.2577 time 24.76s
Final training  270/4999 loss: 1.2079 time 24.85s
Final training  271/4999 loss: 1.2421 time 24.80s
Final training  272/4999 loss: 1.2776 time 24.76s
Final training  273/4999 loss: 1.2480 time 24.68s
Final training  274/4999 loss: 1.2181 time 24.54s
Final training  275/4999 loss: 1.2262 time 25.08s
Final training  276/4999 loss: 1.2142 time 24.53s
Final training  277/4999 loss: 1.2890 time 24.43s
Final training  278/4999 loss: 1.2498 time 24.15s
Final training  279/4999 loss: 1.2683 time 24.66s
Final training  280/4999 loss: 1.2214 time 24.94s
Final training  281/4999 loss: 1.2235 time 24.74s
Final training  282/4999 loss: 1.2579 time 24.96s
Final training  283/4999 loss: 1.3075 time 24.56s
Final training  284/4999 loss: 1.2060 time 24.64s
Final training  285/4999 loss: 1.2216 time 24.66s
Final training  286/4999 loss: 1.1803 time 24.77s
Final training  287/4999 loss: 1.2363 time 24.94s
Final training  288/4999 loss: 1.2116 time 24.68s
Final training  289/4999 loss: 1.1961 time 24.76s
Final training  290/4999 loss: 1.2846 time 24.68s
Final training  291/4999 loss: 1.2033 time 24.49s
Final training  292/4999 loss: 1.1687 time 24.76s
Final training  293/4999 loss: 1.2047 time 24.44s
Final training  294/4999 loss: 1.2302 time 24.35s
Final training  295/4999 loss: 1.2184 time 24.63s
Final training  296/4999 loss: 1.1808 time 24.61s
Final training  297/4999 loss: 1.2191 time 24.43s
Final training  298/4999 loss: 1.1901 time 24.58s
Final training  299/4999 loss: 1.1626 time 25.11s
Dice accuracy for each class:  (tensor([0.9895, 0.4723, 0.7590, 0.7781, 0.0742, 0.0000, 0.7838, 0.4180, 0.3155,
        0.0813, 0.0226, 0.0000, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  299/4999 acc [ 0.339] time 181.72s
Reset trigger time to 0
new best (0.177647 --> 0.338731). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  300/4999 loss: 1.2768 time 24.13s
Final training  301/4999 loss: 1.2171 time 24.65s
Final training  302/4999 loss: 1.1906 time 24.64s
Final training  303/4999 loss: 1.1498 time 24.67s
Final training  304/4999 loss: 1.2042 time 24.98s
Final training  305/4999 loss: 1.2104 time 24.89s
Final training  306/4999 loss: 1.1904 time 24.47s
Final training  307/4999 loss: 1.1325 time 24.98s
Final training  308/4999 loss: 1.1470 time 24.69s
Final training  309/4999 loss: 1.1896 time 24.58s
Final training  310/4999 loss: 1.2219 time 23.83s
Final training  311/4999 loss: 1.1818 time 24.51s
Final training  312/4999 loss: 1.1284 time 24.37s
Final training  313/4999 loss: 1.1185 time 24.52s
Final training  314/4999 loss: 1.1748 time 24.66s
Final training  315/4999 loss: 1.1302 time 24.50s
Final training  316/4999 loss: 1.1920 time 24.41s
Final training  317/4999 loss: 1.1783 time 24.60s
Final training  318/4999 loss: 1.1388 time 24.51s
Final training  319/4999 loss: 1.1771 time 24.90s
Final training  320/4999 loss: 1.2127 time 24.89s
Final training  321/4999 loss: 1.1827 time 24.76s
Final training  322/4999 loss: 1.1706 time 24.51s
Final training  323/4999 loss: 1.1222 time 24.58s
Final training  324/4999 loss: 1.1592 time 24.77s
Final training  325/4999 loss: 1.1868 time 24.59s
Final training  326/4999 loss: 1.1633 time 24.52s
Final training  327/4999 loss: 1.1882 time 24.65s
Final training  328/4999 loss: 1.2000 time 24.69s
Final training  329/4999 loss: 1.1645 time 24.57s
Final training  330/4999 loss: 1.1812 time 24.44s
Final training  331/4999 loss: 1.1757 time 24.62s
Final training  332/4999 loss: 1.1533 time 25.08s
Final training  333/4999 loss: 1.1216 time 25.13s
Final training  334/4999 loss: 1.2184 time 25.18s
Final training  335/4999 loss: 1.0848 time 25.15s
Final training  336/4999 loss: 1.1459 time 25.23s
Final training  337/4999 loss: 1.1892 time 25.10s
Final training  338/4999 loss: 1.1528 time 24.74s
Final training  339/4999 loss: 1.1847 time 24.51s
Final training  340/4999 loss: 1.1592 time 24.72s
Final training  341/4999 loss: 1.1152 time 24.96s
Final training  342/4999 loss: 1.1648 time 24.77s
Final training  343/4999 loss: 1.1687 time 24.10s
Final training  344/4999 loss: 1.1674 time 24.32s
Final training  345/4999 loss: 1.1081 time 24.64s
Final training  346/4999 loss: 1.1303 time 24.60s
Final training  347/4999 loss: 1.1591 time 24.48s
Final training  348/4999 loss: 1.1640 time 24.58s
Final training  349/4999 loss: 1.1312 time 24.71s
Final training  350/4999 loss: 1.1515 time 24.54s
Final training  351/4999 loss: 1.1636 time 24.56s
Final training  352/4999 loss: 1.1332 time 24.53s
Final training  353/4999 loss: 1.1629 time 24.31s
Final training  354/4999 loss: 1.1263 time 24.41s
Final training  355/4999 loss: 1.1773 time 24.65s
Final training  356/4999 loss: 1.1178 time 24.67s
Final training  357/4999 loss: 1.1040 time 24.96s
Final training  358/4999 loss: 1.1379 time 24.53s
Final training  359/4999 loss: 1.1220 time 24.63s
Final training  360/4999 loss: 1.1705 time 24.83s
Final training  361/4999 loss: 1.1375 time 24.60s
Final training  362/4999 loss: 1.1093 time 24.65s
Final training  363/4999 loss: 1.1340 time 24.09s
Final training  364/4999 loss: 1.1182 time 24.49s
Final training  365/4999 loss: 1.1748 time 24.49s
Final training  366/4999 loss: 1.1161 time 24.34s
Final training  367/4999 loss: 1.1821 time 24.66s
Final training  368/4999 loss: 1.1585 time 24.87s
Final training  369/4999 loss: 1.1807 time 24.35s
Final training  370/4999 loss: 1.0859 time 24.54s
Final training  371/4999 loss: 1.1657 time 24.62s
Final training  372/4999 loss: 1.0818 time 24.64s
Final training  373/4999 loss: 1.1606 time 24.46s
Final training  374/4999 loss: 1.0890 time 24.59s
Final training  375/4999 loss: 1.1065 time 24.44s
Final training  376/4999 loss: 1.0922 time 24.43s
Final training  377/4999 loss: 1.1223 time 24.14s
Final training  378/4999 loss: 1.1176 time 24.66s
Final training  379/4999 loss: 1.0822 time 24.70s
Final training  380/4999 loss: 1.0442 time 24.49s
Final training  381/4999 loss: 1.0705 time 24.50s
Final training  382/4999 loss: 1.1166 time 24.53s
Final training  383/4999 loss: 1.1153 time 24.66s
Final training  384/4999 loss: 1.1206 time 24.62s
Final training  385/4999 loss: 1.1263 time 24.72s
Final training  386/4999 loss: 1.1092 time 24.48s
Final training  387/4999 loss: 1.1398 time 24.58s
Final training  388/4999 loss: 1.1000 time 24.44s
Final training  389/4999 loss: 1.1061 time 24.32s
Final training  390/4999 loss: 1.1174 time 24.69s
Final training  391/4999 loss: 1.0912 time 24.34s
Final training  392/4999 loss: 1.0771 time 24.57s
Final training  393/4999 loss: 1.0699 time 24.47s
Final training  394/4999 loss: 1.0948 time 24.67s
Final training  395/4999 loss: 1.0602 time 24.41s
Final training  396/4999 loss: 1.0511 time 24.73s
Final training  397/4999 loss: 1.1249 time 24.28s
Final training  398/4999 loss: 1.0947 time 24.62s
Final training  399/4999 loss: 1.1244 time 24.53s
Dice accuracy for each class:  (tensor([0.9922, 0.8880, 0.7927, 0.8151, 0.2845, 0.0000, 0.8568, 0.5711, 0.7959,
        0.6325, 0.4262, 0.1321, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  399/4999 acc [ 0.516] time 182.48s
Reset trigger time to 0
new best (0.338731 --> 0.516157). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  400/4999 loss: 1.1039 time 24.49s
Final training  401/4999 loss: 1.1045 time 24.56s
Final training  402/4999 loss: 1.0708 time 24.68s
Final training  403/4999 loss: 1.0955 time 24.44s
Final training  404/4999 loss: 1.1162 time 24.73s
Final training  405/4999 loss: 1.1040 time 25.08s
Final training  406/4999 loss: 1.1495 time 25.02s
Final training  407/4999 loss: 1.0895 time 24.87s
Final training  408/4999 loss: 1.0522 time 23.97s
Final training  409/4999 loss: 1.0632 time 24.40s
Final training  410/4999 loss: 1.1091 time 24.22s
Final training  411/4999 loss: 1.1139 time 24.47s
Final training  412/4999 loss: 1.0742 time 24.30s
Final training  413/4999 loss: 1.1377 time 24.34s
Final training  414/4999 loss: 1.1202 time 24.46s
Final training  415/4999 loss: 1.0982 time 24.33s
Final training  416/4999 loss: 1.0710 time 24.54s
Final training  417/4999 loss: 1.1121 time 24.55s
Final training  418/4999 loss: 1.1260 time 24.53s
Final training  419/4999 loss: 1.0647 time 24.72s
Final training  420/4999 loss: 1.0467 time 24.63s
Final training  421/4999 loss: 1.1073 time 24.44s
Final training  422/4999 loss: 1.0868 time 24.31s
Final training  423/4999 loss: 1.1129 time 24.22s
Final training  424/4999 loss: 1.0879 time 24.90s
Final training  425/4999 loss: 1.0786 time 24.58s
Final training  426/4999 loss: 1.0873 time 24.69s
Final training  427/4999 loss: 1.0485 time 24.69s
Final training  428/4999 loss: 1.0402 time 24.58s
Final training  429/4999 loss: 1.0889 time 24.54s
Final training  430/4999 loss: 1.0897 time 24.47s
Final training  431/4999 loss: 1.0454 time 24.47s
Final training  432/4999 loss: 1.0617 time 24.27s
Final training  433/4999 loss: 1.0771 time 24.41s
Final training  434/4999 loss: 1.1170 time 24.50s
Final training  435/4999 loss: 1.0925 time 24.48s
Final training  436/4999 loss: 1.1259 time 24.85s
Final training  437/4999 loss: 1.0629 time 24.83s
Final training  438/4999 loss: 1.0171 time 24.81s
Final training  439/4999 loss: 1.0097 time 24.99s
Final training  440/4999 loss: 1.0886 time 25.05s
Final training  441/4999 loss: 1.0378 time 25.03s
Final training  442/4999 loss: 1.0444 time 24.94s
Final training  443/4999 loss: 1.0920 time 24.86s
Final training  444/4999 loss: 1.0603 time 24.60s
Final training  445/4999 loss: 1.0479 time 24.52s
Final training  446/4999 loss: 1.0659 time 24.41s
Final training  447/4999 loss: 1.0763 time 24.77s
Final training  448/4999 loss: 1.0104 time 24.34s
Final training  449/4999 loss: 1.0648 time 24.66s
Final training  450/4999 loss: 1.0696 time 24.46s
Final training  451/4999 loss: 1.0258 time 24.67s
Final training  452/4999 loss: 1.0906 time 24.53s
Final training  453/4999 loss: 1.1205 time 24.80s
Final training  454/4999 loss: 1.0851 time 24.95s
Final training  455/4999 loss: 1.0649 time 25.12s
Final training  456/4999 loss: 1.0381 time 24.72s
Final training  457/4999 loss: 1.0836 time 24.91s
Final training  458/4999 loss: 1.0616 time 24.98s
Final training  459/4999 loss: 1.0398 time 24.69s
Final training  460/4999 loss: 1.0548 time 24.62s
Final training  461/4999 loss: 1.0986 time 24.48s
Final training  462/4999 loss: 1.1280 time 24.99s
Final training  463/4999 loss: 1.0770 time 24.45s
Final training  464/4999 loss: 1.0333 time 24.57s
Final training  465/4999 loss: 1.0522 time 24.69s
Final training  466/4999 loss: 1.1094 time 24.85s
Final training  467/4999 loss: 1.0877 time 24.89s
Final training  468/4999 loss: 1.0541 time 24.69s
Final training  469/4999 loss: 1.0311 time 24.94s
Final training  470/4999 loss: 1.0516 time 24.73s
Final training  471/4999 loss: 1.0382 time 24.64s
Final training  472/4999 loss: 1.0934 time 24.57s
Final training  473/4999 loss: 1.0412 time 24.75s
Final training  474/4999 loss: 1.0261 time 24.63s
Final training  475/4999 loss: 1.0587 time 24.40s
Final training  476/4999 loss: 1.0628 time 24.54s
Final training  477/4999 loss: 1.0443 time 24.46s
Final training  478/4999 loss: 1.0201 time 24.37s
Final training  479/4999 loss: 1.0675 time 24.67s
Final training  480/4999 loss: 1.0170 time 24.33s
Final training  481/4999 loss: 1.0475 time 24.34s
Final training  482/4999 loss: 1.0536 time 24.64s
Final training  483/4999 loss: 1.0183 time 24.63s
Final training  484/4999 loss: 1.0130 time 24.39s
Final training  485/4999 loss: 1.0298 time 24.40s
Final training  486/4999 loss: 1.0408 time 24.50s
Final training  487/4999 loss: 1.0199 time 24.66s
Final training  488/4999 loss: 1.0498 time 24.55s
Final training  489/4999 loss: 1.0263 time 24.73s
Final training  490/4999 loss: 1.0175 time 24.28s
Final training  491/4999 loss: 1.0174 time 24.69s
Final training  492/4999 loss: 0.9913 time 24.58s
Final training  493/4999 loss: 1.0276 time 24.67s
Final training  494/4999 loss: 1.0545 time 24.67s
Final training  495/4999 loss: 1.0264 time 24.67s
Final training  496/4999 loss: 1.0331 time 24.70s
Final training  497/4999 loss: 1.0357 time 24.76s
Final training  498/4999 loss: 1.0525 time 24.77s
Final training  499/4999 loss: 1.0517 time 24.53s
Dice accuracy for each class:  (tensor([0.9876, 0.8617, 0.8531, 0.8247, 0.4162, 0.0000, 0.7678, 0.4730, 0.7901,
        0.7151, 0.5635, 0.2620, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  499/4999 acc [ 0.538] time 183.50s
Reset trigger time to 0
new best (0.516157 --> 0.538300). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  500/4999 loss: 1.0444 time 24.72s
Final training  501/4999 loss: 1.0573 time 24.72s
Final training  502/4999 loss: 1.0732 time 24.60s
Final training  503/4999 loss: 1.0420 time 24.59s
Final training  504/4999 loss: 1.0361 time 24.76s
Final training  505/4999 loss: 1.1184 time 24.61s
Final training  506/4999 loss: 1.0387 time 24.54s
Final training  507/4999 loss: 1.0657 time 24.88s
Final training  508/4999 loss: 1.0555 time 24.77s
Final training  509/4999 loss: 1.0403 time 24.79s
Final training  510/4999 loss: 1.0659 time 24.76s
Final training  511/4999 loss: 1.0624 time 24.81s
Final training  512/4999 loss: 1.0542 time 24.84s
Final training  513/4999 loss: 1.0392 time 24.83s
Final training  514/4999 loss: 1.0786 time 24.74s
Final training  515/4999 loss: 1.0051 time 24.51s
Final training  516/4999 loss: 1.0181 time 24.73s
Final training  517/4999 loss: 1.0565 time 24.94s
Final training  518/4999 loss: 1.0265 time 24.86s
Final training  519/4999 loss: 1.0358 time 24.77s
Final training  520/4999 loss: 1.0133 time 24.66s
Final training  521/4999 loss: 1.0099 time 24.94s
Final training  522/4999 loss: 1.0194 time 24.75s
Final training  523/4999 loss: 1.0660 time 24.75s
Final training  524/4999 loss: 1.0319 time 24.71s
Final training  525/4999 loss: 0.9966 time 24.82s
Final training  526/4999 loss: 0.9966 time 24.62s
Final training  527/4999 loss: 1.0489 time 24.75s
Final training  528/4999 loss: 1.0420 time 24.83s
Final training  529/4999 loss: 1.0563 time 24.62s
Final training  530/4999 loss: 1.0517 time 24.74s
Final training  531/4999 loss: 1.0688 time 24.84s
Final training  532/4999 loss: 1.0690 time 24.68s
Final training  533/4999 loss: 1.0289 time 24.54s
Final training  534/4999 loss: 1.0752 time 24.64s
Final training  535/4999 loss: 1.0601 time 24.77s
Final training  536/4999 loss: 0.9911 time 24.68s
Final training  537/4999 loss: 1.0152 time 24.80s
Final training  538/4999 loss: 0.9921 time 24.74s
Final training  539/4999 loss: 0.9928 time 24.72s
Final training  540/4999 loss: 0.9724 time 24.74s
Final training  541/4999 loss: 0.9808 time 25.24s
Final training  542/4999 loss: 0.9717 time 25.04s
Final training  543/4999 loss: 0.9946 time 24.37s
Final training  544/4999 loss: 0.9851 time 24.66s
Final training  545/4999 loss: 1.0019 time 24.46s
Final training  546/4999 loss: 1.0546 time 24.66s
Final training  547/4999 loss: 1.0973 time 24.70s
Final training  548/4999 loss: 1.0435 time 24.54s
Final training  549/4999 loss: 1.0585 time 24.70s
Final training  550/4999 loss: 1.0268 time 24.59s
Final training  551/4999 loss: 1.0253 time 24.75s
Final training  552/4999 loss: 1.0041 time 24.73s
Final training  553/4999 loss: 1.0227 time 24.56s
Final training  554/4999 loss: 0.9683 time 25.01s
Final training  555/4999 loss: 0.9760 time 24.64s
Final training  556/4999 loss: 1.0351 time 24.84s
Final training  557/4999 loss: 1.0377 time 24.37s
Final training  558/4999 loss: 1.0139 time 24.63s
Final training  559/4999 loss: 1.0087 time 24.72s
Final training  560/4999 loss: 0.9858 time 24.39s
Final training  561/4999 loss: 1.0328 time 24.60s
Final training  562/4999 loss: 1.0779 time 24.64s
Final training  563/4999 loss: 0.9923 time 24.59s
Final training  564/4999 loss: 1.0002 time 24.60s
Final training  565/4999 loss: 0.9956 time 24.52s
Final training  566/4999 loss: 0.9737 time 24.59s
Final training  567/4999 loss: 1.0409 time 24.74s
Final training  568/4999 loss: 1.0201 time 24.42s
Final training  569/4999 loss: 0.9657 time 24.67s
Final training  570/4999 loss: 0.9812 time 24.65s
Final training  571/4999 loss: 0.9959 time 24.87s
Final training  572/4999 loss: 0.9646 time 24.86s
Final training  573/4999 loss: 1.0199 time 24.64s
Final training  574/4999 loss: 1.0076 time 24.60s
Final training  575/4999 loss: 0.9981 time 24.52s
Final training  576/4999 loss: 1.0014 time 24.86s
Final training  577/4999 loss: 0.9796 time 24.76s
Final training  578/4999 loss: 0.9968 time 24.79s
Final training  579/4999 loss: 1.0267 time 24.62s
Final training  580/4999 loss: 1.0058 time 24.96s
Final training  581/4999 loss: 0.9911 time 24.71s
Final training  582/4999 loss: 1.0039 time 24.66s
Final training  583/4999 loss: 0.9677 time 24.78s
Final training  584/4999 loss: 0.9874 time 24.51s
Final training  585/4999 loss: 1.0158 time 24.49s
Final training  586/4999 loss: 1.0067 time 24.14s
Final training  587/4999 loss: 0.9947 time 24.60s
Final training  588/4999 loss: 0.9828 time 24.51s
Final training  589/4999 loss: 1.0273 time 24.66s
Final training  590/4999 loss: 0.9821 time 24.36s
Final training  591/4999 loss: 0.9770 time 24.37s
Final training  592/4999 loss: 0.9754 time 24.84s
Final training  593/4999 loss: 1.0232 time 24.08s
Final training  594/4999 loss: 1.0203 time 24.71s
Final training  595/4999 loss: 1.0208 time 24.49s
Final training  596/4999 loss: 1.0058 time 24.83s
Final training  597/4999 loss: 0.9934 time 25.08s
Final training  598/4999 loss: 1.0112 time 24.99s
Final training  599/4999 loss: 0.9592 time 24.87s
Dice accuracy for each class:  (tensor([0.9921, 0.9061, 0.8984, 0.8996, 0.5139, 0.0000, 0.8169, 0.6805, 0.8385,
        0.7483, 0.6117, 0.5503, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  599/4999 acc [ 0.605] time 181.70s
Reset trigger time to 0
new best (0.538300 --> 0.604962). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  600/4999 loss: 0.9830 time 25.62s
Final training  601/4999 loss: 0.9658 time 25.35s
Final training  602/4999 loss: 0.9647 time 25.51s
Final training  603/4999 loss: 0.9574 time 25.16s
Final training  604/4999 loss: 1.0401 time 25.26s
Final training  605/4999 loss: 0.9945 time 25.35s
Final training  606/4999 loss: 1.0374 time 25.12s
Final training  607/4999 loss: 1.0384 time 25.17s
Final training  608/4999 loss: 0.9949 time 25.00s
Final training  609/4999 loss: 0.9869 time 25.08s
Final training  610/4999 loss: 0.9696 time 25.44s
Final training  611/4999 loss: 0.9716 time 25.34s
Final training  612/4999 loss: 0.9957 time 25.15s
Final training  613/4999 loss: 0.9769 time 25.20s
Final training  614/4999 loss: 0.9959 time 25.35s
Final training  615/4999 loss: 0.9618 time 25.28s
Final training  616/4999 loss: 0.9914 time 25.00s
Final training  617/4999 loss: 0.9920 time 25.34s
Final training  618/4999 loss: 0.9805 time 25.32s
Final training  619/4999 loss: 0.9756 time 25.28s
Final training  620/4999 loss: 1.0477 time 25.10s
Final training  621/4999 loss: 1.0119 time 24.87s
Final training  622/4999 loss: 0.9515 time 25.00s
Final training  623/4999 loss: 0.9802 time 24.69s
Final training  624/4999 loss: 0.9497 time 24.58s
Final training  625/4999 loss: 0.9864 time 24.89s
Final training  626/4999 loss: 0.9893 time 24.62s
Final training  627/4999 loss: 0.9734 time 24.68s
Final training  628/4999 loss: 0.9746 time 24.50s
Final training  629/4999 loss: 0.9796 time 24.69s
Final training  630/4999 loss: 1.0158 time 24.64s
Final training  631/4999 loss: 0.9835 time 24.77s
Final training  632/4999 loss: 0.9820 time 24.67s
Final training  633/4999 loss: 0.9719 time 24.59s
Final training  634/4999 loss: 0.9665 time 24.81s
Final training  635/4999 loss: 0.9784 time 25.08s
Final training  636/4999 loss: 1.0022 time 25.07s
Final training  637/4999 loss: 1.0082 time 24.84s
Final training  638/4999 loss: 0.9926 time 24.86s
Final training  639/4999 loss: 1.0237 time 25.04s
Final training  640/4999 loss: 1.0065 time 25.09s
Final training  641/4999 loss: 1.0156 time 24.72s
Final training  642/4999 loss: 0.9658 time 24.60s
Final training  643/4999 loss: 0.9936 time 24.84s
Final training  644/4999 loss: 1.0154 time 24.74s
Final training  645/4999 loss: 1.0212 time 24.86s
Final training  646/4999 loss: 0.9987 time 24.82s
Final training  647/4999 loss: 0.9603 time 24.56s
Final training  648/4999 loss: 0.9452 time 24.69s
Final training  649/4999 loss: 0.9491 time 24.74s
Final training  650/4999 loss: 0.9817 time 24.79s
Final training  651/4999 loss: 1.0005 time 24.76s
Final training  652/4999 loss: 0.9706 time 24.77s
Final training  653/4999 loss: 1.0035 time 24.72s
Final training  654/4999 loss: 0.9920 time 24.85s
Final training  655/4999 loss: 0.9872 time 24.86s
Final training  656/4999 loss: 0.9557 time 24.89s
Final training  657/4999 loss: 1.0113 time 24.99s
Final training  658/4999 loss: 0.9667 time 24.75s
Final training  659/4999 loss: 0.9866 time 24.79s
Final training  660/4999 loss: 0.9748 time 24.82s
Final training  661/4999 loss: 0.9582 time 24.52s
Final training  662/4999 loss: 0.9486 time 25.23s
Final training  663/4999 loss: 0.9845 time 24.94s
Final training  664/4999 loss: 0.9825 time 25.06s
Final training  665/4999 loss: 0.9917 time 25.00s
Final training  666/4999 loss: 0.9971 time 24.67s
Final training  667/4999 loss: 0.9964 time 24.77s
Final training  668/4999 loss: 0.9840 time 24.75s
Final training  669/4999 loss: 0.9555 time 24.87s
Final training  670/4999 loss: 0.9490 time 24.81s
Final training  671/4999 loss: 0.9641 time 24.79s
Final training  672/4999 loss: 0.9717 time 24.60s
Final training  673/4999 loss: 0.9586 time 24.92s
Final training  674/4999 loss: 0.9437 time 24.62s
Final training  675/4999 loss: 0.9488 time 24.68s
Final training  676/4999 loss: 0.9190 time 24.49s
Final training  677/4999 loss: 0.9558 time 24.63s
Final training  678/4999 loss: 0.9746 time 24.63s
Final training  679/4999 loss: 0.9743 time 24.74s
Final training  680/4999 loss: 0.9880 time 24.74s
Final training  681/4999 loss: 0.9987 time 24.89s
Final training  682/4999 loss: 0.9660 time 24.81s
Final training  683/4999 loss: 1.0262 time 24.65s
Final training  684/4999 loss: 1.0006 time 24.63s
Final training  685/4999 loss: 0.9762 time 24.69s
Final training  686/4999 loss: 0.9523 time 24.45s
Final training  687/4999 loss: 1.0067 time 24.72s
Final training  688/4999 loss: 0.9626 time 24.93s
Final training  689/4999 loss: 0.9866 time 24.61s
Final training  690/4999 loss: 0.9708 time 24.71s
Final training  691/4999 loss: 0.9660 time 24.64s
Final training  692/4999 loss: 0.9491 time 24.58s
Final training  693/4999 loss: 0.9756 time 24.63s
Final training  694/4999 loss: 0.9536 time 24.67s
Final training  695/4999 loss: 0.9907 time 24.85s
Final training  696/4999 loss: 0.9854 time 24.70s
Final training  697/4999 loss: 0.9746 time 24.68s
Final training  698/4999 loss: 0.9604 time 24.76s
Final training  699/4999 loss: 0.9648 time 24.64s
Dice accuracy for each class:  (tensor([0.9948, 0.9339, 0.8883, 0.9268, 0.6663, 0.0000, 0.8917, 0.7681, 0.8389,
        0.7791, 0.6927, 0.6019, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  699/4999 acc [ 0.641] time 181.88s
Reset trigger time to 0
new best (0.604962 --> 0.641011). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  700/4999 loss: 0.9543 time 24.62s
Final training  701/4999 loss: 1.0045 time 24.77s
Final training  702/4999 loss: 0.9734 time 24.39s
Final training  703/4999 loss: 0.9483 time 24.64s
Final training  704/4999 loss: 0.9411 time 24.69s
Final training  705/4999 loss: 0.9428 time 24.77s
Final training  706/4999 loss: 0.9956 time 24.74s
Final training  707/4999 loss: 1.0014 time 24.46s
Final training  708/4999 loss: 0.9562 time 24.54s
Final training  709/4999 loss: 0.9754 time 24.74s
Final training  710/4999 loss: 0.9714 time 24.87s
Final training  711/4999 loss: 0.9371 time 24.72s
Final training  712/4999 loss: 0.9647 time 25.06s
Final training  713/4999 loss: 1.0221 time 24.78s
Final training  714/4999 loss: 0.9350 time 25.14s
Final training  715/4999 loss: 0.9809 time 24.74s
Final training  716/4999 loss: 0.9507 time 24.69s
Final training  717/4999 loss: 0.9434 time 24.70s
Final training  718/4999 loss: 0.9644 time 24.73s
Final training  719/4999 loss: 0.9465 time 24.83s
Final training  720/4999 loss: 0.9666 time 24.77s
Final training  721/4999 loss: 0.9816 time 24.83s
Final training  722/4999 loss: 0.9867 time 24.89s
Final training  723/4999 loss: 0.9589 time 24.81s
Final training  724/4999 loss: 0.9619 time 24.65s
Final training  725/4999 loss: 0.9457 time 24.63s
Final training  726/4999 loss: 0.9389 time 24.41s
Final training  727/4999 loss: 0.9629 time 24.67s
Final training  728/4999 loss: 0.9816 time 24.90s
Final training  729/4999 loss: 1.0054 time 24.73s
Final training  730/4999 loss: 0.9637 time 24.56s
Final training  731/4999 loss: 0.9644 time 24.75s
Final training  732/4999 loss: 0.9683 time 24.58s
Final training  733/4999 loss: 0.9796 time 24.83s
Final training  734/4999 loss: 0.9619 time 24.66s
Final training  735/4999 loss: 0.9760 time 24.54s
Final training  736/4999 loss: 0.9507 time 24.63s
Final training  737/4999 loss: 0.9538 time 24.52s
Final training  738/4999 loss: 0.9433 time 24.66s
Final training  739/4999 loss: 0.9539 time 24.75s
Final training  740/4999 loss: 0.9370 time 24.82s
Final training  741/4999 loss: 0.9481 time 24.60s
Final training  742/4999 loss: 0.9795 time 24.81s
Final training  743/4999 loss: 0.9748 time 25.09s
Final training  744/4999 loss: 0.9572 time 24.95s
Final training  745/4999 loss: 0.9401 time 24.80s
Final training  746/4999 loss: 0.9454 time 24.93s
Final training  747/4999 loss: 0.9848 time 25.10s
Final training  748/4999 loss: 0.9493 time 25.06s
Final training  749/4999 loss: 0.9662 time 24.96s
Final training  750/4999 loss: 0.9549 time 24.94s
Final training  751/4999 loss: 0.9673 time 24.92s
Final training  752/4999 loss: 0.9405 time 24.67s
Final training  753/4999 loss: 0.9270 time 24.95s
Final training  754/4999 loss: 0.9545 time 25.12s
Final training  755/4999 loss: 0.9693 time 25.12s
Final training  756/4999 loss: 0.9586 time 24.79s
Final training  757/4999 loss: 0.9477 time 24.79s
Final training  758/4999 loss: 0.9360 time 24.88s
Final training  759/4999 loss: 0.9604 time 24.68s
Final training  760/4999 loss: 0.9278 time 24.80s
Final training  761/4999 loss: 0.9395 time 24.71s
Final training  762/4999 loss: 0.9556 time 24.88s
Final training  763/4999 loss: 0.9470 time 24.59s
Final training  764/4999 loss: 0.9422 time 24.46s
Final training  765/4999 loss: 0.9240 time 25.08s
Final training  766/4999 loss: 0.9324 time 24.72s
Final training  767/4999 loss: 0.9218 time 24.68s
Final training  768/4999 loss: 0.9451 time 24.84s
Final training  769/4999 loss: 0.9873 time 24.72s
Final training  770/4999 loss: 0.9410 time 24.94s
Final training  771/4999 loss: 0.9602 time 24.78s
Final training  772/4999 loss: 0.9385 time 24.61s
Final training  773/4999 loss: 0.9489 time 24.65s
Final training  774/4999 loss: 0.9670 time 24.52s
Final training  775/4999 loss: 0.9939 time 24.13s
Final training  776/4999 loss: 0.9551 time 24.35s
Final training  777/4999 loss: 1.0356 time 24.76s
Final training  778/4999 loss: 0.9572 time 25.23s
Final training  779/4999 loss: 0.9372 time 25.12s
Final training  780/4999 loss: 0.9187 time 25.16s
Final training  781/4999 loss: 0.9265 time 25.33s
Final training  782/4999 loss: 0.9607 time 25.03s
Final training  783/4999 loss: 0.9231 time 24.72s
Final training  784/4999 loss: 0.9234 time 24.57s
Final training  785/4999 loss: 0.9359 time 24.80s
Final training  786/4999 loss: 0.9508 time 25.06s
Final training  787/4999 loss: 0.9461 time 24.64s
Final training  788/4999 loss: 0.9533 time 24.58s
Final training  789/4999 loss: 0.9485 time 24.67s
Final training  790/4999 loss: 0.9429 time 24.72s
Final training  791/4999 loss: 0.9492 time 24.88s
Final training  792/4999 loss: 0.9583 time 24.77s
Final training  793/4999 loss: 0.9675 time 24.79s
Final training  794/4999 loss: 0.9290 time 24.74s
Final training  795/4999 loss: 0.9225 time 24.69s
Final training  796/4999 loss: 0.9506 time 24.72s
Final training  797/4999 loss: 0.9152 time 24.69s
Final training  798/4999 loss: 0.9287 time 24.62s
Final training  799/4999 loss: 0.9892 time 24.69s
Dice accuracy for each class:  (tensor([0.9927, 0.9041, 0.9231, 0.9127, 0.7125, 0.0017, 0.8231, 0.7535, 0.8590,
        0.7918, 0.7021, 0.6855, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  799/4999 acc [ 0.646] time 183.58s
Reset trigger time to 0
new best (0.641011 --> 0.646248). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  800/4999 loss: 0.9323 time 24.67s
Final training  801/4999 loss: 0.9304 time 24.63s
Final training  802/4999 loss: 0.9253 time 24.52s
Final training  803/4999 loss: 0.9099 time 24.55s
Final training  804/4999 loss: 0.8992 time 24.74s
Final training  805/4999 loss: 0.9127 time 24.47s
Final training  806/4999 loss: 0.9163 time 24.68s
Final training  807/4999 loss: 0.9399 time 24.75s
Final training  808/4999 loss: 0.9438 time 24.56s
Final training  809/4999 loss: 0.9572 time 25.13s
Final training  810/4999 loss: 0.9427 time 24.98s
Final training  811/4999 loss: 0.9417 time 25.23s
Final training  812/4999 loss: 0.9543 time 25.43s
Final training  813/4999 loss: 0.9238 time 24.89s
Final training  814/4999 loss: 0.9911 time 24.91s
Final training  815/4999 loss: 0.9201 time 25.07s
Final training  816/4999 loss: 0.9446 time 25.25s
Final training  817/4999 loss: 0.9601 time 24.87s
Final training  818/4999 loss: 0.9862 time 25.14s
Final training  819/4999 loss: 0.9632 time 25.10s
Final training  820/4999 loss: 0.9625 time 25.20s
Final training  821/4999 loss: 0.9214 time 24.85s
Final training  822/4999 loss: 0.9325 time 25.08s
Final training  823/4999 loss: 0.9655 time 24.62s
Final training  824/4999 loss: 0.9280 time 24.80s
Final training  825/4999 loss: 0.9323 time 25.04s
Final training  826/4999 loss: 0.9146 time 24.88s
Final training  827/4999 loss: 0.9665 time 24.58s
Final training  828/4999 loss: 0.9727 time 24.78s
Final training  829/4999 loss: 0.9535 time 24.59s
Final training  830/4999 loss: 0.9522 time 24.60s
Final training  831/4999 loss: 0.9301 time 24.62s
Final training  832/4999 loss: 0.9253 time 24.57s
Final training  833/4999 loss: 0.9406 time 24.64s
Final training  834/4999 loss: 0.9459 time 24.26s
Final training  835/4999 loss: 0.9088 time 24.52s
Final training  836/4999 loss: 0.9322 time 24.68s
Final training  837/4999 loss: 0.9237 time 24.84s
Final training  838/4999 loss: 0.9449 time 24.64s
Final training  839/4999 loss: 0.9377 time 24.55s
Final training  840/4999 loss: 0.9444 time 24.75s
Final training  841/4999 loss: 0.9085 time 24.59s
Final training  842/4999 loss: 0.9243 time 24.79s
Final training  843/4999 loss: 0.9391 time 24.74s
Final training  844/4999 loss: 0.9025 time 25.09s
Final training  845/4999 loss: 0.9191 time 25.05s
Final training  846/4999 loss: 0.9131 time 24.75s
Final training  847/4999 loss: 0.9260 time 24.45s
Final training  848/4999 loss: 0.9141 time 24.66s
Final training  849/4999 loss: 0.9338 time 24.28s
Final training  850/4999 loss: 0.9446 time 25.16s
Final training  851/4999 loss: 0.9146 time 24.80s
Final training  852/4999 loss: 0.9742 time 24.38s
Final training  853/4999 loss: 0.9214 time 24.42s
Final training  854/4999 loss: 0.9246 time 24.88s
Final training  855/4999 loss: 0.9134 time 24.62s
Final training  856/4999 loss: 0.9335 time 24.77s
Final training  857/4999 loss: 0.9244 time 24.72s
Final training  858/4999 loss: 0.9122 time 24.83s
Final training  859/4999 loss: 0.9508 time 24.76s
Final training  860/4999 loss: 0.9542 time 24.25s
Final training  861/4999 loss: 0.9489 time 24.49s
Final training  862/4999 loss: 0.9017 time 24.43s
Final training  863/4999 loss: 0.9317 time 24.72s
Final training  864/4999 loss: 0.9585 time 24.91s
Final training  865/4999 loss: 0.9329 time 24.91s
Final training  866/4999 loss: 0.9190 time 25.09s
Final training  867/4999 loss: 0.9414 time 24.87s
Final training  868/4999 loss: 0.9887 time 24.99s
Final training  869/4999 loss: 0.9144 time 25.01s
Final training  870/4999 loss: 0.9357 time 24.75s
Final training  871/4999 loss: 0.9305 time 24.81s
Final training  872/4999 loss: 0.9125 time 24.46s
Final training  873/4999 loss: 0.9326 time 24.67s
Final training  874/4999 loss: 0.9204 time 24.68s
Final training  875/4999 loss: 0.8975 time 24.56s
Final training  876/4999 loss: 0.9167 time 24.86s
Final training  877/4999 loss: 0.9149 time 24.70s
Final training  878/4999 loss: 0.9215 time 24.67s
Final training  879/4999 loss: 0.9059 time 24.39s
Final training  880/4999 loss: 0.9339 time 24.48s
Final training  881/4999 loss: 0.9349 time 24.66s
Final training  882/4999 loss: 0.9463 time 24.62s
Final training  883/4999 loss: 1.0185 time 24.89s
Final training  884/4999 loss: 0.9573 time 24.61s
Final training  885/4999 loss: 0.9123 time 24.68s
Final training  886/4999 loss: 0.9355 time 24.67s
Final training  887/4999 loss: 0.9315 time 24.59s
Final training  888/4999 loss: 0.9413 time 24.64s
Final training  889/4999 loss: 0.9864 time 24.58s
Final training  890/4999 loss: 0.9944 time 24.58s
Final training  891/4999 loss: 0.9876 time 24.71s
Final training  892/4999 loss: 0.9754 time 24.51s
Final training  893/4999 loss: 0.9482 time 25.06s
Final training  894/4999 loss: 0.9321 time 25.16s
Final training  895/4999 loss: 0.9182 time 24.71s
Final training  896/4999 loss: 0.9056 time 24.60s
Final training  897/4999 loss: 0.9305 time 24.38s
Final training  898/4999 loss: 0.9492 time 24.61s
Final training  899/4999 loss: 0.9972 time 24.47s
Dice accuracy for each class:  (tensor([0.9915, 0.8857, 0.9070, 0.9140, 0.6237, 0.4456, 0.8331, 0.5421, 0.8526,
        0.7596, 0.7244, 0.6902, 0.0000, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  899/4999 acc [ 0.655] time 181.96s
Reset trigger time to 0
new best (0.646248 --> 0.655016). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  900/4999 loss: 0.9459 time 24.81s
Final training  901/4999 loss: 0.9212 time 24.71s
Final training  902/4999 loss: 0.9278 time 24.61s
Final training  903/4999 loss: 0.9335 time 24.65s
Final training  904/4999 loss: 0.9579 time 24.83s
Final training  905/4999 loss: 0.9011 time 24.60s
Final training  906/4999 loss: 0.9197 time 24.24s
Final training  907/4999 loss: 0.9170 time 24.31s
Final training  908/4999 loss: 0.9286 time 24.48s
Final training  909/4999 loss: 0.8972 time 24.55s
Final training  910/4999 loss: 0.9068 time 24.54s
Final training  911/4999 loss: 0.9232 time 24.46s
Final training  912/4999 loss: 0.9585 time 24.64s
Final training  913/4999 loss: 0.9397 time 24.47s
Final training  914/4999 loss: 0.9291 time 24.75s
Final training  915/4999 loss: 0.9619 time 24.81s
Final training  916/4999 loss: 0.9080 time 24.63s
Final training  917/4999 loss: 0.9795 time 24.61s
Final training  918/4999 loss: 0.9462 time 24.53s
Final training  919/4999 loss: 0.9284 time 24.47s
Final training  920/4999 loss: 0.9031 time 24.39s
Final training  921/4999 loss: 0.9274 time 24.55s
Final training  922/4999 loss: 0.9284 time 24.39s
Final training  923/4999 loss: 0.9037 time 24.59s
Final training  924/4999 loss: 0.8992 time 24.52s
Final training  925/4999 loss: 0.9420 time 24.43s
Final training  926/4999 loss: 0.9226 time 24.66s
Final training  927/4999 loss: 0.9071 time 24.64s
Final training  928/4999 loss: 0.9443 time 24.38s
Final training  929/4999 loss: 0.9206 time 24.55s
Final training  930/4999 loss: 0.9366 time 24.63s
Final training  931/4999 loss: 0.9264 time 24.25s
Final training  932/4999 loss: 0.9168 time 24.54s
Final training  933/4999 loss: 0.9055 time 24.42s
Final training  934/4999 loss: 0.9350 time 24.56s
Final training  935/4999 loss: 0.9192 time 24.40s
Final training  936/4999 loss: 0.9108 time 24.51s
Final training  937/4999 loss: 0.9092 time 24.49s
Final training  938/4999 loss: 0.9116 time 24.92s
Final training  939/4999 loss: 0.8921 time 24.76s
Final training  940/4999 loss: 0.9190 time 24.62s
Final training  941/4999 loss: 0.9439 time 24.55s
Final training  942/4999 loss: 0.9196 time 24.77s
Final training  943/4999 loss: 0.9479 time 24.74s
Final training  944/4999 loss: 0.9020 time 24.60s
Final training  945/4999 loss: 0.9307 time 24.68s
Final training  946/4999 loss: 0.9439 time 24.45s
Final training  947/4999 loss: 0.9315 time 24.77s
Final training  948/4999 loss: 0.9071 time 24.88s
Final training  949/4999 loss: 0.9240 time 24.92s
Final training  950/4999 loss: 0.8861 time 23.83s
Final training  951/4999 loss: 0.9265 time 24.75s
Final training  952/4999 loss: 0.9358 time 24.71s
Final training  953/4999 loss: 0.9096 time 24.99s
Final training  954/4999 loss: 0.9238 time 24.84s
Final training  955/4999 loss: 0.9177 time 24.58s
Final training  956/4999 loss: 0.9166 time 24.71s
Final training  957/4999 loss: 0.9258 time 24.34s
Final training  958/4999 loss: 0.9088 time 24.51s
Final training  959/4999 loss: 0.9126 time 24.69s
Final training  960/4999 loss: 0.9168 time 24.74s
Final training  961/4999 loss: 0.9054 time 24.58s
Final training  962/4999 loss: 0.9044 time 24.39s
Final training  963/4999 loss: 0.9139 time 24.50s
Final training  964/4999 loss: 0.9530 time 24.32s
Final training  965/4999 loss: 0.9035 time 24.56s
Final training  966/4999 loss: 0.9185 time 24.70s
Final training  967/4999 loss: 0.9039 time 24.40s
Final training  968/4999 loss: 0.8854 time 25.00s
Final training  969/4999 loss: 0.9093 time 24.81s
Final training  970/4999 loss: 0.8947 time 24.86s
Final training  971/4999 loss: 0.9061 time 24.64s
Final training  972/4999 loss: 0.9167 time 24.54s
Final training  973/4999 loss: 0.8944 time 24.67s
Final training  974/4999 loss: 0.9072 time 24.51s
Final training  975/4999 loss: 0.9033 time 24.43s
Final training  976/4999 loss: 0.9132 time 24.30s
Final training  977/4999 loss: 0.8842 time 24.82s
Final training  978/4999 loss: 0.9260 time 24.95s
Final training  979/4999 loss: 0.9090 time 24.54s
Final training  980/4999 loss: 0.9073 time 24.36s
Final training  981/4999 loss: 0.9008 time 24.29s
Final training  982/4999 loss: 0.9118 time 24.53s
Final training  983/4999 loss: 0.9461 time 24.58s
Final training  984/4999 loss: 0.9293 time 24.89s
Final training  985/4999 loss: 0.9165 time 24.47s
Final training  986/4999 loss: 0.9029 time 24.70s
Final training  987/4999 loss: 0.9106 time 24.65s
Final training  988/4999 loss: 0.8998 time 24.79s
Final training  989/4999 loss: 0.8911 time 24.94s
Final training  990/4999 loss: 0.8972 time 24.95s
Final training  991/4999 loss: 0.9057 time 24.70s
Final training  992/4999 loss: 0.8933 time 24.85s
Final training  993/4999 loss: 0.9121 time 24.77s
Final training  994/4999 loss: 0.9082 time 24.64s
Final training  995/4999 loss: 0.8928 time 24.46s
Final training  996/4999 loss: 0.9118 time 24.61s
Final training  997/4999 loss: 0.8875 time 24.46s
Final training  998/4999 loss: 0.8953 time 24.48s
Final training  999/4999 loss: 0.8910 time 24.34s
Dice accuracy for each class:  (tensor([0.9950, 0.9454, 0.9319, 0.9373, 0.7228, 0.5954, 0.8963, 0.7297, 0.8685,
        0.8231, 0.7321, 0.7036, 0.0202, 0.0000], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  999/4999 acc [ 0.707] time 182.42s
Reset trigger time to 0
new best (0.655016 --> 0.706800). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  1000/4999 loss: 0.8934 time 24.92s
Final training  1001/4999 loss: 0.9091 time 25.00s
Final training  1002/4999 loss: 0.9224 time 24.50s
Final training  1003/4999 loss: 0.9146 time 24.39s
Final training  1004/4999 loss: 0.9240 time 24.59s
Final training  1005/4999 loss: 0.9021 time 24.50s
Final training  1006/4999 loss: 0.9053 time 24.54s
Final training  1007/4999 loss: 0.9233 time 24.87s
Final training  1008/4999 loss: 0.8948 time 24.93s
Final training  1009/4999 loss: 0.9044 time 24.83s
Final training  1010/4999 loss: 0.8864 time 24.90s
Final training  1011/4999 loss: 0.8890 time 25.13s
Final training  1012/4999 loss: 0.9064 time 25.22s
Final training  1013/4999 loss: 0.8935 time 24.49s
Final training  1014/4999 loss: 0.9028 time 25.27s
Final training  1015/4999 loss: 0.8924 time 25.04s
Final training  1016/4999 loss: 0.8831 time 24.55s
Final training  1017/4999 loss: 0.8912 time 24.54s
Final training  1018/4999 loss: 0.9217 time 24.64s
Final training  1019/4999 loss: 0.9083 time 24.74s
Final training  1020/4999 loss: 0.8916 time 24.64s
Final training  1021/4999 loss: 0.9123 time 24.71s
Final training  1022/4999 loss: 0.9164 time 24.75s
Final training  1023/4999 loss: 0.9274 time 24.56s
Final training  1024/4999 loss: 0.8898 time 24.50s
Final training  1025/4999 loss: 0.9248 time 24.59s
Final training  1026/4999 loss: 0.9013 time 24.50s
Final training  1027/4999 loss: 0.9421 time 24.89s
Final training  1028/4999 loss: 0.9292 time 24.62s
Final training  1029/4999 loss: 0.9302 time 25.02s
Final training  1030/4999 loss: 0.9853 time 24.60s
Final training  1031/4999 loss: 0.9685 time 24.70s
Final training  1032/4999 loss: 0.9376 time 24.63s
Final training  1033/4999 loss: 0.9385 time 24.71s
Final training  1034/4999 loss: 0.9127 time 24.37s
Final training  1035/4999 loss: 0.9250 time 24.54s
Final training  1036/4999 loss: 0.9230 time 24.53s
Final training  1037/4999 loss: 0.9047 time 24.71s
Final training  1038/4999 loss: 0.9269 time 24.60s
Final training  1039/4999 loss: 0.9032 time 24.53s
Final training  1040/4999 loss: 0.9064 time 24.54s
Final training  1041/4999 loss: 0.8719 time 24.65s
Final training  1042/4999 loss: 0.8961 time 24.74s
Final training  1043/4999 loss: 0.8791 time 24.30s
Final training  1044/4999 loss: 0.9045 time 24.63s
Final training  1045/4999 loss: 0.8946 time 24.60s
Final training  1046/4999 loss: 0.8921 time 24.72s
Final training  1047/4999 loss: 0.9065 time 24.75s
Final training  1048/4999 loss: 0.9068 time 24.87s
Final training  1049/4999 loss: 0.9204 time 24.31s
Final training  1050/4999 loss: 0.8771 time 24.80s
Final training  1051/4999 loss: 0.9079 time 24.84s
Final training  1052/4999 loss: 0.9138 time 24.75s
Final training  1053/4999 loss: 0.8934 time 25.05s
Final training  1054/4999 loss: 0.8983 time 25.00s
Final training  1055/4999 loss: 0.9073 time 24.67s
Final training  1056/4999 loss: 0.8910 time 24.75s
Final training  1057/4999 loss: 0.9081 time 24.66s
Final training  1058/4999 loss: 0.9010 time 24.56s
Final training  1059/4999 loss: 0.9045 time 24.78s
Final training  1060/4999 loss: 0.8965 time 25.23s
Final training  1061/4999 loss: 0.9173 time 24.94s
Final training  1062/4999 loss: 0.9111 time 24.51s
Final training  1063/4999 loss: 0.8799 time 24.42s
Final training  1064/4999 loss: 0.9345 time 24.59s
Final training  1065/4999 loss: 0.8774 time 24.71s
Final training  1066/4999 loss: 0.9122 time 24.67s
Final training  1067/4999 loss: 0.9705 time 24.64s
Final training  1068/4999 loss: 0.9221 time 24.40s
Final training  1069/4999 loss: 0.9143 time 24.47s
Final training  1070/4999 loss: 0.9341 time 24.53s
Final training  1071/4999 loss: 0.9038 time 24.39s
Final training  1072/4999 loss: 0.8923 time 24.53s
Final training  1073/4999 loss: 0.8780 time 24.54s
Final training  1074/4999 loss: 0.8739 time 24.65s
Final training  1075/4999 loss: 0.9038 time 24.74s
Final training  1076/4999 loss: 0.9395 time 24.69s
Final training  1077/4999 loss: 0.9244 time 24.78s
Final training  1078/4999 loss: 0.9022 time 24.95s
Final training  1079/4999 loss: 0.8972 time 24.67s
Final training  1080/4999 loss: 0.9072 time 24.78s
Final training  1081/4999 loss: 0.9174 time 24.49s
Final training  1082/4999 loss: 0.9043 time 24.50s
Final training  1083/4999 loss: 0.8944 time 24.70s
Final training  1084/4999 loss: 0.9735 time 24.56s
Final training  1085/4999 loss: 0.9172 time 24.63s
Final training  1086/4999 loss: 0.8948 time 24.65s
Final training  1087/4999 loss: 0.9039 time 24.61s
Final training  1088/4999 loss: 0.8842 time 24.28s
Final training  1089/4999 loss: 0.9117 time 24.43s
Final training  1090/4999 loss: 0.9190 time 24.52s
Final training  1091/4999 loss: 0.8870 time 24.68s
Final training  1092/4999 loss: 0.8891 time 24.91s
Final training  1093/4999 loss: 0.9364 time 24.54s
Final training  1094/4999 loss: 0.8915 time 24.89s
Final training  1095/4999 loss: 0.8870 time 24.53s
Final training  1096/4999 loss: 0.8845 time 24.85s
Final training  1097/4999 loss: 0.9047 time 24.83s
Final training  1098/4999 loss: 0.9121 time 24.86s
Final training  1099/4999 loss: 0.8838 time 24.71s
Dice accuracy for each class:  (tensor([9.9545e-01, 9.4207e-01, 8.5655e-01, 8.8602e-01, 7.3466e-01, 5.4798e-01,
        9.3301e-01, 6.4424e-01, 8.8449e-01, 8.1717e-01, 7.3407e-01, 6.7760e-01,
        3.0380e-01, 1.1969e-04], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1099/4999 acc [ 0.711] time 181.78s
Reset trigger time to 0
new best (0.706800 --> 0.710598). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  1100/4999 loss: 0.8945 time 24.83s
Final training  1101/4999 loss: 0.8914 time 25.03s
Final training  1102/4999 loss: 0.8927 time 24.65s
Final training  1103/4999 loss: 0.8784 time 24.24s
Final training  1104/4999 loss: 0.8856 time 24.74s
Final training  1105/4999 loss: 0.8882 time 24.56s
Final training  1106/4999 loss: 0.8904 time 24.71s
Final training  1107/4999 loss: 0.8972 time 24.50s
Final training  1108/4999 loss: 0.8909 time 24.83s
Final training  1109/4999 loss: 0.8690 time 24.74s
Final training  1110/4999 loss: 0.9240 time 24.67s
Final training  1111/4999 loss: 0.9225 time 24.81s
Final training  1112/4999 loss: 0.9162 time 25.05s
Final training  1113/4999 loss: 0.8876 time 24.78s
Final training  1114/4999 loss: 0.8897 time 24.54s
Final training  1115/4999 loss: 0.8836 time 24.50s
Final training  1116/4999 loss: 0.8819 time 24.46s
Final training  1117/4999 loss: 0.8823 time 24.88s
Final training  1118/4999 loss: 0.9058 time 24.56s
Final training  1119/4999 loss: 0.8894 time 24.51s
Final training  1120/4999 loss: 0.8712 time 24.64s
Final training  1121/4999 loss: 0.9099 time 24.58s
Final training  1122/4999 loss: 0.9269 time 24.57s
Final training  1123/4999 loss: 0.8745 time 24.71s
Final training  1124/4999 loss: 0.8953 time 24.73s
Final training  1125/4999 loss: 0.9023 time 24.51s
Final training  1126/4999 loss: 0.9329 time 24.20s
Final training  1127/4999 loss: 0.8965 time 24.74s
Final training  1128/4999 loss: 0.9120 time 24.64s
Final training  1129/4999 loss: 0.8852 time 24.51s
Final training  1130/4999 loss: 0.9218 time 24.79s
Final training  1131/4999 loss: 0.8945 time 24.58s
Final training  1132/4999 loss: 0.9004 time 24.71s
Final training  1133/4999 loss: 0.9089 time 24.61s
Final training  1134/4999 loss: 0.9010 time 24.50s
Final training  1135/4999 loss: 0.8778 time 24.54s
Final training  1136/4999 loss: 0.8893 time 24.45s
Final training  1137/4999 loss: 0.9116 time 25.05s
Final training  1138/4999 loss: 0.9032 time 25.05s
Final training  1139/4999 loss: 0.8998 time 25.17s
Final training  1140/4999 loss: 0.8910 time 25.30s
Final training  1141/4999 loss: 0.9243 time 25.00s
Final training  1142/4999 loss: 0.9344 time 25.06s
Final training  1143/4999 loss: 0.8853 time 25.14s
Final training  1144/4999 loss: 0.9158 time 24.87s
Final training  1145/4999 loss: 0.9238 time 24.86s
Final training  1146/4999 loss: 0.9161 time 25.19s
Final training  1147/4999 loss: 0.8978 time 25.05s
Final training  1148/4999 loss: 0.9302 time 24.95s
Final training  1149/4999 loss: 0.9067 time 25.13s
Final training  1150/4999 loss: 0.9016 time 25.10s
Final training  1151/4999 loss: 0.8800 time 25.21s
Final training  1152/4999 loss: 0.8749 time 24.91s
Final training  1153/4999 loss: 0.9317 time 25.77s
Final training  1154/4999 loss: 0.9290 time 25.42s
Final training  1155/4999 loss: 0.9356 time 24.87s
Final training  1156/4999 loss: 0.9089 time 24.68s
Final training  1157/4999 loss: 0.9138 time 25.03s
Final training  1158/4999 loss: 0.8861 time 24.97s
Final training  1159/4999 loss: 0.9203 time 24.85s
Final training  1160/4999 loss: 0.8874 time 24.65s
Final training  1161/4999 loss: 0.8988 time 24.70s
Final training  1162/4999 loss: 0.8880 time 24.60s
Final training  1163/4999 loss: 0.8769 time 24.65s
Final training  1164/4999 loss: 0.9145 time 24.97s
Final training  1165/4999 loss: 0.8879 time 24.64s
Final training  1166/4999 loss: 0.8617 time 24.59s
Final training  1167/4999 loss: 0.8943 time 24.38s
Final training  1168/4999 loss: 0.8878 time 24.42s
Final training  1169/4999 loss: 0.8944 time 24.58s
Final training  1170/4999 loss: 0.8593 time 24.75s
Final training  1171/4999 loss: 0.8742 time 24.70s
Final training  1172/4999 loss: 0.8749 time 24.75s
Final training  1173/4999 loss: 0.8873 time 24.76s
Final training  1174/4999 loss: 0.9011 time 25.17s
Final training  1175/4999 loss: 0.8803 time 25.13s
Final training  1176/4999 loss: 0.8866 time 24.80s
Final training  1177/4999 loss: 0.8929 time 24.79s
Final training  1178/4999 loss: 0.9060 time 25.04s
Final training  1179/4999 loss: 0.8850 time 25.30s
Final training  1180/4999 loss: 0.8766 time 25.29s
Final training  1181/4999 loss: 0.8916 time 24.99s
Final training  1182/4999 loss: 0.8715 time 24.99s
Final training  1183/4999 loss: 0.9075 time 24.86s
Final training  1184/4999 loss: 0.8951 time 24.71s
Final training  1185/4999 loss: 0.8944 time 24.84s
Final training  1186/4999 loss: 0.8984 time 25.11s
Final training  1187/4999 loss: 0.8812 time 24.80s
Final training  1188/4999 loss: 0.8770 time 24.79s
Final training  1189/4999 loss: 0.9084 time 25.14s
Final training  1190/4999 loss: 0.8924 time 24.82s
Final training  1191/4999 loss: 0.8564 time 24.79s
Final training  1192/4999 loss: 0.8771 time 24.62s
Final training  1193/4999 loss: 0.8894 time 24.74s
Final training  1194/4999 loss: 0.8690 time 24.89s
Final training  1195/4999 loss: 0.8610 time 24.77s
Final training  1196/4999 loss: 0.8678 time 24.66s
Final training  1197/4999 loss: 0.8590 time 24.70s
Final training  1198/4999 loss: 0.8542 time 24.85s
Final training  1199/4999 loss: 0.8455 time 24.57s
Dice accuracy for each class:  (tensor([0.9946, 0.9446, 0.9359, 0.9319, 0.7546, 0.6805, 0.8813, 0.6929, 0.8873,
        0.8230, 0.7355, 0.7024, 0.5647, 0.2928], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1199/4999 acc [ 0.773] time 183.57s
Reset trigger time to 0
new best (0.710598 --> 0.773125). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  1200/4999 loss: 0.8845 time 24.59s
Final training  1201/4999 loss: 0.8870 time 24.62s
Final training  1202/4999 loss: 0.9055 time 24.57s
Final training  1203/4999 loss: 0.9068 time 24.70s
Final training  1204/4999 loss: 0.8927 time 24.78s
Final training  1205/4999 loss: 0.8843 time 24.73s
Final training  1206/4999 loss: 0.8700 time 24.59s
Final training  1207/4999 loss: 0.8864 time 24.61s
Final training  1208/4999 loss: 0.8832 time 24.71s
Final training  1209/4999 loss: 0.8708 time 24.27s
Final training  1210/4999 loss: 0.9067 time 24.58s
Final training  1211/4999 loss: 0.9072 time 24.52s
Final training  1212/4999 loss: 0.9077 time 24.78s
Final training  1213/4999 loss: 0.8976 time 24.92s
Final training  1214/4999 loss: 0.8862 time 24.93s
Final training  1215/4999 loss: 0.8620 time 24.69s
Final training  1216/4999 loss: 0.8689 time 24.68s
Final training  1217/4999 loss: 0.8747 time 24.66s
Final training  1218/4999 loss: 0.8704 time 24.76s
Final training  1219/4999 loss: 0.8837 time 24.55s
Final training  1220/4999 loss: 0.8806 time 24.60s
Final training  1221/4999 loss: 0.8866 time 24.45s
Final training  1222/4999 loss: 0.8714 time 24.34s
Final training  1223/4999 loss: 0.8838 time 24.39s
Final training  1224/4999 loss: 0.9055 time 24.83s
Final training  1225/4999 loss: 0.9287 time 24.56s
Final training  1226/4999 loss: 0.9266 time 24.78s
Final training  1227/4999 loss: 0.8795 time 24.87s
Final training  1228/4999 loss: 0.8782 time 24.82s
Final training  1229/4999 loss: 0.8925 time 25.04s
Final training  1230/4999 loss: 0.9109 time 24.71s
Final training  1231/4999 loss: 0.8514 time 24.72s
Final training  1232/4999 loss: 0.8852 time 24.99s
Final training  1233/4999 loss: 0.8676 time 25.06s
Final training  1234/4999 loss: 0.8807 time 24.62s
Final training  1235/4999 loss: 0.8641 time 24.54s
Final training  1236/4999 loss: 0.8970 time 24.86s
Final training  1237/4999 loss: 0.8773 time 24.59s
Final training  1238/4999 loss: 0.8898 time 24.96s
Final training  1239/4999 loss: 0.8916 time 25.38s
Final training  1240/4999 loss: 0.9035 time 25.17s
Final training  1241/4999 loss: 0.9098 time 25.56s
Final training  1242/4999 loss: 0.9091 time 25.17s
Final training  1243/4999 loss: 0.8758 time 25.17s
Final training  1244/4999 loss: 0.9061 time 25.00s
Final training  1245/4999 loss: 0.8926 time 24.87s
Final training  1246/4999 loss: 0.8865 time 24.88s
Final training  1247/4999 loss: 0.8729 time 24.82s
Final training  1248/4999 loss: 0.8886 time 24.60s
Final training  1249/4999 loss: 0.8756 time 24.70s
Final training  1250/4999 loss: 0.8559 time 24.61s
Final training  1251/4999 loss: 0.8631 time 24.77s
Final training  1252/4999 loss: 0.8586 time 24.72s
Final training  1253/4999 loss: 0.8827 time 24.83s
Final training  1254/4999 loss: 0.8978 time 24.81s
Final training  1255/4999 loss: 0.8964 time 24.42s
Final training  1256/4999 loss: 0.9040 time 24.33s
Final training  1257/4999 loss: 0.9027 time 24.65s
Final training  1258/4999 loss: 0.9007 time 24.74s
Final training  1259/4999 loss: 0.9071 time 24.73s
Final training  1260/4999 loss: 0.8749 time 24.61s
Final training  1261/4999 loss: 0.8922 time 24.93s
Final training  1262/4999 loss: 0.8941 time 24.75s
Final training  1263/4999 loss: 0.9114 time 24.65s
Final training  1264/4999 loss: 0.8799 time 24.62s
Final training  1265/4999 loss: 0.8677 time 24.40s
Final training  1266/4999 loss: 0.8834 time 24.78s
Final training  1267/4999 loss: 0.8714 time 24.58s
Final training  1268/4999 loss: 0.8939 time 24.59s
Final training  1269/4999 loss: 0.8597 time 24.82s
Final training  1270/4999 loss: 0.8629 time 24.71s
Final training  1271/4999 loss: 0.9313 time 24.72s
Final training  1272/4999 loss: 0.9494 time 24.84s
Final training  1273/4999 loss: 0.8786 time 24.75s
Final training  1274/4999 loss: 0.8885 time 25.09s
Final training  1275/4999 loss: 0.8750 time 25.30s
Final training  1276/4999 loss: 0.8633 time 25.42s
Final training  1277/4999 loss: 0.9049 time 25.00s
Final training  1278/4999 loss: 0.8751 time 25.10s
Final training  1279/4999 loss: 0.8659 time 24.61s
Final training  1280/4999 loss: 0.8925 time 24.44s
Final training  1281/4999 loss: 0.8695 time 24.67s
Final training  1282/4999 loss: 0.8799 time 24.85s
Final training  1283/4999 loss: 0.8807 time 24.77s
Final training  1284/4999 loss: 0.8702 time 24.57s
Final training  1285/4999 loss: 0.8886 time 24.42s
Final training  1286/4999 loss: 0.8820 time 24.53s
Final training  1287/4999 loss: 0.8758 time 25.16s
Final training  1288/4999 loss: 0.8857 time 25.23s
Final training  1289/4999 loss: 0.8621 time 24.65s
Final training  1290/4999 loss: 0.8683 time 24.78s
Final training  1291/4999 loss: 0.8819 time 24.53s
Final training  1292/4999 loss: 0.8672 time 24.63s
Final training  1293/4999 loss: 0.8705 time 24.67s
Final training  1294/4999 loss: 0.9070 time 24.64s
Final training  1295/4999 loss: 0.8743 time 24.46s
Final training  1296/4999 loss: 0.8606 time 24.57s
Final training  1297/4999 loss: 0.8975 time 24.52s
Final training  1298/4999 loss: 0.8944 time 24.37s
Final training  1299/4999 loss: 0.8890 time 24.51s
Dice accuracy for each class:  (tensor([0.9938, 0.9269, 0.9125, 0.9117, 0.7368, 0.6489, 0.8496, 0.7261, 0.8682,
        0.8152, 0.7205, 0.6952, 0.5227, 0.4713], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1299/4999 acc [ 0.772] time 183.63s
trigger times: 1
Final training  1300/4999 loss: 0.8801 time 24.92s
Final training  1301/4999 loss: 0.8839 time 24.56s
Final training  1302/4999 loss: 0.8755 time 24.51s
Final training  1303/4999 loss: 0.8307 time 25.06s
Final training  1304/4999 loss: 0.9118 time 24.80s
Final training  1305/4999 loss: 0.8517 time 24.74s
Final training  1306/4999 loss: 0.8647 time 24.64s
Final training  1307/4999 loss: 0.8759 time 24.43s
Final training  1308/4999 loss: 0.8692 time 24.95s
Final training  1309/4999 loss: 0.8572 time 24.84s
Final training  1310/4999 loss: 0.8787 time 24.72s
Final training  1311/4999 loss: 0.8862 time 24.33s
Final training  1312/4999 loss: 0.8562 time 24.57s
Final training  1313/4999 loss: 0.8689 time 24.57s
Final training  1314/4999 loss: 0.8613 time 24.56s
Final training  1315/4999 loss: 0.8578 time 24.69s
Final training  1316/4999 loss: 0.8760 time 25.25s
Final training  1317/4999 loss: 0.8843 time 25.33s
Final training  1318/4999 loss: 0.8963 time 24.81s
Final training  1319/4999 loss: 0.8647 time 24.77s
Final training  1320/4999 loss: 0.8732 time 24.70s
Final training  1321/4999 loss: 0.8525 time 24.42s
Final training  1322/4999 loss: 0.8599 time 24.79s
Final training  1323/4999 loss: 0.8782 time 24.68s
Final training  1324/4999 loss: 0.8791 time 24.83s
Final training  1325/4999 loss: 0.8473 time 24.92s
Final training  1326/4999 loss: 0.8537 time 24.76s
Final training  1327/4999 loss: 0.8559 time 24.74s
Final training  1328/4999 loss: 0.8572 time 24.62s
Final training  1329/4999 loss: 0.8610 time 24.82s
Final training  1330/4999 loss: 0.8488 time 24.67s
Final training  1331/4999 loss: 0.8552 time 24.60s
Final training  1332/4999 loss: 0.8799 time 24.39s
Final training  1333/4999 loss: 0.9017 time 24.88s
Final training  1334/4999 loss: 0.9026 time 24.79s
Final training  1335/4999 loss: 0.8935 time 24.77s
Final training  1336/4999 loss: 0.8726 time 24.60s
Final training  1337/4999 loss: 0.8885 time 24.90s
Final training  1338/4999 loss: 0.8701 time 25.53s
Final training  1339/4999 loss: 0.8630 time 25.19s
Final training  1340/4999 loss: 0.8700 time 25.24s
Final training  1341/4999 loss: 0.8640 time 25.24s
Final training  1342/4999 loss: 0.8543 time 25.09s
Final training  1343/4999 loss: 0.8638 time 25.06s
Final training  1344/4999 loss: 0.8709 time 24.92s
Final training  1345/4999 loss: 0.8605 time 24.33s
Final training  1346/4999 loss: 0.8782 time 24.62s
Final training  1347/4999 loss: 0.9325 time 24.51s
Final training  1348/4999 loss: 1.0179 time 24.34s
Final training  1349/4999 loss: 0.9082 time 24.63s
Final training  1350/4999 loss: 0.8830 time 24.69s
Final training  1351/4999 loss: 0.8734 time 24.59s
Final training  1352/4999 loss: 0.8633 time 24.68s
Final training  1353/4999 loss: 0.9026 time 24.52s
Final training  1354/4999 loss: 0.8874 time 24.64s
Final training  1355/4999 loss: 0.8620 time 24.72s
Final training  1356/4999 loss: 0.8548 time 24.90s
Final training  1357/4999 loss: 0.8591 time 24.93s
Final training  1358/4999 loss: 0.8775 time 24.80s
Final training  1359/4999 loss: 0.8534 time 25.03s
Final training  1360/4999 loss: 0.8706 time 24.92s
Final training  1361/4999 loss: 0.8506 time 25.12s
Final training  1362/4999 loss: 0.8535 time 24.96s
Final training  1363/4999 loss: 0.8467 time 25.07s
Final training  1364/4999 loss: 0.8609 time 25.14s
Final training  1365/4999 loss: 0.8518 time 25.09s
Final training  1366/4999 loss: 0.8642 time 25.03s
Final training  1367/4999 loss: 0.8563 time 25.03s
Final training  1368/4999 loss: 0.8794 time 25.07s
Final training  1369/4999 loss: 0.8682 time 25.03s
Final training  1370/4999 loss: 0.8795 time 25.42s
Final training  1371/4999 loss: 0.8782 time 25.18s
Final training  1372/4999 loss: 0.9094 time 25.14s
Final training  1373/4999 loss: 0.8729 time 25.40s
Final training  1374/4999 loss: 0.9091 time 25.07s
Final training  1375/4999 loss: 0.8729 time 25.01s
Final training  1376/4999 loss: 0.8607 time 24.83s
Final training  1377/4999 loss: 0.8940 time 24.84s
Final training  1378/4999 loss: 0.8661 time 24.93s
Final training  1379/4999 loss: 0.8913 time 24.88s
Final training  1380/4999 loss: 0.8876 time 24.58s
Final training  1381/4999 loss: 0.8800 time 24.87s
Final training  1382/4999 loss: 0.8707 time 24.74s
Final training  1383/4999 loss: 0.8409 time 24.62s
Final training  1384/4999 loss: 0.8659 time 24.47s
Final training  1385/4999 loss: 0.8662 time 24.56s
Final training  1386/4999 loss: 0.8653 time 24.64s
Final training  1387/4999 loss: 0.8659 time 24.76s
Final training  1388/4999 loss: 0.8786 time 24.54s
Final training  1389/4999 loss: 0.8557 time 24.32s
Final training  1390/4999 loss: 0.8951 time 24.57s
Final training  1391/4999 loss: 0.8648 time 24.58s
Final training  1392/4999 loss: 0.8504 time 24.69s
Final training  1393/4999 loss: 0.8544 time 24.51s
Final training  1394/4999 loss: 0.8509 time 24.71s
Final training  1395/4999 loss: 0.8748 time 24.70s
Final training  1396/4999 loss: 0.8514 time 24.60s
Final training  1397/4999 loss: 0.8621 time 24.83s
Final training  1398/4999 loss: 0.8515 time 24.56s
Final training  1399/4999 loss: 0.8658 time 24.58s
Dice accuracy for each class:  (tensor([0.9936, 0.9357, 0.9319, 0.9383, 0.6608, 0.6500, 0.8426, 0.7866, 0.8561,
        0.8277, 0.7331, 0.7603, 0.5366, 0.5711], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1399/4999 acc [ 0.789] time 182.00s
Reset trigger time to 0
new best (0.773125 --> 0.788773). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  1400/4999 loss: 0.8565 time 24.69s
Final training  1401/4999 loss: 0.8555 time 25.12s
Final training  1402/4999 loss: 0.8570 time 24.95s
Final training  1403/4999 loss: 0.8726 time 24.48s
Final training  1404/4999 loss: 0.8824 time 24.70s
Final training  1405/4999 loss: 0.8753 time 24.54s
Final training  1406/4999 loss: 0.8658 time 24.71s
Final training  1407/4999 loss: 0.8642 time 24.64s
Final training  1408/4999 loss: 0.8320 time 24.56s
Final training  1409/4999 loss: 0.8728 time 24.44s
Final training  1410/4999 loss: 0.8565 time 24.73s
Final training  1411/4999 loss: 0.8731 time 24.69s
Final training  1412/4999 loss: 0.8429 time 24.89s
Final training  1413/4999 loss: 0.8316 time 25.11s
Final training  1414/4999 loss: 0.8400 time 25.05s
Final training  1415/4999 loss: 0.8468 time 25.00s
Final training  1416/4999 loss: 0.8632 time 24.87s
Final training  1417/4999 loss: 0.8595 time 24.78s
Final training  1418/4999 loss: 0.8938 time 24.54s
Final training  1419/4999 loss: 0.8755 time 24.57s
Final training  1420/4999 loss: 0.8484 time 24.67s
Final training  1421/4999 loss: 0.8541 time 24.61s
Final training  1422/4999 loss: 0.8632 time 24.70s
Final training  1423/4999 loss: 0.8404 time 24.79s
Final training  1424/4999 loss: 0.8677 time 24.79s
Final training  1425/4999 loss: 0.8464 time 24.80s
Final training  1426/4999 loss: 0.8624 time 24.83s
Final training  1427/4999 loss: 0.8806 time 24.71s
Final training  1428/4999 loss: 0.8505 time 24.80s
Final training  1429/4999 loss: 0.8631 time 25.06s
Final training  1430/4999 loss: 0.8623 time 24.94s
Final training  1431/4999 loss: 0.8489 time 24.61s
Final training  1432/4999 loss: 0.8622 time 24.64s
Final training  1433/4999 loss: 0.8528 time 24.72s
Final training  1434/4999 loss: 0.8696 time 24.59s
Final training  1435/4999 loss: 0.8427 time 24.55s
Final training  1436/4999 loss: 0.8459 time 24.57s
Final training  1437/4999 loss: 0.8602 time 24.63s
Final training  1438/4999 loss: 0.8507 time 24.69s
Final training  1439/4999 loss: 0.8486 time 24.59s
Final training  1440/4999 loss: 0.8398 time 24.53s
Final training  1441/4999 loss: 0.8472 time 24.79s
Final training  1442/4999 loss: 0.8636 time 24.76s
Final training  1443/4999 loss: 0.8621 time 24.59s
Final training  1444/4999 loss: 0.8660 time 24.68s
Final training  1445/4999 loss: 0.8552 time 24.93s
Final training  1446/4999 loss: 0.8634 time 24.30s
Final training  1447/4999 loss: 0.8864 time 24.52s
Final training  1448/4999 loss: 0.8964 time 24.34s
Final training  1449/4999 loss: 0.8878 time 24.64s
Final training  1450/4999 loss: 0.8742 time 24.73s
Final training  1451/4999 loss: 0.8740 time 24.56s
Final training  1452/4999 loss: 0.8669 time 25.08s
Final training  1453/4999 loss: 0.8657 time 24.78s
Final training  1454/4999 loss: 0.8759 time 25.41s
Final training  1455/4999 loss: 0.8879 time 24.97s
Final training  1456/4999 loss: 0.8787 time 24.71s
Final training  1457/4999 loss: 0.8810 time 24.54s
Final training  1458/4999 loss: 0.8820 time 24.42s
Final training  1459/4999 loss: 0.8811 time 24.56s
Final training  1460/4999 loss: 0.8365 time 24.65s
Final training  1461/4999 loss: 0.8509 time 24.54s
Final training  1462/4999 loss: 0.8445 time 24.39s
Final training  1463/4999 loss: 0.8426 time 24.62s
Final training  1464/4999 loss: 0.8504 time 24.46s
Final training  1465/4999 loss: 0.8479 time 24.93s
Final training  1466/4999 loss: 0.8564 time 24.61s
Final training  1467/4999 loss: 0.8633 time 24.91s
Final training  1468/4999 loss: 0.8735 time 24.73s
Final training  1469/4999 loss: 0.8591 time 24.68s
Final training  1470/4999 loss: 0.8535 time 24.72s
Final training  1471/4999 loss: 0.8375 time 24.57s
Final training  1472/4999 loss: 0.8509 time 24.66s
Final training  1473/4999 loss: 0.8363 time 24.52s
Final training  1474/4999 loss: 0.8738 time 24.68s
Final training  1475/4999 loss: 0.8763 time 24.72s
Final training  1476/4999 loss: 0.8599 time 24.65s
Final training  1477/4999 loss: 0.8572 time 24.46s
Final training  1478/4999 loss: 0.8651 time 24.73s
Final training  1479/4999 loss: 0.8768 time 24.73s
Final training  1480/4999 loss: 0.8659 time 24.62s
Final training  1481/4999 loss: 0.8603 time 24.69s
Final training  1482/4999 loss: 0.8484 time 24.76s
Final training  1483/4999 loss: 0.8534 time 24.82s
Final training  1484/4999 loss: 0.8535 time 24.34s
Final training  1485/4999 loss: 0.8894 time 24.56s
Final training  1486/4999 loss: 0.8576 time 24.70s
Final training  1487/4999 loss: 0.8804 time 24.64s
Final training  1488/4999 loss: 0.8464 time 24.37s
Final training  1489/4999 loss: 0.8797 time 24.69s
Final training  1490/4999 loss: 0.8952 time 24.78s
Final training  1491/4999 loss: 0.8956 time 24.94s
Final training  1492/4999 loss: 0.8794 time 24.76s
Final training  1493/4999 loss: 0.8668 time 24.58s
Final training  1494/4999 loss: 0.8724 time 24.59s
Final training  1495/4999 loss: 0.8507 time 24.63s
Final training  1496/4999 loss: 0.8371 time 24.93s
Final training  1497/4999 loss: 0.8548 time 24.90s
Final training  1498/4999 loss: 0.8439 time 24.60s
Final training  1499/4999 loss: 0.8719 time 24.78s
Dice accuracy for each class:  (tensor([0.9953, 0.9502, 0.9394, 0.9402, 0.7568, 0.6702, 0.8896, 0.7980, 0.9003,
        0.8267, 0.7283, 0.7646, 0.6092, 0.5857], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1499/4999 acc [ 0.812] time 183.61s
Reset trigger time to 0
new best (0.788773 --> 0.811592). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  1500/4999 loss: 0.8437 time 24.67s
Final training  1501/4999 loss: 0.8657 time 24.65s
Final training  1502/4999 loss: 0.8625 time 24.11s
Final training  1503/4999 loss: 0.8615 time 24.58s
Final training  1504/4999 loss: 0.8602 time 24.76s
Final training  1505/4999 loss: 0.8661 time 24.80s
Final training  1506/4999 loss: 0.8948 time 24.79s
Final training  1507/4999 loss: 0.8874 time 24.69s
Final training  1508/4999 loss: 0.9126 time 24.84s
Final training  1509/4999 loss: 0.8432 time 24.59s
Final training  1510/4999 loss: 0.8550 time 24.96s
Final training  1511/4999 loss: 0.8262 time 24.65s
Final training  1512/4999 loss: 0.8406 time 24.94s
Final training  1513/4999 loss: 0.8610 time 24.73s
Final training  1514/4999 loss: 0.8943 time 24.77s
Final training  1515/4999 loss: 0.8616 time 24.81s
Final training  1516/4999 loss: 0.8635 time 24.58s
Final training  1517/4999 loss: 0.8483 time 24.56s
Final training  1518/4999 loss: 0.8254 time 24.54s
Final training  1519/4999 loss: 0.8376 time 24.81s
Final training  1520/4999 loss: 0.8428 time 24.77s
Final training  1521/4999 loss: 0.8591 time 24.96s
Final training  1522/4999 loss: 0.8476 time 24.78s
Final training  1523/4999 loss: 0.8703 time 24.82s
Final training  1524/4999 loss: 0.8565 time 24.73s
Final training  1525/4999 loss: 0.8677 time 24.70s
Final training  1526/4999 loss: 0.8898 time 24.70s
Final training  1527/4999 loss: 0.8738 time 24.50s
Final training  1528/4999 loss: 0.8805 time 25.07s
Final training  1529/4999 loss: 0.8760 time 25.16s
Final training  1530/4999 loss: 0.8787 time 25.18s
Final training  1531/4999 loss: 0.8263 time 25.02s
Final training  1532/4999 loss: 0.8508 time 25.08s
Final training  1533/4999 loss: 0.8412 time 25.06s
Final training  1534/4999 loss: 0.8562 time 25.16s
Final training  1535/4999 loss: 0.8417 time 25.13s
Final training  1536/4999 loss: 0.8521 time 25.38s
Final training  1537/4999 loss: 0.8596 time 25.15s
Final training  1538/4999 loss: 0.8427 time 24.90s
Final training  1539/4999 loss: 0.8572 time 25.18s
Final training  1540/4999 loss: 0.8578 time 25.16s
Final training  1541/4999 loss: 0.8404 time 25.23s
Final training  1542/4999 loss: 0.8703 time 25.19s
Final training  1543/4999 loss: 0.8487 time 25.03s
Final training  1544/4999 loss: 0.8838 time 24.92s
Final training  1545/4999 loss: 0.8603 time 24.99s
Final training  1546/4999 loss: 0.8457 time 24.59s
Final training  1547/4999 loss: 0.8732 time 24.73s
Final training  1548/4999 loss: 0.8372 time 25.02s
Final training  1549/4999 loss: 0.8666 time 25.10s
Final training  1550/4999 loss: 0.8420 time 25.48s
Final training  1551/4999 loss: 0.8363 time 25.50s
Final training  1552/4999 loss: 0.8496 time 25.14s
Final training  1553/4999 loss: 0.8442 time 25.23s
Final training  1554/4999 loss: 0.8416 time 24.96s
Final training  1555/4999 loss: 0.8440 time 24.92s
Final training  1556/4999 loss: 0.8860 time 24.97s
Final training  1557/4999 loss: 0.8650 time 24.55s
Final training  1558/4999 loss: 0.8495 time 24.51s
Final training  1559/4999 loss: 0.8341 time 24.62s
Final training  1560/4999 loss: 0.8475 time 24.55s
Final training  1561/4999 loss: 0.8631 time 24.64s
Final training  1562/4999 loss: 0.8703 time 24.77s
Final training  1563/4999 loss: 0.8372 time 24.81s
Final training  1564/4999 loss: 0.8366 time 24.71s
Final training  1565/4999 loss: 0.8514 time 24.65s
Final training  1566/4999 loss: 0.8717 time 24.56s
Final training  1567/4999 loss: 0.8517 time 24.58s
Final training  1568/4999 loss: 0.8550 time 24.55s
Final training  1569/4999 loss: 0.8818 time 24.53s
Final training  1570/4999 loss: 0.8613 time 24.63s
Final training  1571/4999 loss: 0.8730 time 24.79s
Final training  1572/4999 loss: 0.8445 time 24.70s
Final training  1573/4999 loss: 0.8628 time 24.70s
Final training  1574/4999 loss: 0.8312 time 24.85s
Final training  1575/4999 loss: 0.8399 time 24.99s
Final training  1576/4999 loss: 0.8432 time 24.54s
Final training  1577/4999 loss: 0.8342 time 24.53s
Final training  1578/4999 loss: 0.8572 time 24.66s
Final training  1579/4999 loss: 0.8754 time 24.57s
Final training  1580/4999 loss: 0.9009 time 24.92s
Final training  1581/4999 loss: 0.8479 time 24.61s
Final training  1582/4999 loss: 0.8665 time 24.78s
Final training  1583/4999 loss: 0.8445 time 24.54s
Final training  1584/4999 loss: 0.9009 time 24.63s
Final training  1585/4999 loss: 0.8652 time 24.70s
Final training  1586/4999 loss: 0.8788 time 24.68s
Final training  1587/4999 loss: 0.8543 time 24.68s
Final training  1588/4999 loss: 0.8392 time 24.79s
Final training  1589/4999 loss: 0.8586 time 24.61s
Final training  1590/4999 loss: 0.8532 time 24.45s
Final training  1591/4999 loss: 0.8405 time 24.89s
Final training  1592/4999 loss: 0.8431 time 24.88s
Final training  1593/4999 loss: 0.8836 time 25.01s
Final training  1594/4999 loss: 0.9098 time 24.59s
Final training  1595/4999 loss: 0.8902 time 24.80s
Final training  1596/4999 loss: 0.8626 time 24.66s
Final training  1597/4999 loss: 0.8899 time 24.51s
Final training  1598/4999 loss: 0.8380 time 24.69s
Final training  1599/4999 loss: 0.8369 time 24.67s
Dice accuracy for each class:  (tensor([0.9953, 0.9299, 0.9341, 0.9196, 0.6990, 0.6520, 0.8908, 0.7835, 0.8754,
        0.8310, 0.7165, 0.7693, 0.5682, 0.5690], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1599/4999 acc [ 0.796] time 182.08s
trigger times: 1
Final training  1600/4999 loss: 0.8391 time 24.66s
Final training  1601/4999 loss: 0.8595 time 24.77s
Final training  1602/4999 loss: 0.8538 time 24.70s
Final training  1603/4999 loss: 0.8678 time 24.52s
Final training  1604/4999 loss: 0.8470 time 24.57s
Final training  1605/4999 loss: 0.8567 time 24.71s
Final training  1606/4999 loss: 0.8564 time 24.86s
Final training  1607/4999 loss: 0.8552 time 24.78s
Final training  1608/4999 loss: 0.8515 time 24.70s
Final training  1609/4999 loss: 0.8280 time 24.81s
Final training  1610/4999 loss: 0.8559 time 24.56s
Final training  1611/4999 loss: 0.8248 time 24.58s
Final training  1612/4999 loss: 0.8973 time 24.59s
Final training  1613/4999 loss: 0.8624 time 24.54s
Final training  1614/4999 loss: 0.8318 time 24.68s
Final training  1615/4999 loss: 0.8734 time 24.58s
Final training  1616/4999 loss: 0.8657 time 24.42s
Final training  1617/4999 loss: 0.8779 time 24.49s
Final training  1618/4999 loss: 0.8940 time 24.67s
Final training  1619/4999 loss: 0.8762 time 24.59s
Final training  1620/4999 loss: 0.8760 time 24.62s
Final training  1621/4999 loss: 0.8458 time 24.70s
Final training  1622/4999 loss: 0.8818 time 25.05s
Final training  1623/4999 loss: 0.8494 time 24.76s
Final training  1624/4999 loss: 0.8659 time 24.80s
Final training  1625/4999 loss: 0.8858 time 24.69s
Final training  1626/4999 loss: 0.8753 time 24.63s
Final training  1627/4999 loss: 0.8597 time 24.70s
Final training  1628/4999 loss: 0.8506 time 24.67s
Final training  1629/4999 loss: 0.8409 time 24.55s
Final training  1630/4999 loss: 0.8247 time 24.65s
Final training  1631/4999 loss: 0.8586 time 24.34s
Final training  1632/4999 loss: 0.8495 time 24.56s
Final training  1633/4999 loss: 0.8453 time 24.51s
Final training  1634/4999 loss: 0.8404 time 24.72s
Final training  1635/4999 loss: 0.8445 time 24.31s
Final training  1636/4999 loss: 0.8562 time 24.55s
Final training  1637/4999 loss: 0.8869 time 24.39s
Final training  1638/4999 loss: 0.8466 time 24.53s
Final training  1639/4999 loss: 0.8509 time 24.61s
Final training  1640/4999 loss: 0.8435 time 24.63s
Final training  1641/4999 loss: 0.8618 time 24.39s
Final training  1642/4999 loss: 0.8525 time 24.74s
Final training  1643/4999 loss: 0.8355 time 24.27s
Final training  1644/4999 loss: 0.8451 time 24.69s
Final training  1645/4999 loss: 0.8495 time 24.53s
Final training  1646/4999 loss: 0.8415 time 24.45s
Final training  1647/4999 loss: 0.8392 time 24.31s
Final training  1648/4999 loss: 0.8863 time 24.68s
Final training  1649/4999 loss: 0.8477 time 24.30s
Final training  1650/4999 loss: 0.8552 time 24.64s
Final training  1651/4999 loss: 0.8253 time 24.45s
Final training  1652/4999 loss: 0.8399 time 24.77s
Final training  1653/4999 loss: 0.8473 time 24.71s
Final training  1654/4999 loss: 0.8369 time 24.95s
Final training  1655/4999 loss: 0.8339 time 24.70s
Final training  1656/4999 loss: 0.8265 time 24.65s
Final training  1657/4999 loss: 0.8213 time 24.46s
Final training  1658/4999 loss: 0.8288 time 24.35s
Final training  1659/4999 loss: 0.8269 time 24.59s
Final training  1660/4999 loss: 0.8495 time 24.72s
Final training  1661/4999 loss: 0.8415 time 24.58s
Final training  1662/4999 loss: 0.8494 time 24.47s
Final training  1663/4999 loss: 0.8003 time 24.80s
Final training  1664/4999 loss: 0.8883 time 24.70s
Final training  1665/4999 loss: 0.8346 time 24.53s
Final training  1666/4999 loss: 0.8432 time 24.52s
Final training  1667/4999 loss: 0.8498 time 24.58s
Final training  1668/4999 loss: 0.8511 time 24.75s
Final training  1669/4999 loss: 0.8787 time 24.92s
Final training  1670/4999 loss: 0.8547 time 24.55s
Final training  1671/4999 loss: 0.8912 time 24.65s
Final training  1672/4999 loss: 0.8711 time 24.71s
Final training  1673/4999 loss: 0.9041 time 24.50s
Final training  1674/4999 loss: 0.8434 time 24.28s
Final training  1675/4999 loss: 0.8347 time 24.31s
Final training  1676/4999 loss: 0.8608 time 24.56s
Final training  1677/4999 loss: 0.8318 time 24.73s
Final training  1678/4999 loss: 0.8700 time 24.64s
Final training  1679/4999 loss: 0.8635 time 24.72s
Final training  1680/4999 loss: 0.8669 time 24.72s
Final training  1681/4999 loss: 0.8854 time 24.51s
Final training  1682/4999 loss: 0.8826 time 24.72s
Final training  1683/4999 loss: 0.8385 time 24.78s
Final training  1684/4999 loss: 0.8484 time 24.51s
Final training  1685/4999 loss: 0.8492 time 24.61s
Final training  1686/4999 loss: 0.8253 time 24.50s
Final training  1687/4999 loss: 0.8401 time 24.49s
Final training  1688/4999 loss: 0.8504 time 24.35s
Final training  1689/4999 loss: 0.8952 time 24.47s
Final training  1690/4999 loss: 0.8753 time 24.59s
Final training  1691/4999 loss: 0.8816 time 24.49s
Final training  1692/4999 loss: 0.8440 time 24.28s
Final training  1693/4999 loss: 0.8757 time 24.69s
Final training  1694/4999 loss: 0.8637 time 24.51s
Final training  1695/4999 loss: 0.8448 time 24.57s
Final training  1696/4999 loss: 0.8283 time 24.70s
Final training  1697/4999 loss: 0.8491 time 24.52s
Final training  1698/4999 loss: 0.8516 time 24.36s
Final training  1699/4999 loss: 0.8337 time 24.60s
Dice accuracy for each class:  (tensor([0.9945, 0.9504, 0.9362, 0.9337, 0.7285, 0.7091, 0.8690, 0.7678, 0.9042,
        0.8449, 0.7311, 0.7297, 0.5627, 0.5960], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1699/4999 acc [ 0.805] time 181.74s
trigger times: 2
Final training  1700/4999 loss: 0.8560 time 24.55s
Final training  1701/4999 loss: 0.8390 time 25.00s
Final training  1702/4999 loss: 0.8600 time 25.09s
Final training  1703/4999 loss: 0.8221 time 24.56s
Final training  1704/4999 loss: 0.8341 time 24.69s
Final training  1705/4999 loss: 0.8571 time 24.53s
Final training  1706/4999 loss: 0.8585 time 24.55s
Final training  1707/4999 loss: 0.8413 time 24.73s
Final training  1708/4999 loss: 0.8531 time 24.57s
Final training  1709/4999 loss: 0.8480 time 24.23s
Final training  1710/4999 loss: 0.8311 time 24.85s
Final training  1711/4999 loss: 0.8632 time 24.69s
Final training  1712/4999 loss: 0.8546 time 24.49s
Final training  1713/4999 loss: 0.8602 time 24.78s
Final training  1714/4999 loss: 0.8321 time 24.79s
Final training  1715/4999 loss: 0.8425 time 24.73s
Final training  1716/4999 loss: 0.8450 time 25.10s
Final training  1717/4999 loss: 0.8459 time 25.38s
Final training  1718/4999 loss: 0.8623 time 25.06s
Final training  1719/4999 loss: 0.8780 time 25.14s
Final training  1720/4999 loss: 0.8688 time 25.18s
Final training  1721/4999 loss: 0.8285 time 25.29s
Final training  1722/4999 loss: 0.8676 time 24.79s
Final training  1723/4999 loss: 0.8530 time 25.08s
Final training  1724/4999 loss: 0.8463 time 25.15s
Final training  1725/4999 loss: 0.8619 time 24.72s
Final training  1726/4999 loss: 0.8546 time 24.84s
Final training  1727/4999 loss: 0.8483 time 24.55s
Final training  1728/4999 loss: 0.8481 time 24.53s
Final training  1729/4999 loss: 0.8451 time 24.48s
Final training  1730/4999 loss: 0.8505 time 24.68s
Final training  1731/4999 loss: 0.8651 time 24.52s
Final training  1732/4999 loss: 0.8437 time 24.41s
Final training  1733/4999 loss: 0.8418 time 24.74s
Final training  1734/4999 loss: 0.8273 time 24.42s
Final training  1735/4999 loss: 0.8694 time 24.63s
Final training  1736/4999 loss: 0.8688 time 24.72s
Final training  1737/4999 loss: 0.8503 time 24.55s
Final training  1738/4999 loss: 0.8239 time 24.39s
Final training  1739/4999 loss: 0.8302 time 24.43s
Final training  1740/4999 loss: 0.8280 time 24.56s
Final training  1741/4999 loss: 0.8512 time 24.48s
Final training  1742/4999 loss: 0.8462 time 24.64s
Final training  1743/4999 loss: 0.8532 time 24.47s
Final training  1744/4999 loss: 0.8920 time 24.48s
Final training  1745/4999 loss: 0.8759 time 24.19s
Final training  1746/4999 loss: 0.8281 time 24.57s
Final training  1747/4999 loss: 0.8439 time 24.37s
Final training  1748/4999 loss: 0.8450 time 24.89s
Final training  1749/4999 loss: 0.8316 time 24.84s
Final training  1750/4999 loss: 0.8324 time 24.43s
Final training  1751/4999 loss: 0.8265 time 24.97s
Final training  1752/4999 loss: 0.8625 time 24.80s
Final training  1753/4999 loss: 0.8834 time 24.77s
Final training  1754/4999 loss: 0.8707 time 24.60s
Final training  1755/4999 loss: 0.8245 time 24.29s
Final training  1756/4999 loss: 0.8288 time 24.60s
Final training  1757/4999 loss: 0.8581 time 24.56s
Final training  1758/4999 loss: 0.8713 time 24.47s
Final training  1759/4999 loss: 0.8338 time 24.73s
Final training  1760/4999 loss: 0.8517 time 24.63s
Final training  1761/4999 loss: 0.8581 time 24.65s
Final training  1762/4999 loss: 0.8392 time 24.60s
Final training  1763/4999 loss: 0.9333 time 24.63s
Final training  1764/4999 loss: 0.8503 time 24.62s
Final training  1765/4999 loss: 0.8653 time 24.71s
Final training  1766/4999 loss: 0.8697 time 24.21s
Final training  1767/4999 loss: 0.8442 time 24.59s
Final training  1768/4999 loss: 0.8633 time 24.57s
Final training  1769/4999 loss: 0.8355 time 24.57s
Final training  1770/4999 loss: 0.8191 time 24.63s
Final training  1771/4999 loss: 0.8198 time 24.43s
Final training  1772/4999 loss: 0.8613 time 24.58s
Final training  1773/4999 loss: 0.8235 time 24.69s
Final training  1774/4999 loss: 0.8394 time 24.97s
Final training  1775/4999 loss: 0.8338 time 24.97s
Final training  1776/4999 loss: 0.8572 time 24.72s
Final training  1777/4999 loss: 0.8392 time 24.65s
Final training  1778/4999 loss: 0.8690 time 25.14s
Final training  1779/4999 loss: 0.8584 time 25.31s
Final training  1780/4999 loss: 0.8807 time 24.96s
Final training  1781/4999 loss: 0.8574 time 24.88s
Final training  1782/4999 loss: 0.8252 time 24.93s
Final training  1783/4999 loss: 0.8672 time 24.62s
Final training  1784/4999 loss: 0.8430 time 24.57s
Final training  1785/4999 loss: 0.8391 time 24.70s
Final training  1786/4999 loss: 0.8524 time 24.70s
Final training  1787/4999 loss: 0.8523 time 24.61s
Final training  1788/4999 loss: 0.8282 time 24.47s
Final training  1789/4999 loss: 0.8312 time 24.22s
Final training  1790/4999 loss: 0.8367 time 24.56s
Final training  1791/4999 loss: 0.8345 time 24.24s
Final training  1792/4999 loss: 0.8404 time 24.51s
Final training  1793/4999 loss: 0.8397 time 24.86s
Final training  1794/4999 loss: 0.8135 time 24.79s
Final training  1795/4999 loss: 0.8271 time 24.44s
Final training  1796/4999 loss: 0.8395 time 24.62s
Final training  1797/4999 loss: 0.8524 time 24.49s
Final training  1798/4999 loss: 0.8540 time 24.63s
Final training  1799/4999 loss: 0.8579 time 24.67s
Dice accuracy for each class:  (tensor([0.9942, 0.9380, 0.9334, 0.9393, 0.7100, 0.7128, 0.8693, 0.7241, 0.8836,
        0.8389, 0.7125, 0.7462, 0.5785, 0.5352], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1799/4999 acc [ 0.795] time 183.13s
trigger times: 3
Final training  1800/4999 loss: 0.8578 time 24.83s
Final training  1801/4999 loss: 0.8629 time 24.68s
Final training  1802/4999 loss: 0.8655 time 24.69s
Final training  1803/4999 loss: 0.8270 time 24.65s
Final training  1804/4999 loss: 0.8435 time 24.71s
Final training  1805/4999 loss: 0.8182 time 24.41s
Final training  1806/4999 loss: 0.8458 time 24.59s
Final training  1807/4999 loss: 0.8254 time 24.60s
Final training  1808/4999 loss: 0.8515 time 24.72s
Final training  1809/4999 loss: 0.8242 time 24.74s
Final training  1810/4999 loss: 0.8146 time 24.89s
Final training  1811/4999 loss: 0.8301 time 25.25s
Final training  1812/4999 loss: 0.8672 time 25.03s
Final training  1813/4999 loss: 0.8104 time 25.10s
Final training  1814/4999 loss: 0.8641 time 24.94s
Final training  1815/4999 loss: 0.8865 time 24.73s
Final training  1816/4999 loss: 0.8360 time 24.74s
Final training  1817/4999 loss: 0.8407 time 24.54s
Final training  1818/4999 loss: 0.8467 time 24.64s
Final training  1819/4999 loss: 0.8352 time 24.78s
Final training  1820/4999 loss: 0.8798 time 24.82s
Final training  1821/4999 loss: 0.8401 time 24.68s
Final training  1822/4999 loss: 0.8624 time 24.73s
Final training  1823/4999 loss: 0.8752 time 24.54s
Final training  1824/4999 loss: 0.8458 time 24.76s
Final training  1825/4999 loss: 0.8367 time 24.89s
Final training  1826/4999 loss: 0.8322 time 24.37s
Final training  1827/4999 loss: 0.8490 time 24.50s
Final training  1828/4999 loss: 0.8435 time 24.48s
Final training  1829/4999 loss: 0.8612 time 24.35s
Final training  1830/4999 loss: 0.8219 time 24.61s
Final training  1831/4999 loss: 0.8466 time 24.12s
Final training  1832/4999 loss: 0.8380 time 24.76s
Final training  1833/4999 loss: 0.8442 time 24.44s
Final training  1834/4999 loss: 0.8295 time 24.80s
Final training  1835/4999 loss: 0.8464 time 25.41s
Final training  1836/4999 loss: 0.8291 time 25.42s
Final training  1837/4999 loss: 0.8729 time 25.32s
Final training  1838/4999 loss: 0.8440 time 25.26s
Final training  1839/4999 loss: 0.9073 time 25.15s
Final training  1840/4999 loss: 0.9248 time 25.49s
Final training  1841/4999 loss: 0.8603 time 24.65s
Final training  1842/4999 loss: 0.8321 time 24.61s
Final training  1843/4999 loss: 0.8558 time 24.60s
Final training  1844/4999 loss: 0.8458 time 24.68s
Final training  1845/4999 loss: 0.8475 time 24.49s
Final training  1846/4999 loss: 0.8454 time 24.59s
Final training  1847/4999 loss: 0.8827 time 24.40s
Final training  1848/4999 loss: 0.8215 time 24.63s
Final training  1849/4999 loss: 0.8466 time 24.50s
Final training  1850/4999 loss: 0.8413 time 24.52s
Final training  1851/4999 loss: 0.8524 time 24.59s
Final training  1852/4999 loss: 0.8326 time 24.62s
Final training  1853/4999 loss: 0.8440 time 24.81s
Final training  1854/4999 loss: 0.8518 time 25.06s
Final training  1855/4999 loss: 0.8551 time 25.14s
Final training  1856/4999 loss: 0.8470 time 25.20s
Final training  1857/4999 loss: 0.8405 time 24.90s
Final training  1858/4999 loss: 0.8424 time 24.97s
Final training  1859/4999 loss: 0.8525 time 24.75s
Final training  1860/4999 loss: 0.8192 time 24.70s
Final training  1861/4999 loss: 0.8238 time 24.42s
Final training  1862/4999 loss: 0.8625 time 24.71s
Final training  1863/4999 loss: 0.8382 time 24.54s
Final training  1864/4999 loss: 0.8298 time 24.67s
Final training  1865/4999 loss: 0.8276 time 24.59s
Final training  1866/4999 loss: 0.8529 time 24.83s
Final training  1867/4999 loss: 0.8347 time 25.00s
Final training  1868/4999 loss: 0.8351 time 24.61s
Final training  1869/4999 loss: 0.8332 time 24.63s
Final training  1870/4999 loss: 0.8483 time 24.61s
Final training  1871/4999 loss: 0.8279 time 24.72s
Final training  1872/4999 loss: 0.7999 time 24.64s
Final training  1873/4999 loss: 0.8353 time 24.89s
Final training  1874/4999 loss: 0.8246 time 24.93s
Final training  1875/4999 loss: 0.8293 time 24.53s
Final training  1876/4999 loss: 0.8731 time 24.56s
Final training  1877/4999 loss: 0.8582 time 24.54s
Final training  1878/4999 loss: 0.8386 time 24.83s
Final training  1879/4999 loss: 0.8300 time 24.54s
Final training  1880/4999 loss: 0.8262 time 24.51s
Final training  1881/4999 loss: 0.8676 time 24.66s
Final training  1882/4999 loss: 0.8842 time 24.32s
Final training  1883/4999 loss: 0.9024 time 24.52s
Final training  1884/4999 loss: 0.8581 time 24.45s
Final training  1885/4999 loss: 0.8838 time 24.58s
Final training  1886/4999 loss: 0.8481 time 24.23s
Final training  1887/4999 loss: 0.8578 time 24.51s
Final training  1888/4999 loss: 0.8413 time 24.69s
Final training  1889/4999 loss: 0.8421 time 24.55s
Final training  1890/4999 loss: 0.8353 time 24.57s
Final training  1891/4999 loss: 0.8309 time 24.63s
Final training  1892/4999 loss: 0.8658 time 24.57s
Final training  1893/4999 loss: 0.8478 time 24.59s
Final training  1894/4999 loss: 0.8548 time 24.64s
Final training  1895/4999 loss: 0.8734 time 24.62s
Final training  1896/4999 loss: 0.8876 time 24.55s
Final training  1897/4999 loss: 0.9009 time 24.62s
Final training  1898/4999 loss: 0.8343 time 24.92s
Final training  1899/4999 loss: 0.8372 time 24.57s
Dice accuracy for each class:  (tensor([0.9958, 0.9402, 0.9401, 0.9383, 0.7393, 0.7095, 0.9119, 0.7923, 0.8985,
        0.8429, 0.7516, 0.7306, 0.6253, 0.6167], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1899/4999 acc [ 0.817] time 183.60s
Reset trigger time to 0
new best (0.811592 --> 0.817409). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  1900/4999 loss: 0.8349 time 25.16s
Final training  1901/4999 loss: 0.8254 time 24.92s
Final training  1902/4999 loss: 0.8292 time 24.71s
Final training  1903/4999 loss: 0.8510 time 24.54s
Final training  1904/4999 loss: 0.8327 time 24.91s
Final training  1905/4999 loss: 0.8349 time 24.60s
Final training  1906/4999 loss: 0.8156 time 24.48s
Final training  1907/4999 loss: 0.8440 time 24.71s
Final training  1908/4999 loss: 0.8391 time 24.73s
Final training  1909/4999 loss: 0.8368 time 24.76s
Final training  1910/4999 loss: 0.8326 time 24.87s
Final training  1911/4999 loss: 0.8296 time 24.75s
Final training  1912/4999 loss: 0.8102 time 24.62s
Final training  1913/4999 loss: 0.8269 time 24.81s
Final training  1914/4999 loss: 0.8016 time 24.88s
Final training  1915/4999 loss: 0.8428 time 25.05s
Final training  1916/4999 loss: 0.8108 time 24.80s
Final training  1917/4999 loss: 0.8073 time 24.83s
Final training  1918/4999 loss: 0.8267 time 24.90s
Final training  1919/4999 loss: 0.8003 time 24.83s
Final training  1920/4999 loss: 0.8098 time 24.87s
Final training  1921/4999 loss: 0.8181 time 25.11s
Final training  1922/4999 loss: 0.8300 time 25.00s
Final training  1923/4999 loss: 0.8319 time 25.04s
Final training  1924/4999 loss: 0.8289 time 24.76s
Final training  1925/4999 loss: 0.8210 time 24.52s
Final training  1926/4999 loss: 0.8198 time 24.71s
Final training  1927/4999 loss: 0.8384 time 24.69s
Final training  1928/4999 loss: 0.8407 time 24.61s
Final training  1929/4999 loss: 0.8253 time 24.76s
Final training  1930/4999 loss: 0.8400 time 24.71s
Final training  1931/4999 loss: 0.8373 time 24.81s
Final training  1932/4999 loss: 0.8326 time 24.78s
Final training  1933/4999 loss: 0.8358 time 24.36s
Final training  1934/4999 loss: 0.8130 time 24.64s
Final training  1935/4999 loss: 0.8247 time 24.50s
Final training  1936/4999 loss: 0.8140 time 24.52s
Final training  1937/4999 loss: 0.8392 time 24.73s
Final training  1938/4999 loss: 0.8313 time 24.60s
Final training  1939/4999 loss: 0.8416 time 24.62s
Final training  1940/4999 loss: 0.8349 time 24.55s
Final training  1941/4999 loss: 0.8632 time 24.57s
Final training  1942/4999 loss: 0.8377 time 24.70s
Final training  1943/4999 loss: 0.8740 time 24.72s
Final training  1944/4999 loss: 0.8752 time 24.74s
Final training  1945/4999 loss: 0.8907 time 24.64s
Final training  1946/4999 loss: 0.8647 time 24.99s
Final training  1947/4999 loss: 0.8601 time 24.99s
Final training  1948/4999 loss: 0.8401 time 24.84s
Final training  1949/4999 loss: 0.8491 time 24.53s
Final training  1950/4999 loss: 0.8589 time 24.00s
Final training  1951/4999 loss: 0.8350 time 24.39s
Final training  1952/4999 loss: 0.8167 time 24.61s
Final training  1953/4999 loss: 0.8316 time 24.40s
Final training  1954/4999 loss: 0.8373 time 24.70s
Final training  1955/4999 loss: 0.8267 time 24.92s
Final training  1956/4999 loss: 0.8488 time 24.42s
Final training  1957/4999 loss: 0.8196 time 24.62s
Final training  1958/4999 loss: 0.8457 time 24.54s
Final training  1959/4999 loss: 0.8600 time 24.57s
Final training  1960/4999 loss: 0.8563 time 24.15s
Final training  1961/4999 loss: 0.8309 time 24.37s
Final training  1962/4999 loss: 0.8319 time 24.83s
Final training  1963/4999 loss: 0.8238 time 24.36s
Final training  1964/4999 loss: 0.8364 time 24.47s
Final training  1965/4999 loss: 0.8629 time 24.66s
Final training  1966/4999 loss: 0.8753 time 24.72s
Final training  1967/4999 loss: 0.8400 time 24.55s
Final training  1968/4999 loss: 0.8299 time 24.71s
Final training  1969/4999 loss: 0.8707 time 24.73s
Final training  1970/4999 loss: 0.8611 time 24.43s
Final training  1971/4999 loss: 0.8604 time 24.51s
Final training  1972/4999 loss: 0.8373 time 24.57s
Final training  1973/4999 loss: 0.8506 time 24.23s
Final training  1974/4999 loss: 0.8568 time 24.54s
Final training  1975/4999 loss: 0.8622 time 24.57s
Final training  1976/4999 loss: 0.8534 time 24.48s
Final training  1977/4999 loss: 0.8790 time 24.45s
Final training  1978/4999 loss: 0.8439 time 24.71s
Final training  1979/4999 loss: 0.8529 time 24.95s
Final training  1980/4999 loss: 0.8478 time 24.77s
Final training  1981/4999 loss: 0.8139 time 24.75s
Final training  1982/4999 loss: 0.8376 time 24.60s
Final training  1983/4999 loss: 0.8306 time 24.91s
Final training  1984/4999 loss: 0.8552 time 24.94s
Final training  1985/4999 loss: 0.8288 time 24.88s
Final training  1986/4999 loss: 0.8248 time 24.88s
Final training  1987/4999 loss: 0.8331 time 24.65s
Final training  1988/4999 loss: 0.8275 time 24.99s
Final training  1989/4999 loss: 0.8188 time 25.04s
Final training  1990/4999 loss: 0.8273 time 24.95s
Final training  1991/4999 loss: 0.8139 time 25.16s
Final training  1992/4999 loss: 0.8066 time 25.06s
Final training  1993/4999 loss: 0.7964 time 25.08s
Final training  1994/4999 loss: 0.8302 time 24.81s
Final training  1995/4999 loss: 0.8510 time 24.76s
Final training  1996/4999 loss: 0.8024 time 24.52s
Final training  1997/4999 loss: 0.8430 time 24.61s
Final training  1998/4999 loss: 0.8256 time 24.59s
Final training  1999/4999 loss: 0.8323 time 24.71s
Dice accuracy for each class:  (tensor([0.9954, 0.9481, 0.9366, 0.9408, 0.7739, 0.6805, 0.8963, 0.7550, 0.8977,
        0.8365, 0.7379, 0.7532, 0.6462, 0.5960], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  1999/4999 acc [ 0.814] time 181.70s
trigger times: 1
Final training  2000/4999 loss: 0.8114 time 24.53s
Final training  2001/4999 loss: 0.8370 time 24.62s
Final training  2002/4999 loss: 0.8584 time 24.66s
Final training  2003/4999 loss: 0.8295 time 24.59s
Final training  2004/4999 loss: 0.8221 time 24.58s
Final training  2005/4999 loss: 0.8373 time 24.62s
Final training  2006/4999 loss: 0.8086 time 24.67s
Final training  2007/4999 loss: 0.8199 time 24.50s
Final training  2008/4999 loss: 0.8286 time 24.60s
Final training  2009/4999 loss: 0.8117 time 24.52s
Final training  2010/4999 loss: 0.8137 time 24.65s
Final training  2011/4999 loss: 0.8374 time 24.53s
Final training  2012/4999 loss: 0.8230 time 24.58s
Final training  2013/4999 loss: 0.8065 time 24.53s
Final training  2014/4999 loss: 0.8392 time 24.50s
Final training  2015/4999 loss: 0.8130 time 24.23s
Final training  2016/4999 loss: 0.8119 time 24.94s
Final training  2017/4999 loss: 0.8201 time 25.13s
Final training  2018/4999 loss: 0.8351 time 24.72s
Final training  2019/4999 loss: 0.8232 time 24.53s
Final training  2020/4999 loss: 0.8363 time 24.18s
Final training  2021/4999 loss: 0.8075 time 24.67s
Final training  2022/4999 loss: 0.8363 time 24.41s
Final training  2023/4999 loss: 0.8496 time 24.40s
Final training  2024/4999 loss: 0.8419 time 24.84s
Final training  2025/4999 loss: 0.8089 time 24.13s
Final training  2026/4999 loss: 0.8256 time 24.75s
Final training  2027/4999 loss: 0.8410 time 24.64s
Final training  2028/4999 loss: 0.8293 time 24.75s
Final training  2029/4999 loss: 0.8290 time 24.70s
Final training  2030/4999 loss: 0.8616 time 24.63s
Final training  2031/4999 loss: 0.8082 time 24.53s
Final training  2032/4999 loss: 0.7974 time 24.59s
Final training  2033/4999 loss: 0.8122 time 24.76s
Final training  2034/4999 loss: 0.8050 time 24.46s
Final training  2035/4999 loss: 0.8354 time 24.51s
Final training  2036/4999 loss: 0.8137 time 24.51s
Final training  2037/4999 loss: 0.8509 time 24.71s
Final training  2038/4999 loss: 0.8329 time 24.66s
Final training  2039/4999 loss: 0.8025 time 24.55s
Final training  2040/4999 loss: 0.8211 time 24.60s
Final training  2041/4999 loss: 0.8162 time 24.58s
Final training  2042/4999 loss: 0.8168 time 24.61s
Final training  2043/4999 loss: 0.8121 time 24.53s
Final training  2044/4999 loss: 0.8353 time 24.70s
Final training  2045/4999 loss: 0.8092 time 24.61s
Final training  2046/4999 loss: 0.8197 time 24.81s
Final training  2047/4999 loss: 0.8029 time 24.73s
Final training  2048/4999 loss: 0.8422 time 24.51s
Final training  2049/4999 loss: 0.8190 time 24.44s
Final training  2050/4999 loss: 0.8358 time 24.50s
Final training  2051/4999 loss: 0.8371 time 24.70s
Final training  2052/4999 loss: 0.8361 time 24.59s
Final training  2053/4999 loss: 0.8406 time 24.89s
Final training  2054/4999 loss: 0.8173 time 24.86s
Final training  2055/4999 loss: 0.8066 time 24.76s
Final training  2056/4999 loss: 0.8234 time 24.81s
Final training  2057/4999 loss: 0.8441 time 24.59s
Final training  2058/4999 loss: 0.8229 time 24.33s
Final training  2059/4999 loss: 0.8354 time 24.48s
Final training  2060/4999 loss: 0.8394 time 24.28s
Final training  2061/4999 loss: 0.8218 time 24.58s
Final training  2062/4999 loss: 0.8109 time 24.66s
Final training  2063/4999 loss: 0.8260 time 24.57s
Final training  2064/4999 loss: 0.8110 time 24.46s
Final training  2065/4999 loss: 0.8258 time 24.67s
Final training  2066/4999 loss: 0.8084 time 24.70s
Final training  2067/4999 loss: 0.8791 time 24.61s
Final training  2068/4999 loss: 0.8912 time 24.47s
Final training  2069/4999 loss: 0.9996 time 24.65s
Final training  2070/4999 loss: 0.8930 time 24.72s
Final training  2071/4999 loss: 0.8669 time 24.64s
Final training  2072/4999 loss: 0.8395 time 24.62s
Final training  2073/4999 loss: 0.8186 time 24.50s
Final training  2074/4999 loss: 0.8266 time 24.55s
Final training  2075/4999 loss: 0.8557 time 24.78s
Final training  2076/4999 loss: 0.8499 time 24.86s
Final training  2077/4999 loss: 0.8598 time 25.17s
Final training  2078/4999 loss: 0.8579 time 24.83s
Final training  2079/4999 loss: 0.8907 time 24.66s
Final training  2080/4999 loss: 0.8434 time 24.80s
Final training  2081/4999 loss: 0.8486 time 24.57s
Final training  2082/4999 loss: 0.8142 time 24.70s
Final training  2083/4999 loss: 0.8216 time 24.87s
Final training  2084/4999 loss: 0.8371 time 24.70s
Final training  2085/4999 loss: 0.8478 time 24.42s
Final training  2086/4999 loss: 0.8240 time 24.70s
Final training  2087/4999 loss: 0.8150 time 24.58s
Final training  2088/4999 loss: 0.8213 time 24.84s
Final training  2089/4999 loss: 0.8235 time 24.58s
Final training  2090/4999 loss: 0.8096 time 24.77s
Final training  2091/4999 loss: 0.8581 time 24.52s
Final training  2092/4999 loss: 0.8371 time 24.35s
Final training  2093/4999 loss: 0.8184 time 24.59s
Final training  2094/4999 loss: 0.8151 time 24.19s
Final training  2095/4999 loss: 0.8174 time 24.59s
Final training  2096/4999 loss: 0.8227 time 24.53s
Final training  2097/4999 loss: 0.8434 time 23.95s
Final training  2098/4999 loss: 0.8044 time 24.65s
Final training  2099/4999 loss: 0.8327 time 24.47s
Dice accuracy for each class:  (tensor([0.9954, 0.9459, 0.9404, 0.9397, 0.7554, 0.6637, 0.8952, 0.7641, 0.9017,
        0.8437, 0.7377, 0.7394, 0.6460, 0.5988], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2099/4999 acc [ 0.813] time 183.65s
trigger times: 2
Final training  2100/4999 loss: 0.8325 time 24.63s
Final training  2101/4999 loss: 0.8101 time 24.87s
Final training  2102/4999 loss: 0.8440 time 24.37s
Final training  2103/4999 loss: 0.8466 time 24.61s
Final training  2104/4999 loss: 0.8603 time 24.49s
Final training  2105/4999 loss: 0.8264 time 24.81s
Final training  2106/4999 loss: 0.8465 time 24.95s
Final training  2107/4999 loss: 0.8298 time 25.30s
Final training  2108/4999 loss: 0.8310 time 25.33s
Final training  2109/4999 loss: 0.8029 time 25.07s
Final training  2110/4999 loss: 0.8359 time 24.82s
Final training  2111/4999 loss: 0.8506 time 24.77s
Final training  2112/4999 loss: 0.8684 time 24.76s
Final training  2113/4999 loss: 0.8268 time 24.62s
Final training  2114/4999 loss: 0.8385 time 24.93s
Final training  2115/4999 loss: 0.8256 time 24.54s
Final training  2116/4999 loss: 0.8269 time 24.77s
Final training  2117/4999 loss: 0.8339 time 24.84s
Final training  2118/4999 loss: 0.8229 time 24.61s
Final training  2119/4999 loss: 0.8658 time 24.54s
Final training  2120/4999 loss: 0.8265 time 24.34s
Final training  2121/4999 loss: 0.8218 time 24.97s
Final training  2122/4999 loss: 0.8474 time 24.82s
Final training  2123/4999 loss: 0.8311 time 24.51s
Final training  2124/4999 loss: 0.8434 time 24.76s
Final training  2125/4999 loss: 0.8417 time 24.61s
Final training  2126/4999 loss: 0.8363 time 24.84s
Final training  2127/4999 loss: 0.8372 time 24.86s
Final training  2128/4999 loss: 0.8217 time 24.78s
Final training  2129/4999 loss: 0.8262 time 24.61s
Final training  2130/4999 loss: 0.8279 time 24.79s
Final training  2131/4999 loss: 0.8348 time 24.54s
Final training  2132/4999 loss: 0.7934 time 24.55s
Final training  2133/4999 loss: 0.8290 time 24.82s
Final training  2134/4999 loss: 0.8331 time 24.72s
Final training  2135/4999 loss: 0.7964 time 24.81s
Final training  2136/4999 loss: 0.8231 time 24.35s
Final training  2137/4999 loss: 0.8279 time 24.67s
Final training  2138/4999 loss: 0.8064 time 24.64s
Final training  2139/4999 loss: 0.8122 time 24.84s
Final training  2140/4999 loss: 0.8203 time 24.43s
Final training  2141/4999 loss: 0.8133 time 24.57s
Final training  2142/4999 loss: 0.8139 time 24.42s
Final training  2143/4999 loss: 0.8625 time 24.56s
Final training  2144/4999 loss: 0.8310 time 24.69s
Final training  2145/4999 loss: 0.8104 time 24.68s
Final training  2146/4999 loss: 0.8243 time 24.46s
Final training  2147/4999 loss: 0.8217 time 24.78s
Final training  2148/4999 loss: 0.8453 time 24.65s
Final training  2149/4999 loss: 0.8570 time 25.34s
Final training  2150/4999 loss: 0.8512 time 24.52s
Final training  2151/4999 loss: 0.8364 time 24.53s
Final training  2152/4999 loss: 0.8195 time 24.59s
Final training  2153/4999 loss: 0.8238 time 24.57s
Final training  2154/4999 loss: 0.8586 time 24.44s
Final training  2155/4999 loss: 0.8633 time 24.48s
Final training  2156/4999 loss: 0.8239 time 24.58s
Final training  2157/4999 loss: 0.8096 time 24.75s
Final training  2158/4999 loss: 0.8289 time 24.72s
Final training  2159/4999 loss: 0.8178 time 24.83s
Final training  2160/4999 loss: 0.8092 time 24.64s
Final training  2161/4999 loss: 0.8309 time 24.75s
Final training  2162/4999 loss: 0.8227 time 24.64s
Final training  2163/4999 loss: 0.8288 time 24.74s
Final training  2164/4999 loss: 0.7950 time 24.68s
Final training  2165/4999 loss: 0.8197 time 24.63s
Final training  2166/4999 loss: 0.8192 time 24.53s
Final training  2167/4999 loss: 0.8082 time 24.79s
Final training  2168/4999 loss: 0.8097 time 24.35s
Final training  2169/4999 loss: 0.8336 time 24.67s
Final training  2170/4999 loss: 0.8379 time 24.69s
Final training  2171/4999 loss: 0.8121 time 24.67s
Final training  2172/4999 loss: 0.8130 time 24.64s
Final training  2173/4999 loss: 0.8295 time 24.81s
Final training  2174/4999 loss: 0.8196 time 25.07s
Final training  2175/4999 loss: 0.8165 time 24.83s
Final training  2176/4999 loss: 0.8356 time 24.86s
Final training  2177/4999 loss: 0.7984 time 24.63s
Final training  2178/4999 loss: 0.8149 time 24.09s
Final training  2179/4999 loss: 0.8394 time 24.53s
Final training  2180/4999 loss: 0.8125 time 24.63s
Final training  2181/4999 loss: 0.8343 time 24.86s
Final training  2182/4999 loss: 0.8363 time 24.46s
Final training  2183/4999 loss: 0.8575 time 24.68s
Final training  2184/4999 loss: 0.8631 time 24.31s
Final training  2185/4999 loss: 0.8508 time 24.67s
Final training  2186/4999 loss: 0.8451 time 24.64s
Final training  2187/4999 loss: 0.8357 time 24.91s
Final training  2188/4999 loss: 0.8449 time 24.95s
Final training  2189/4999 loss: 0.8330 time 25.18s
Final training  2190/4999 loss: 0.8415 time 25.05s
Final training  2191/4999 loss: 0.8356 time 24.56s
Final training  2192/4999 loss: 0.8380 time 24.59s
Final training  2193/4999 loss: 0.8301 time 24.64s
Final training  2194/4999 loss: 0.8238 time 24.56s
Final training  2195/4999 loss: 0.8218 time 24.41s
Final training  2196/4999 loss: 0.8150 time 24.60s
Final training  2197/4999 loss: 0.8108 time 24.58s
Final training  2198/4999 loss: 0.8287 time 25.09s
Final training  2199/4999 loss: 0.8485 time 25.35s
Dice accuracy for each class:  (tensor([0.9942, 0.9266, 0.9346, 0.9415, 0.7303, 0.7228, 0.8570, 0.7370, 0.8873,
        0.8404, 0.7554, 0.7760, 0.6425, 0.6223], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2199/4999 acc [ 0.813] time 183.50s
trigger times: 3
Final training  2200/4999 loss: 0.8312 time 24.56s
Final training  2201/4999 loss: 0.8468 time 24.64s
Final training  2202/4999 loss: 0.8402 time 24.57s
Final training  2203/4999 loss: 0.8212 time 24.56s
Final training  2204/4999 loss: 0.8227 time 24.48s
Final training  2205/4999 loss: 0.8093 time 24.72s
Final training  2206/4999 loss: 0.8136 time 24.59s
Final training  2207/4999 loss: 0.8199 time 24.58s
Final training  2208/4999 loss: 0.8162 time 24.13s
Final training  2209/4999 loss: 0.7958 time 24.70s
Final training  2210/4999 loss: 0.7930 time 24.70s
Final training  2211/4999 loss: 0.8235 time 24.90s
Final training  2212/4999 loss: 0.8375 time 24.76s
Final training  2213/4999 loss: 0.8511 time 24.72s
Final training  2214/4999 loss: 0.8355 time 24.66s
Final training  2215/4999 loss: 0.8402 time 24.37s
Final training  2216/4999 loss: 0.8203 time 24.67s
Final training  2217/4999 loss: 0.8191 time 25.08s
Final training  2218/4999 loss: 0.8346 time 25.04s
Final training  2219/4999 loss: 0.8038 time 25.04s
Final training  2220/4999 loss: 0.8396 time 25.36s
Final training  2221/4999 loss: 0.8300 time 25.09s
Final training  2222/4999 loss: 0.8222 time 25.03s
Final training  2223/4999 loss: 0.8298 time 25.34s
Final training  2224/4999 loss: 0.8173 time 25.18s
Final training  2225/4999 loss: 0.8254 time 24.82s
Final training  2226/4999 loss: 0.8198 time 24.49s
Final training  2227/4999 loss: 0.7786 time 24.54s
Final training  2228/4999 loss: 0.8552 time 24.51s
Final training  2229/4999 loss: 0.8471 time 24.67s
Final training  2230/4999 loss: 0.8294 time 24.70s
Final training  2231/4999 loss: 0.8217 time 24.34s
Final training  2232/4999 loss: 0.8133 time 24.64s
Final training  2233/4999 loss: 0.8052 time 24.96s
Final training  2234/4999 loss: 0.8089 time 24.52s
Final training  2235/4999 loss: 0.8507 time 24.33s
Final training  2236/4999 loss: 0.8386 time 24.48s
Final training  2237/4999 loss: 0.8228 time 24.67s
Final training  2238/4999 loss: 0.8290 time 24.56s
Final training  2239/4999 loss: 0.8350 time 24.58s
Final training  2240/4999 loss: 0.8274 time 24.41s
Final training  2241/4999 loss: 0.8509 time 24.48s
Final training  2242/4999 loss: 0.8261 time 24.58s
Final training  2243/4999 loss: 0.8191 time 24.63s
Final training  2244/4999 loss: 0.8472 time 24.45s
Final training  2245/4999 loss: 0.8185 time 24.64s
Final training  2246/4999 loss: 0.8354 time 24.69s
Final training  2247/4999 loss: 0.8010 time 24.44s
Final training  2248/4999 loss: 0.8175 time 24.80s
Final training  2249/4999 loss: 0.8270 time 24.69s
Final training  2250/4999 loss: 0.8356 time 24.64s
Final training  2251/4999 loss: 0.8080 time 24.65s
Final training  2252/4999 loss: 0.8198 time 24.75s
Final training  2253/4999 loss: 0.8018 time 24.60s
Final training  2254/4999 loss: 0.8006 time 24.63s
Final training  2255/4999 loss: 0.7821 time 24.56s
Final training  2256/4999 loss: 0.8297 time 24.57s
Final training  2257/4999 loss: 0.8350 time 24.57s
Final training  2258/4999 loss: 0.8264 time 24.85s
Final training  2259/4999 loss: 0.8837 time 24.62s
Final training  2260/4999 loss: 0.8699 time 24.68s
Final training  2261/4999 loss: 0.8360 time 24.41s
Final training  2262/4999 loss: 0.8282 time 24.49s
Final training  2263/4999 loss: 0.8273 time 24.59s
Final training  2264/4999 loss: 0.8137 time 24.91s
Final training  2265/4999 loss: 0.8121 time 24.76s
Final training  2266/4999 loss: 0.8215 time 24.67s
Final training  2267/4999 loss: 0.8070 time 24.59s
Final training  2268/4999 loss: 0.8209 time 24.57s
Final training  2269/4999 loss: 0.8466 time 24.56s
Final training  2270/4999 loss: 0.8528 time 24.66s
Final training  2271/4999 loss: 0.8484 time 24.69s
Final training  2272/4999 loss: 0.8496 time 24.49s
Final training  2273/4999 loss: 0.8189 time 24.91s
Final training  2274/4999 loss: 0.8630 time 24.90s
Final training  2275/4999 loss: 0.8581 time 24.91s
Final training  2276/4999 loss: 0.8332 time 24.77s
Final training  2277/4999 loss: 0.8658 time 24.91s
Final training  2278/4999 loss: 0.8065 time 24.55s
Final training  2279/4999 loss: 0.8240 time 24.37s
Final training  2280/4999 loss: 0.8327 time 24.54s
Final training  2281/4999 loss: 0.8395 time 24.61s
Final training  2282/4999 loss: 0.8361 time 24.86s
Final training  2283/4999 loss: 0.8210 time 24.57s
Final training  2284/4999 loss: 0.8245 time 24.47s
Final training  2285/4999 loss: 0.8232 time 24.64s
Final training  2286/4999 loss: 0.8128 time 24.55s
Final training  2287/4999 loss: 0.8424 time 24.60s
Final training  2288/4999 loss: 0.8462 time 24.75s
Final training  2289/4999 loss: 0.8206 time 24.48s
Final training  2290/4999 loss: 0.8080 time 24.58s
Final training  2291/4999 loss: 0.8113 time 24.99s
Final training  2292/4999 loss: 0.8337 time 25.02s
Final training  2293/4999 loss: 0.8045 time 25.06s
Final training  2294/4999 loss: 0.8325 time 24.78s
Final training  2295/4999 loss: 0.8174 time 24.67s
Final training  2296/4999 loss: 0.8082 time 24.67s
Final training  2297/4999 loss: 0.7926 time 24.71s
Final training  2298/4999 loss: 0.8275 time 24.74s
Final training  2299/4999 loss: 0.8133 time 24.71s
Dice accuracy for each class:  (tensor([0.9962, 0.9063, 0.9407, 0.9401, 0.7459, 0.7165, 0.9395, 0.7604, 0.8997,
        0.8503, 0.7295, 0.7595, 0.5941, 0.5844], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2299/4999 acc [ 0.812] time 181.73s
trigger times: 4
Final training  2300/4999 loss: 0.8213 time 24.79s
Final training  2301/4999 loss: 0.8187 time 24.70s
Final training  2302/4999 loss: 0.8118 time 24.68s
Final training  2303/4999 loss: 0.8251 time 24.57s
Final training  2304/4999 loss: 0.8357 time 24.66s
Final training  2305/4999 loss: 0.8082 time 24.68s
Final training  2306/4999 loss: 0.8345 time 24.43s
Final training  2307/4999 loss: 0.8421 time 24.70s
Final training  2308/4999 loss: 0.8722 time 24.81s
Final training  2309/4999 loss: 0.8301 time 24.77s
Final training  2310/4999 loss: 0.8100 time 24.85s
Final training  2311/4999 loss: 0.8280 time 24.73s
Final training  2312/4999 loss: 0.7974 time 24.56s
Final training  2313/4999 loss: 0.7855 time 24.56s
Final training  2314/4999 loss: 0.8359 time 24.72s
Final training  2315/4999 loss: 0.8275 time 24.42s
Final training  2316/4999 loss: 0.8159 time 24.50s
Final training  2317/4999 loss: 0.8186 time 24.51s
Final training  2318/4999 loss: 0.8400 time 24.61s
Final training  2319/4999 loss: 0.8485 time 24.44s
Final training  2320/4999 loss: 0.8358 time 25.02s
Final training  2321/4999 loss: 0.8654 time 24.76s
Final training  2322/4999 loss: 0.8464 time 24.48s
Final training  2323/4999 loss: 0.8265 time 24.80s
Final training  2324/4999 loss: 0.8223 time 24.80s
Final training  2325/4999 loss: 0.8248 time 24.74s
Final training  2326/4999 loss: 0.8431 time 24.60s
Final training  2327/4999 loss: 0.8384 time 24.59s
Final training  2328/4999 loss: 0.8052 time 24.72s
Final training  2329/4999 loss: 0.8318 time 24.50s
Final training  2330/4999 loss: 0.8033 time 24.48s
Final training  2331/4999 loss: 0.7984 time 24.59s
Final training  2332/4999 loss: 0.8144 time 24.69s
Final training  2333/4999 loss: 0.8440 time 24.67s
Final training  2334/4999 loss: 0.8476 time 24.65s
Final training  2335/4999 loss: 0.8266 time 25.09s
Final training  2336/4999 loss: 0.8532 time 24.81s
Final training  2337/4999 loss: 0.8250 time 24.60s
Final training  2338/4999 loss: 0.8462 time 24.62s
Final training  2339/4999 loss: 0.7933 time 24.54s
Final training  2340/4999 loss: 0.8182 time 24.50s
Final training  2341/4999 loss: 0.8215 time 24.67s
Final training  2342/4999 loss: 0.8213 time 24.76s
Final training  2343/4999 loss: 0.8045 time 24.52s
Final training  2344/4999 loss: 0.8144 time 24.47s
Final training  2345/4999 loss: 0.8165 time 24.47s
Final training  2346/4999 loss: 0.8279 time 24.69s
Final training  2347/4999 loss: 0.8147 time 24.58s
Final training  2348/4999 loss: 0.8057 time 24.52s
Final training  2349/4999 loss: 0.8286 time 24.63s
Final training  2350/4999 loss: 0.8152 time 24.52s
Final training  2351/4999 loss: 0.8174 time 24.59s
Final training  2352/4999 loss: 0.8257 time 24.69s
Final training  2353/4999 loss: 0.8079 time 24.21s
Final training  2354/4999 loss: 0.8014 time 24.80s
Final training  2355/4999 loss: 0.8241 time 24.49s
Final training  2356/4999 loss: 0.8131 time 24.65s
Final training  2357/4999 loss: 0.8046 time 24.47s
Final training  2358/4999 loss: 0.8355 time 24.21s
Final training  2359/4999 loss: 0.8034 time 24.54s
Final training  2360/4999 loss: 0.7993 time 24.36s
Final training  2361/4999 loss: 0.8113 time 24.72s
Final training  2362/4999 loss: 0.8100 time 24.57s
Final training  2363/4999 loss: 0.8186 time 24.53s
Final training  2364/4999 loss: 0.8351 time 24.56s
Final training  2365/4999 loss: 0.8067 time 24.41s
Final training  2366/4999 loss: 0.8285 time 24.75s
Final training  2367/4999 loss: 0.8173 time 24.46s
Final training  2368/4999 loss: 0.8312 time 24.59s
Final training  2369/4999 loss: 0.8226 time 24.62s
Final training  2370/4999 loss: 0.8086 time 24.85s
Final training  2371/4999 loss: 0.8401 time 24.46s
Final training  2372/4999 loss: 0.7963 time 24.50s
Final training  2373/4999 loss: 0.8044 time 24.56s
Final training  2374/4999 loss: 0.8026 time 24.49s
Final training  2375/4999 loss: 0.7983 time 24.74s
Final training  2376/4999 loss: 0.8046 time 24.60s
Final training  2377/4999 loss: 0.8192 time 24.39s
Final training  2378/4999 loss: 0.8211 time 24.55s
Final training  2379/4999 loss: 0.8275 time 24.75s
Final training  2380/4999 loss: 0.8290 time 24.62s
Final training  2381/4999 loss: 0.7750 time 24.55s
Final training  2382/4999 loss: 0.8265 time 24.66s
Final training  2383/4999 loss: 0.8047 time 24.47s
Final training  2384/4999 loss: 0.8316 time 24.18s
Final training  2385/4999 loss: 0.8276 time 24.66s
Final training  2386/4999 loss: 0.8254 time 24.77s
Final training  2387/4999 loss: 0.8435 time 24.42s
Final training  2388/4999 loss: 0.8004 time 24.76s
Final training  2389/4999 loss: 0.8064 time 24.85s
Final training  2390/4999 loss: 0.7948 time 24.66s
Final training  2391/4999 loss: 0.8255 time 24.71s
Final training  2392/4999 loss: 0.8259 time 24.26s
Final training  2393/4999 loss: 0.8164 time 24.56s
Final training  2394/4999 loss: 0.7981 time 24.48s
Final training  2395/4999 loss: 0.8358 time 24.66s
Final training  2396/4999 loss: 0.8264 time 24.42s
Final training  2397/4999 loss: 0.8097 time 24.61s
Final training  2398/4999 loss: 0.8249 time 24.84s
Final training  2399/4999 loss: 0.8217 time 24.60s
Dice accuracy for each class:  (tensor([0.9953, 0.9400, 0.9380, 0.9431, 0.7139, 0.7211, 0.8886, 0.7957, 0.9026,
        0.8551, 0.7423, 0.7697, 0.6359, 0.5821], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2399/4999 acc [ 0.817] time 183.33s
trigger times: 5
Final training  2400/4999 loss: 0.8021 time 24.61s
Final training  2401/4999 loss: 0.8042 time 23.85s
Final training  2402/4999 loss: 0.8223 time 24.57s
Final training  2403/4999 loss: 0.8149 time 24.57s
Final training  2404/4999 loss: 0.8109 time 24.57s
Final training  2405/4999 loss: 0.8226 time 24.47s
Final training  2406/4999 loss: 0.8292 time 24.60s
Final training  2407/4999 loss: 0.7991 time 24.59s
Final training  2408/4999 loss: 0.8264 time 24.76s
Final training  2409/4999 loss: 0.8477 time 24.71s
Final training  2410/4999 loss: 0.8244 time 24.80s
Final training  2411/4999 loss: 0.8219 time 24.68s
Final training  2412/4999 loss: 0.8305 time 24.60s
Final training  2413/4999 loss: 0.8299 time 24.43s
Final training  2414/4999 loss: 0.7948 time 24.38s
Final training  2415/4999 loss: 0.7944 time 24.68s
Final training  2416/4999 loss: 0.8344 time 24.63s
Final training  2417/4999 loss: 0.8430 time 24.64s
Final training  2418/4999 loss: 0.7945 time 24.42s
Final training  2419/4999 loss: 0.8177 time 24.53s
Final training  2420/4999 loss: 0.8414 time 24.66s
Final training  2421/4999 loss: 0.8067 time 24.74s
Final training  2422/4999 loss: 0.7921 time 24.58s
Final training  2423/4999 loss: 0.8331 time 24.79s
Final training  2424/4999 loss: 0.8269 time 24.67s
Final training  2425/4999 loss: 0.8504 time 24.77s
Final training  2426/4999 loss: 0.8322 time 24.71s
Final training  2427/4999 loss: 0.8292 time 24.44s
Final training  2428/4999 loss: 0.7988 time 24.46s
Final training  2429/4999 loss: 0.8397 time 24.57s
Final training  2430/4999 loss: 0.8259 time 24.67s
Final training  2431/4999 loss: 0.8236 time 24.67s
Final training  2432/4999 loss: 0.8148 time 24.48s
Final training  2433/4999 loss: 0.8052 time 24.28s
Final training  2434/4999 loss: 0.8216 time 24.72s
Final training  2435/4999 loss: 0.8186 time 24.90s
Final training  2436/4999 loss: 0.8266 time 24.49s
Final training  2437/4999 loss: 0.8162 time 24.89s
Final training  2438/4999 loss: 0.8296 time 24.96s
Final training  2439/4999 loss: 0.8331 time 24.71s
Final training  2440/4999 loss: 0.8253 time 24.55s
Final training  2441/4999 loss: 0.8069 time 24.66s
Final training  2442/4999 loss: 0.8175 time 24.82s
Final training  2443/4999 loss: 0.8082 time 24.58s
Final training  2444/4999 loss: 0.8182 time 24.33s
Final training  2445/4999 loss: 0.8210 time 24.36s
Final training  2446/4999 loss: 0.8194 time 24.49s
Final training  2447/4999 loss: 0.8155 time 24.66s
Final training  2448/4999 loss: 0.8233 time 24.31s
Final training  2449/4999 loss: 0.8589 time 24.64s
Final training  2450/4999 loss: 0.8356 time 24.29s
Final training  2451/4999 loss: 0.8306 time 24.21s
Final training  2452/4999 loss: 0.7909 time 24.78s
Final training  2453/4999 loss: 0.8116 time 24.57s
Final training  2454/4999 loss: 0.8058 time 24.56s
Final training  2455/4999 loss: 0.8274 time 24.50s
Final training  2456/4999 loss: 0.8348 time 24.60s
Final training  2457/4999 loss: 0.8232 time 24.46s
Final training  2458/4999 loss: 0.8162 time 24.42s
Final training  2459/4999 loss: 0.8282 time 24.71s
Final training  2460/4999 loss: 0.8693 time 25.18s
Final training  2461/4999 loss: 0.8072 time 24.81s
Final training  2462/4999 loss: 0.8538 time 24.88s
Final training  2463/4999 loss: 0.8190 time 24.94s
Final training  2464/4999 loss: 0.8255 time 24.86s
Final training  2465/4999 loss: 0.8132 time 24.76s
Final training  2466/4999 loss: 0.8080 time 24.78s
Final training  2467/4999 loss: 0.8291 time 24.62s
Final training  2468/4999 loss: 0.8020 time 24.94s
Final training  2469/4999 loss: 0.8294 time 25.11s
Final training  2470/4999 loss: 0.8221 time 25.13s
Final training  2471/4999 loss: 0.8099 time 25.09s
Final training  2472/4999 loss: 0.8135 time 25.09s
Final training  2473/4999 loss: 0.8257 time 24.87s
Final training  2474/4999 loss: 0.8385 time 24.98s
Final training  2475/4999 loss: 0.8491 time 25.37s
Final training  2476/4999 loss: 0.8011 time 25.16s
Final training  2477/4999 loss: 0.8163 time 25.24s
Final training  2478/4999 loss: 0.8181 time 25.06s
Final training  2479/4999 loss: 0.8168 time 25.04s
Final training  2480/4999 loss: 0.8003 time 24.88s
Final training  2481/4999 loss: 0.8183 time 24.79s
Final training  2482/4999 loss: 0.8246 time 25.12s
Final training  2483/4999 loss: 0.7953 time 25.47s
Final training  2484/4999 loss: 0.8119 time 25.42s
Final training  2485/4999 loss: 0.8045 time 25.43s
Final training  2486/4999 loss: 0.8124 time 25.24s
Final training  2487/4999 loss: 0.7957 time 24.95s
Final training  2488/4999 loss: 0.8341 time 24.93s
Final training  2489/4999 loss: 0.8051 time 24.91s
Final training  2490/4999 loss: 0.8465 time 24.63s
Final training  2491/4999 loss: 0.8251 time 24.61s
Final training  2492/4999 loss: 0.8223 time 24.52s
Final training  2493/4999 loss: 0.8199 time 24.47s
Final training  2494/4999 loss: 0.8227 time 24.94s
Final training  2495/4999 loss: 0.8213 time 24.73s
Final training  2496/4999 loss: 0.8181 time 24.94s
Final training  2497/4999 loss: 0.7939 time 24.77s
Final training  2498/4999 loss: 0.8121 time 24.50s
Final training  2499/4999 loss: 0.8386 time 24.78s
Dice accuracy for each class:  (tensor([0.9958, 0.9417, 0.9412, 0.9428, 0.7200, 0.7218, 0.9091, 0.7947, 0.9020,
        0.8514, 0.7492, 0.7649, 0.6608, 0.6060], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2499/4999 acc [ 0.823] time 183.48s
Reset trigger time to 0
new best (0.817409 --> 0.822836). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  2500/4999 loss: 0.8101 time 24.31s
Final training  2501/4999 loss: 0.8243 time 24.49s
Final training  2502/4999 loss: 0.8235 time 24.69s
Final training  2503/4999 loss: 0.7935 time 24.95s
Final training  2504/4999 loss: 0.8251 time 24.72s
Final training  2505/4999 loss: 0.8477 time 24.48s
Final training  2506/4999 loss: 0.8170 time 24.56s
Final training  2507/4999 loss: 0.8264 time 24.73s
Final training  2508/4999 loss: 0.7972 time 24.64s
Final training  2509/4999 loss: 0.8254 time 24.25s
Final training  2510/4999 loss: 0.8146 time 24.60s
Final training  2511/4999 loss: 0.8101 time 24.75s
Final training  2512/4999 loss: 0.8253 time 24.76s
Final training  2513/4999 loss: 0.8018 time 24.71s
Final training  2514/4999 loss: 0.7981 time 24.57s
Final training  2515/4999 loss: 0.8041 time 24.61s
Final training  2516/4999 loss: 0.8106 time 24.29s
Final training  2517/4999 loss: 0.7961 time 24.53s
Final training  2518/4999 loss: 0.8256 time 24.61s
Final training  2519/4999 loss: 0.8253 time 24.30s
Final training  2520/4999 loss: 0.8116 time 24.61s
Final training  2521/4999 loss: 0.7993 time 24.66s
Final training  2522/4999 loss: 0.8156 time 24.56s
Final training  2523/4999 loss: 0.8221 time 24.58s
Final training  2524/4999 loss: 0.8285 time 24.51s
Final training  2525/4999 loss: 0.8225 time 24.58s
Final training  2526/4999 loss: 0.8502 time 24.57s
Final training  2527/4999 loss: 0.8407 time 24.92s
Final training  2528/4999 loss: 0.8076 time 24.61s
Final training  2529/4999 loss: 0.7927 time 24.54s
Final training  2530/4999 loss: 0.8399 time 24.60s
Final training  2531/4999 loss: 0.8140 time 24.70s
Final training  2532/4999 loss: 0.8256 time 24.62s
Final training  2533/4999 loss: 0.8030 time 24.36s
Final training  2534/4999 loss: 0.8094 time 25.06s
Final training  2535/4999 loss: 0.8395 time 24.78s
Final training  2536/4999 loss: 0.8565 time 24.63s
Final training  2537/4999 loss: 0.8247 time 24.80s
Final training  2538/4999 loss: 0.8291 time 24.74s
Final training  2539/4999 loss: 0.8256 time 24.90s
Final training  2540/4999 loss: 0.8418 time 24.87s
Final training  2541/4999 loss: 0.8431 time 24.90s
Final training  2542/4999 loss: 0.8523 time 24.96s
Final training  2543/4999 loss: 0.8342 time 24.67s
Final training  2544/4999 loss: 0.8260 time 24.56s
Final training  2545/4999 loss: 0.8294 time 24.51s
Final training  2546/4999 loss: 0.8212 time 24.61s
Final training  2547/4999 loss: 0.8207 time 24.38s
Final training  2548/4999 loss: 0.8220 time 24.66s
Final training  2549/4999 loss: 0.8377 time 24.75s
Final training  2550/4999 loss: 0.8335 time 24.74s
Final training  2551/4999 loss: 0.8031 time 24.55s
Final training  2552/4999 loss: 0.8544 time 24.88s
Final training  2553/4999 loss: 0.7984 time 24.58s
Final training  2554/4999 loss: 0.8126 time 24.61s
Final training  2555/4999 loss: 0.8176 time 24.88s
Final training  2556/4999 loss: 0.8243 time 24.43s
Final training  2557/4999 loss: 0.7952 time 24.45s
Final training  2558/4999 loss: 0.8375 time 24.47s
Final training  2559/4999 loss: 0.8247 time 24.47s
Final training  2560/4999 loss: 0.8248 time 24.51s
Final training  2561/4999 loss: 0.8225 time 24.53s
Final training  2562/4999 loss: 0.8390 time 24.25s
Final training  2563/4999 loss: 0.8187 time 24.50s
Final training  2564/4999 loss: 0.7945 time 24.64s
Final training  2565/4999 loss: 0.8103 time 24.48s
Final training  2566/4999 loss: 0.7986 time 24.50s
Final training  2567/4999 loss: 0.8260 time 24.44s
Final training  2568/4999 loss: 0.8198 time 24.50s
Final training  2569/4999 loss: 0.7942 time 24.47s
Final training  2570/4999 loss: 0.7879 time 24.50s
Final training  2571/4999 loss: 0.8285 time 24.73s
Final training  2572/4999 loss: 0.8052 time 24.54s
Final training  2573/4999 loss: 0.8057 time 24.20s
Final training  2574/4999 loss: 0.8330 time 24.27s
Final training  2575/4999 loss: 0.8122 time 24.68s
Final training  2576/4999 loss: 0.8063 time 24.44s
Final training  2577/4999 loss: 0.8129 time 24.41s
Final training  2578/4999 loss: 0.8166 time 24.26s
Final training  2579/4999 loss: 0.7864 time 24.78s
Final training  2580/4999 loss: 0.8043 time 24.89s
Final training  2581/4999 loss: 0.8398 time 24.55s
Final training  2582/4999 loss: 0.8292 time 24.73s
Final training  2583/4999 loss: 0.8136 time 24.87s
Final training  2584/4999 loss: 0.8188 time 25.11s
Final training  2585/4999 loss: 0.7972 time 24.68s
Final training  2586/4999 loss: 0.7991 time 24.66s
Final training  2587/4999 loss: 0.8126 time 24.85s
Final training  2588/4999 loss: 0.8109 time 24.76s
Final training  2589/4999 loss: 0.7764 time 24.43s
Final training  2590/4999 loss: 0.8191 time 24.60s
Final training  2591/4999 loss: 0.7972 time 24.49s
Final training  2592/4999 loss: 0.7967 time 24.59s
Final training  2593/4999 loss: 0.8124 time 24.54s
Final training  2594/4999 loss: 0.7897 time 24.52s
Final training  2595/4999 loss: 0.8105 time 24.52s
Final training  2596/4999 loss: 0.7899 time 24.58s
Final training  2597/4999 loss: 0.8036 time 24.52s
Final training  2598/4999 loss: 0.7922 time 24.56s
Final training  2599/4999 loss: 0.8112 time 24.61s
Dice accuracy for each class:  (tensor([0.9956, 0.9230, 0.9387, 0.9423, 0.7304, 0.7104, 0.9066, 0.7787, 0.9024,
        0.8521, 0.7379, 0.7595, 0.6138, 0.5551], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2599/4999 acc [ 0.811] time 183.95s
trigger times: 1
Final training  2600/4999 loss: 0.7973 time 24.59s
Final training  2601/4999 loss: 0.8087 time 24.52s
Final training  2602/4999 loss: 0.8306 time 24.75s
Final training  2603/4999 loss: 0.7832 time 24.67s
Final training  2604/4999 loss: 0.8118 time 24.53s
Final training  2605/4999 loss: 0.8201 time 24.30s
Final training  2606/4999 loss: 0.8093 time 24.55s
Final training  2607/4999 loss: 0.8121 time 24.58s
Final training  2608/4999 loss: 0.7943 time 24.49s
Final training  2609/4999 loss: 0.8207 time 24.70s
Final training  2610/4999 loss: 0.8134 time 24.63s
Final training  2611/4999 loss: 0.8046 time 24.49s
Final training  2612/4999 loss: 0.8164 time 24.80s
Final training  2613/4999 loss: 0.7959 time 24.72s
Final training  2614/4999 loss: 0.8129 time 24.87s
Final training  2615/4999 loss: 0.7998 time 24.68s
Final training  2616/4999 loss: 0.8135 time 24.68s
Final training  2617/4999 loss: 0.8153 time 24.59s
Final training  2618/4999 loss: 0.7908 time 24.43s
Final training  2619/4999 loss: 0.8292 time 24.33s
Final training  2620/4999 loss: 0.8303 time 24.58s
Final training  2621/4999 loss: 0.8067 time 24.46s
Final training  2622/4999 loss: 0.8155 time 24.61s
Final training  2623/4999 loss: 0.8119 time 24.34s
Final training  2624/4999 loss: 0.8226 time 24.75s
Final training  2625/4999 loss: 0.7931 time 24.81s
Final training  2626/4999 loss: 0.8072 time 24.60s
Final training  2627/4999 loss: 0.8249 time 24.57s
Final training  2628/4999 loss: 0.7848 time 24.46s
Final training  2629/4999 loss: 0.8173 time 24.53s
Final training  2630/4999 loss: 0.8178 time 24.51s
Final training  2631/4999 loss: 0.8138 time 24.41s
Final training  2632/4999 loss: 0.8156 time 24.48s
Final training  2633/4999 loss: 0.8316 time 24.94s
Final training  2634/4999 loss: 0.8096 time 24.88s
Final training  2635/4999 loss: 0.8078 time 25.01s
Final training  2636/4999 loss: 0.7916 time 24.67s
Final training  2637/4999 loss: 0.8141 time 24.94s
Final training  2638/4999 loss: 0.8164 time 25.01s
Final training  2639/4999 loss: 0.8324 time 24.88s
Final training  2640/4999 loss: 0.8049 time 24.89s
Final training  2641/4999 loss: 0.8153 time 25.08s
Final training  2642/4999 loss: 0.8223 time 24.97s
Final training  2643/4999 loss: 0.7978 time 25.37s
Final training  2644/4999 loss: 0.8271 time 25.07s
Final training  2645/4999 loss: 0.8155 time 24.96s
Final training  2646/4999 loss: 0.7966 time 24.95s
Final training  2647/4999 loss: 0.8114 time 25.17s
Final training  2648/4999 loss: 0.8071 time 24.93s
Final training  2649/4999 loss: 0.8136 time 24.92s
Final training  2650/4999 loss: 0.8094 time 24.98s
Final training  2651/4999 loss: 0.7924 time 24.40s
Final training  2652/4999 loss: 0.8237 time 24.68s
Final training  2653/4999 loss: 0.8087 time 24.75s
Final training  2654/4999 loss: 0.8099 time 24.56s
Final training  2655/4999 loss: 0.8202 time 24.51s
Final training  2656/4999 loss: 0.8096 time 24.69s
Final training  2657/4999 loss: 0.8353 time 24.71s
Final training  2658/4999 loss: 0.7983 time 24.57s
Final training  2659/4999 loss: 0.7775 time 24.48s
Final training  2660/4999 loss: 0.8230 time 24.39s
Final training  2661/4999 loss: 0.7992 time 24.57s
Final training  2662/4999 loss: 0.8192 time 24.65s
Final training  2663/4999 loss: 0.7908 time 24.63s
Final training  2664/4999 loss: 0.8103 time 24.81s
Final training  2665/4999 loss: 0.8035 time 24.55s
Final training  2666/4999 loss: 0.8306 time 24.98s
Final training  2667/4999 loss: 0.8168 time 24.76s
Final training  2668/4999 loss: 0.8187 time 24.60s
Final training  2669/4999 loss: 0.8068 time 24.70s
Final training  2670/4999 loss: 0.8301 time 25.19s
Final training  2671/4999 loss: 0.8135 time 25.19s
Final training  2672/4999 loss: 0.8354 time 25.08s
Final training  2673/4999 loss: 0.8023 time 25.04s
Final training  2674/4999 loss: 0.8385 time 25.28s
Final training  2675/4999 loss: 0.7973 time 24.99s
Final training  2676/4999 loss: 0.7897 time 25.24s
Final training  2677/4999 loss: 0.7965 time 24.91s
Final training  2678/4999 loss: 0.8031 time 25.33s
Final training  2679/4999 loss: 0.8212 time 24.87s
Final training  2680/4999 loss: 0.8084 time 25.44s
Final training  2681/4999 loss: 0.7926 time 25.13s
Final training  2682/4999 loss: 0.8239 time 25.19s
Final training  2683/4999 loss: 0.7825 time 25.28s
Final training  2684/4999 loss: 0.8118 time 25.52s
Final training  2685/4999 loss: 0.8331 time 25.08s
Final training  2686/4999 loss: 0.8070 time 25.37s
Final training  2687/4999 loss: 0.8162 time 25.29s
Final training  2688/4999 loss: 0.8008 time 25.54s
Final training  2689/4999 loss: 0.8262 time 25.07s
Final training  2690/4999 loss: 0.7958 time 25.36s
Final training  2691/4999 loss: 0.8189 time 25.19s
Final training  2692/4999 loss: 0.8010 time 25.09s
Final training  2693/4999 loss: 0.8102 time 24.98s
Final training  2694/4999 loss: 0.7860 time 24.72s
Final training  2695/4999 loss: 0.7997 time 24.92s
Final training  2696/4999 loss: 0.8038 time 25.00s
Final training  2697/4999 loss: 0.8134 time 24.92s
Final training  2698/4999 loss: 0.8141 time 24.89s
Final training  2699/4999 loss: 0.8119 time 24.89s
Dice accuracy for each class:  (tensor([0.9966, 0.9551, 0.9419, 0.9433, 0.7318, 0.7208, 0.9414, 0.7973, 0.9069,
        0.8514, 0.7388, 0.7720, 0.6602, 0.6173], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2699/4999 acc [ 0.828] time 183.39s
Reset trigger time to 0
new best (0.822836 --> 0.827917). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  2700/4999 loss: 0.8175 time 25.12s
Final training  2701/4999 loss: 0.8127 time 25.11s
Final training  2702/4999 loss: 0.7993 time 25.37s
Final training  2703/4999 loss: 0.7958 time 25.01s
Final training  2704/4999 loss: 0.8016 time 25.05s
Final training  2705/4999 loss: 0.8185 time 25.22s
Final training  2706/4999 loss: 0.7889 time 25.20s
Final training  2707/4999 loss: 0.8054 time 25.41s
Final training  2708/4999 loss: 0.8333 time 25.11s
Final training  2709/4999 loss: 0.8276 time 25.16s
Final training  2710/4999 loss: 0.8220 time 24.84s
Final training  2711/4999 loss: 0.7986 time 24.64s
Final training  2712/4999 loss: 0.7960 time 24.74s
Final training  2713/4999 loss: 0.8275 time 24.59s
Final training  2714/4999 loss: 0.8208 time 24.82s
Final training  2715/4999 loss: 0.8900 time 24.63s
Final training  2716/4999 loss: 0.8386 time 24.62s
Final training  2717/4999 loss: 0.8257 time 24.52s
Final training  2718/4999 loss: 0.8165 time 24.66s
Final training  2719/4999 loss: 0.8268 time 24.65s
Final training  2720/4999 loss: 0.8145 time 24.51s
Final training  2721/4999 loss: 0.8101 time 24.64s
Final training  2722/4999 loss: 0.8050 time 24.38s
Final training  2723/4999 loss: 0.7943 time 24.77s
Final training  2724/4999 loss: 0.8064 time 24.68s
Final training  2725/4999 loss: 0.7907 time 24.64s
Final training  2726/4999 loss: 0.8182 time 24.79s
Final training  2727/4999 loss: 0.8072 time 25.12s
Final training  2728/4999 loss: 0.8266 time 24.63s
Final training  2729/4999 loss: 0.8107 time 24.51s
Final training  2730/4999 loss: 0.8169 time 24.47s
Final training  2731/4999 loss: 0.8094 time 24.54s
Final training  2732/4999 loss: 0.8005 time 24.66s
Final training  2733/4999 loss: 0.8151 time 24.52s
Final training  2734/4999 loss: 0.8020 time 24.46s
Final training  2735/4999 loss: 0.8029 time 24.54s
Final training  2736/4999 loss: 0.8245 time 24.47s
Final training  2737/4999 loss: 0.8079 time 24.47s
Final training  2738/4999 loss: 0.7893 time 24.42s
Final training  2739/4999 loss: 0.8304 time 24.75s
Final training  2740/4999 loss: 0.8013 time 24.51s
Final training  2741/4999 loss: 0.7848 time 24.52s
Final training  2742/4999 loss: 0.8278 time 24.33s
Final training  2743/4999 loss: 0.7999 time 24.43s
Final training  2744/4999 loss: 0.8121 time 24.64s
Final training  2745/4999 loss: 0.8100 time 24.56s
Final training  2746/4999 loss: 0.8096 time 24.41s
Final training  2747/4999 loss: 0.8062 time 24.75s
Final training  2748/4999 loss: 0.7998 time 24.56s
Final training  2749/4999 loss: 0.8420 time 24.69s
Final training  2750/4999 loss: 0.8071 time 24.44s
Final training  2751/4999 loss: 0.8267 time 24.49s
Final training  2752/4999 loss: 0.8245 time 24.73s
Final training  2753/4999 loss: 0.8073 time 24.51s
Final training  2754/4999 loss: 0.8153 time 24.71s
Final training  2755/4999 loss: 0.7972 time 24.73s
Final training  2756/4999 loss: 0.8157 time 24.43s
Final training  2757/4999 loss: 0.8036 time 24.60s
Final training  2758/4999 loss: 0.7892 time 24.68s
Final training  2759/4999 loss: 0.8200 time 24.34s
Final training  2760/4999 loss: 0.8052 time 24.51s
Final training  2761/4999 loss: 0.7966 time 24.05s
Final training  2762/4999 loss: 0.8166 time 24.52s
Final training  2763/4999 loss: 0.8265 time 24.83s
Final training  2764/4999 loss: 0.8355 time 25.04s
Final training  2765/4999 loss: 0.8466 time 24.69s
Final training  2766/4999 loss: 0.8396 time 24.90s
Final training  2767/4999 loss: 0.8332 time 25.20s
Final training  2768/4999 loss: 0.7942 time 25.41s
Final training  2769/4999 loss: 0.8099 time 24.57s
Final training  2770/4999 loss: 0.8114 time 24.82s
Final training  2771/4999 loss: 0.8331 time 24.95s
Final training  2772/4999 loss: 0.8161 time 24.92s
Final training  2773/4999 loss: 0.8293 time 25.06s
Final training  2774/4999 loss: 0.8060 time 24.73s
Final training  2775/4999 loss: 0.8333 time 24.77s
Final training  2776/4999 loss: 0.7783 time 24.71s
Final training  2777/4999 loss: 0.7893 time 25.06s
Final training  2778/4999 loss: 0.8027 time 24.76s
Final training  2779/4999 loss: 0.8097 time 24.77s
Final training  2780/4999 loss: 0.8284 time 24.85s
Final training  2781/4999 loss: 0.8013 time 24.77s
Final training  2782/4999 loss: 0.8398 time 24.55s
Final training  2783/4999 loss: 0.8089 time 24.53s
Final training  2784/4999 loss: 0.8164 time 24.57s
Final training  2785/4999 loss: 0.8144 time 24.75s
Final training  2786/4999 loss: 0.8062 time 24.48s
Final training  2787/4999 loss: 0.8209 time 24.60s
Final training  2788/4999 loss: 0.8218 time 25.01s
Final training  2789/4999 loss: 0.8148 time 24.97s
Final training  2790/4999 loss: 0.7942 time 24.89s
Final training  2791/4999 loss: 0.8135 time 25.02s
Final training  2792/4999 loss: 0.8164 time 24.74s
Final training  2793/4999 loss: 0.8182 time 24.81s
Final training  2794/4999 loss: 0.8319 time 24.55s
Final training  2795/4999 loss: 0.8333 time 24.74s
Final training  2796/4999 loss: 0.8074 time 24.41s
Final training  2797/4999 loss: 0.8304 time 24.64s
Final training  2798/4999 loss: 0.8048 time 24.67s
Final training  2799/4999 loss: 0.8315 time 25.08s
Dice accuracy for each class:  (tensor([0.9956, 0.9544, 0.9387, 0.9422, 0.7151, 0.7155, 0.9054, 0.7312, 0.9058,
        0.8473, 0.7304, 0.7515, 0.6135, 0.6708], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2799/4999 acc [ 0.817] time 183.31s
trigger times: 1
Final training  2800/4999 loss: 0.8034 time 25.14s
Final training  2801/4999 loss: 0.8228 time 25.07s
Final training  2802/4999 loss: 0.8136 time 24.99s
Final training  2803/4999 loss: 0.8192 time 25.08s
Final training  2804/4999 loss: 0.8126 time 24.94s
Final training  2805/4999 loss: 0.8247 time 24.92s
Final training  2806/4999 loss: 0.8273 time 24.84s
Final training  2807/4999 loss: 0.8349 time 24.78s
Final training  2808/4999 loss: 0.8412 time 24.37s
Final training  2809/4999 loss: 0.8294 time 24.54s
Final training  2810/4999 loss: 0.8323 time 24.86s
Final training  2811/4999 loss: 0.8229 time 24.85s
Final training  2812/4999 loss: 0.8064 time 24.78s
Final training  2813/4999 loss: 0.7744 time 24.71s
Final training  2814/4999 loss: 0.8275 time 24.46s
Final training  2815/4999 loss: 0.8142 time 24.98s
Final training  2816/4999 loss: 0.8112 time 25.28s
Final training  2817/4999 loss: 0.8187 time 24.79s
Final training  2818/4999 loss: 0.8024 time 24.72s
Final training  2819/4999 loss: 0.8157 time 24.61s
Final training  2820/4999 loss: 0.8185 time 24.78s
Final training  2821/4999 loss: 0.8320 time 24.26s
Final training  2822/4999 loss: 0.8152 time 24.55s
Final training  2823/4999 loss: 0.7991 time 24.93s
Final training  2824/4999 loss: 0.8206 time 24.58s
Final training  2825/4999 loss: 0.8178 time 24.71s
Final training  2826/4999 loss: 0.8053 time 24.90s
Final training  2827/4999 loss: 0.7994 time 24.85s
Final training  2828/4999 loss: 0.8196 time 24.81s
Final training  2829/4999 loss: 0.8173 time 24.80s
Final training  2830/4999 loss: 0.8319 time 24.99s
Final training  2831/4999 loss: 0.8032 time 24.61s
Final training  2832/4999 loss: 0.8151 time 24.70s
Final training  2833/4999 loss: 0.7918 time 24.57s
Final training  2834/4999 loss: 0.8272 time 24.70s
Final training  2835/4999 loss: 0.8347 time 24.57s
Final training  2836/4999 loss: 0.8027 time 24.52s
Final training  2837/4999 loss: 0.7964 time 24.54s
Final training  2838/4999 loss: 0.8120 time 24.49s
Final training  2839/4999 loss: 0.8222 time 24.74s
Final training  2840/4999 loss: 0.7880 time 24.77s
Final training  2841/4999 loss: 0.7997 time 24.77s
Final training  2842/4999 loss: 0.7912 time 24.55s
Final training  2843/4999 loss: 0.7992 time 24.20s
Final training  2844/4999 loss: 0.8077 time 24.40s
Final training  2845/4999 loss: 0.7994 time 24.45s
Final training  2846/4999 loss: 0.8283 time 24.47s
Final training  2847/4999 loss: 0.8387 time 24.48s
Final training  2848/4999 loss: 0.8113 time 24.56s
Final training  2849/4999 loss: 0.8103 time 24.48s
Final training  2850/4999 loss: 0.8004 time 24.52s
Final training  2851/4999 loss: 0.8120 time 24.48s
Final training  2852/4999 loss: 0.7849 time 24.67s
Final training  2853/4999 loss: 0.8087 time 24.69s
Final training  2854/4999 loss: 0.8084 time 24.49s
Final training  2855/4999 loss: 0.8100 time 24.66s
Final training  2856/4999 loss: 0.8046 time 24.55s
Final training  2857/4999 loss: 0.7934 time 24.40s
Final training  2858/4999 loss: 0.7995 time 24.81s
Final training  2859/4999 loss: 0.8177 time 24.94s
Final training  2860/4999 loss: 0.7854 time 24.97s
Final training  2861/4999 loss: 0.8143 time 24.89s
Final training  2862/4999 loss: 0.8247 time 25.27s
Final training  2863/4999 loss: 0.8150 time 25.11s
Final training  2864/4999 loss: 0.8181 time 25.28s
Final training  2865/4999 loss: 0.7957 time 24.89s
Final training  2866/4999 loss: 0.8110 time 24.55s
Final training  2867/4999 loss: 0.8016 time 24.44s
Final training  2868/4999 loss: 0.7972 time 24.60s
Final training  2869/4999 loss: 0.8134 time 24.67s
Final training  2870/4999 loss: 0.8067 time 24.69s
Final training  2871/4999 loss: 0.7981 time 24.50s
Final training  2872/4999 loss: 0.7899 time 24.54s
Final training  2873/4999 loss: 0.8073 time 24.52s
Final training  2874/4999 loss: 0.7802 time 24.75s
Final training  2875/4999 loss: 0.8089 time 24.51s
Final training  2876/4999 loss: 0.8135 time 24.54s
Final training  2877/4999 loss: 0.8175 time 24.67s
Final training  2878/4999 loss: 0.8148 time 24.31s
Final training  2879/4999 loss: 0.7808 time 24.42s
Final training  2880/4999 loss: 0.7715 time 24.79s
Final training  2881/4999 loss: 0.8052 time 24.57s
Final training  2882/4999 loss: 0.8093 time 24.65s
Final training  2883/4999 loss: 0.7957 time 24.84s
Final training  2884/4999 loss: 0.8059 time 24.43s
Final training  2885/4999 loss: 0.7881 time 24.62s
Final training  2886/4999 loss: 0.7941 time 24.76s
Final training  2887/4999 loss: 0.7978 time 24.40s
Final training  2888/4999 loss: 0.7893 time 24.52s
Final training  2889/4999 loss: 0.8137 time 24.58s
Final training  2890/4999 loss: 0.8111 time 24.80s
Final training  2891/4999 loss: 0.8054 time 24.94s
Final training  2892/4999 loss: 0.8056 time 24.85s
Final training  2893/4999 loss: 0.8014 time 24.98s
Final training  2894/4999 loss: 0.8189 time 25.22s
Final training  2895/4999 loss: 0.8296 time 24.75s
Final training  2896/4999 loss: 0.8166 time 24.84s
Final training  2897/4999 loss: 0.8101 time 24.47s
Final training  2898/4999 loss: 0.8040 time 24.55s
Final training  2899/4999 loss: 0.8141 time 24.83s
Dice accuracy for each class:  (tensor([0.9961, 0.9550, 0.9406, 0.9434, 0.7681, 0.7047, 0.9210, 0.7814, 0.9034,
        0.8557, 0.7441, 0.7984, 0.6455, 0.6590], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2899/4999 acc [ 0.831] time 183.58s
Reset trigger time to 0
new best (0.827917 --> 0.830517). 
Saving checkpoint /data/vision_group/medical/btcv/results_gsm/0model_lastsaved_fold0.pt
Copying to model.pt new best model!!!!
Final training  2900/4999 loss: 0.8137 time 24.58s
Final training  2901/4999 loss: 0.8083 time 24.60s
Final training  2902/4999 loss: 0.7771 time 24.79s
Final training  2903/4999 loss: 0.7869 time 24.72s
Final training  2904/4999 loss: 0.7988 time 24.58s
Final training  2905/4999 loss: 0.8051 time 24.39s
Final training  2906/4999 loss: 0.7838 time 24.47s
Final training  2907/4999 loss: 0.7619 time 24.31s
Final training  2908/4999 loss: 0.8069 time 24.59s
Final training  2909/4999 loss: 0.7861 time 24.76s
Final training  2910/4999 loss: 0.7703 time 24.55s
Final training  2911/4999 loss: 0.8049 time 24.53s
Final training  2912/4999 loss: 0.7858 time 24.50s
Final training  2913/4999 loss: 0.7967 time 24.50s
Final training  2914/4999 loss: 0.7981 time 24.73s
Final training  2915/4999 loss: 0.8076 time 24.80s
Final training  2916/4999 loss: 0.7909 time 24.50s
Final training  2917/4999 loss: 0.8003 time 24.40s
Final training  2918/4999 loss: 0.7888 time 24.87s
Final training  2919/4999 loss: 0.8134 time 24.80s
Final training  2920/4999 loss: 0.7950 time 24.74s
Final training  2921/4999 loss: 0.8224 time 24.83s
Final training  2922/4999 loss: 0.8129 time 25.00s
Final training  2923/4999 loss: 0.7990 time 24.94s
Final training  2924/4999 loss: 0.7900 time 24.89s
Final training  2925/4999 loss: 0.8149 time 25.16s
Final training  2926/4999 loss: 0.8172 time 25.22s
Final training  2927/4999 loss: 0.8030 time 24.99s
Final training  2928/4999 loss: 0.8122 time 25.16s
Final training  2929/4999 loss: 0.8373 time 24.52s
Final training  2930/4999 loss: 0.8170 time 25.04s
Final training  2931/4999 loss: 0.8161 time 24.52s
Final training  2932/4999 loss: 0.8195 time 25.04s
Final training  2933/4999 loss: 0.8132 time 24.49s
Final training  2934/4999 loss: 0.8278 time 24.97s
Final training  2935/4999 loss: 0.7860 time 24.57s
Final training  2936/4999 loss: 0.8086 time 25.07s
Final training  2937/4999 loss: 0.7925 time 25.31s
Final training  2938/4999 loss: 0.8058 time 25.47s
Final training  2939/4999 loss: 0.7947 time 25.53s
Final training  2940/4999 loss: 0.7886 time 25.10s
Final training  2941/4999 loss: 0.7962 time 24.68s
Final training  2942/4999 loss: 0.7800 time 24.44s
Final training  2943/4999 loss: 0.8034 time 24.54s
Final training  2944/4999 loss: 0.8182 time 24.58s
Final training  2945/4999 loss: 0.7943 time 24.17s
Final training  2946/4999 loss: 0.7923 time 24.64s
Final training  2947/4999 loss: 0.8053 time 24.76s
Final training  2948/4999 loss: 0.8171 time 24.49s
Final training  2949/4999 loss: 0.8110 time 24.76s
Final training  2950/4999 loss: 0.8225 time 24.74s
Final training  2951/4999 loss: 0.8055 time 24.91s
Final training  2952/4999 loss: 0.7799 time 24.15s
Final training  2953/4999 loss: 0.8306 time 24.88s
Final training  2954/4999 loss: 0.8262 time 24.97s
Final training  2955/4999 loss: 0.8105 time 24.90s
Final training  2956/4999 loss: 0.7935 time 24.72s
Final training  2957/4999 loss: 0.7833 time 24.60s
Final training  2958/4999 loss: 0.8197 time 24.47s
Final training  2959/4999 loss: 0.7840 time 24.60s
Final training  2960/4999 loss: 0.8024 time 24.64s
Final training  2961/4999 loss: 0.7915 time 24.63s
Final training  2962/4999 loss: 0.8068 time 24.55s
Final training  2963/4999 loss: 0.8016 time 24.62s
Final training  2964/4999 loss: 0.8013 time 24.59s
Final training  2965/4999 loss: 0.7940 time 24.60s
Final training  2966/4999 loss: 0.7743 time 25.03s
Final training  2967/4999 loss: 0.7749 time 25.18s
Final training  2968/4999 loss: 0.8128 time 24.94s
Final training  2969/4999 loss: 0.8006 time 24.81s
Final training  2970/4999 loss: 0.8109 time 24.82s
Final training  2971/4999 loss: 0.8061 time 24.98s
Final training  2972/4999 loss: 0.7730 time 24.63s
Final training  2973/4999 loss: 0.8046 time 24.62s
Final training  2974/4999 loss: 0.7623 time 24.55s
Final training  2975/4999 loss: 0.7815 time 24.69s
Final training  2976/4999 loss: 0.8100 time 24.49s
Final training  2977/4999 loss: 0.8005 time 24.54s
Final training  2978/4999 loss: 0.7927 time 24.57s
Final training  2979/4999 loss: 0.8047 time 24.80s
Final training  2980/4999 loss: 0.8105 time 24.54s
Final training  2981/4999 loss: 0.7755 time 24.48s
Final training  2982/4999 loss: 0.8058 time 24.55s
Final training  2983/4999 loss: 0.7856 time 24.36s
Final training  2984/4999 loss: 0.8059 time 24.58s
Final training  2985/4999 loss: 0.7904 time 24.53s
Final training  2986/4999 loss: 0.7827 time 24.46s
Final training  2987/4999 loss: 0.8039 time 24.42s
Final training  2988/4999 loss: 0.7938 time 24.44s
Final training  2989/4999 loss: 0.8077 time 24.50s
Final training  2990/4999 loss: 0.8030 time 24.64s
Final training  2991/4999 loss: 0.8095 time 25.12s
Final training  2992/4999 loss: 0.8043 time 24.97s
Final training  2993/4999 loss: 0.7964 time 25.16s
Final training  2994/4999 loss: 0.8141 time 24.86s
Final training  2995/4999 loss: 0.8016 time 24.91s
Final training  2996/4999 loss: 0.8067 time 24.78s
Final training  2997/4999 loss: 0.8152 time 24.79s
Final training  2998/4999 loss: 0.8234 time 24.70s
Final training  2999/4999 loss: 0.7973 time 24.82s
Dice accuracy for each class:  (tensor([0.9955, 0.9545, 0.9421, 0.9432, 0.7091, 0.7376, 0.8971, 0.7679, 0.9041,
        0.8528, 0.7475, 0.7643, 0.6529, 0.6556], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  2999/4999 acc [ 0.825] time 183.29s
trigger times: 1
Final training  3000/4999 loss: 0.7986 time 24.73s
Final training  3001/4999 loss: 0.7990 time 24.60s
Final training  3002/4999 loss: 0.8243 time 24.70s
Final training  3003/4999 loss: 0.7926 time 24.90s
Final training  3004/4999 loss: 0.7880 time 24.86s
Final training  3005/4999 loss: 0.7890 time 24.53s
Final training  3006/4999 loss: 0.7773 time 24.67s
Final training  3007/4999 loss: 0.8042 time 24.37s
Final training  3008/4999 loss: 0.8237 time 24.73s
Final training  3009/4999 loss: 0.7864 time 24.54s
Final training  3010/4999 loss: 0.7957 time 24.56s
Final training  3011/4999 loss: 0.8023 time 24.70s
Final training  3012/4999 loss: 0.8104 time 24.53s
Final training  3013/4999 loss: 0.7987 time 24.62s
Final training  3014/4999 loss: 0.7792 time 24.35s
Final training  3015/4999 loss: 0.7896 time 24.58s
Final training  3016/4999 loss: 0.8141 time 24.68s
Final training  3017/4999 loss: 0.7990 time 24.50s
Final training  3018/4999 loss: 0.8086 time 24.58s
Final training  3019/4999 loss: 0.7933 time 24.87s
Final training  3020/4999 loss: 0.7931 time 24.52s
Final training  3021/4999 loss: 0.7981 time 24.67s
Final training  3022/4999 loss: 0.8068 time 24.51s
Final training  3023/4999 loss: 0.8101 time 24.62s
Final training  3024/4999 loss: 0.8152 time 24.56s
Final training  3025/4999 loss: 0.8058 time 24.26s
Final training  3026/4999 loss: 0.8074 time 24.69s
Final training  3027/4999 loss: 0.8111 time 24.43s
Final training  3028/4999 loss: 0.7828 time 24.52s
Final training  3029/4999 loss: 0.8059 time 24.58s
Final training  3030/4999 loss: 0.8087 time 24.78s
Final training  3031/4999 loss: 0.8090 time 24.76s
Final training  3032/4999 loss: 0.8091 time 24.63s
Final training  3033/4999 loss: 0.8002 time 24.57s
Final training  3034/4999 loss: 0.8005 time 24.55s
Final training  3035/4999 loss: 0.7907 time 24.65s
Final training  3036/4999 loss: 0.7974 time 24.57s
Final training  3037/4999 loss: 0.8068 time 24.46s
Final training  3038/4999 loss: 0.8199 time 24.72s
Final training  3039/4999 loss: 0.8041 time 24.03s
Final training  3040/4999 loss: 0.8388 time 24.35s
Final training  3041/4999 loss: 0.8028 time 24.57s
Final training  3042/4999 loss: 0.8202 time 24.54s
Final training  3043/4999 loss: 0.8273 time 24.70s
Final training  3044/4999 loss: 0.7839 time 24.55s
Final training  3045/4999 loss: 0.8142 time 24.62s
Final training  3046/4999 loss: 0.8029 time 24.85s
Final training  3047/4999 loss: 0.8134 time 25.06s
Final training  3048/4999 loss: 0.8095 time 24.91s
Final training  3049/4999 loss: 0.8280 time 25.28s
Final training  3050/4999 loss: 0.8151 time 24.87s
Final training  3051/4999 loss: 0.8033 time 24.86s
Final training  3052/4999 loss: 0.7934 time 24.72s
Final training  3053/4999 loss: 0.8199 time 24.79s
Final training  3054/4999 loss: 0.8181 time 24.65s
Final training  3055/4999 loss: 0.7861 time 24.51s
Final training  3056/4999 loss: 0.8248 time 24.83s
Final training  3057/4999 loss: 0.7795 time 24.97s
Final training  3058/4999 loss: 0.7952 time 24.94s
Final training  3059/4999 loss: 0.8118 time 24.81s
Final training  3060/4999 loss: 0.8230 time 24.75s
Final training  3061/4999 loss: 0.8002 time 24.89s
Final training  3062/4999 loss: 0.7916 time 24.75s
Final training  3063/4999 loss: 0.8006 time 24.79s
Final training  3064/4999 loss: 0.8155 time 24.65s
Final training  3065/4999 loss: 0.7902 time 24.54s
Final training  3066/4999 loss: 0.7959 time 24.41s
Final training  3067/4999 loss: 0.7913 time 24.75s
Final training  3068/4999 loss: 0.8049 time 24.62s
Final training  3069/4999 loss: 0.7741 time 24.66s
Final training  3070/4999 loss: 0.7930 time 24.72s
Final training  3071/4999 loss: 0.7961 time 24.61s
Final training  3072/4999 loss: 0.7899 time 24.60s
Final training  3073/4999 loss: 0.8145 time 24.75s
Final training  3074/4999 loss: 0.7970 time 24.70s
Final training  3075/4999 loss: 0.8145 time 24.60s
Final training  3076/4999 loss: 0.7949 time 24.28s
Final training  3077/4999 loss: 0.7966 time 24.72s
Final training  3078/4999 loss: 0.8135 time 24.66s
Final training  3079/4999 loss: 0.7666 time 24.89s
Final training  3080/4999 loss: 0.7884 time 24.46s
Final training  3081/4999 loss: 0.7907 time 24.68s
Final training  3082/4999 loss: 0.7854 time 24.61s
Final training  3083/4999 loss: 0.7831 time 24.73s
Final training  3084/4999 loss: 0.7846 time 24.66s
Final training  3085/4999 loss: 0.8117 time 24.63s
Final training  3086/4999 loss: 0.8102 time 24.50s
Final training  3087/4999 loss: 0.8051 time 24.65s
Final training  3088/4999 loss: 0.8036 time 24.50s
Final training  3089/4999 loss: 0.8226 time 24.55s
Final training  3090/4999 loss: 0.7964 time 24.68s
Final training  3091/4999 loss: 0.7918 time 24.60s
Final training  3092/4999 loss: 0.8080 time 24.41s
Final training  3093/4999 loss: 0.7875 time 24.53s
Final training  3094/4999 loss: 0.8345 time 24.70s
Final training  3095/4999 loss: 0.8009 time 24.72s
Final training  3096/4999 loss: 0.7850 time 24.54s
Final training  3097/4999 loss: 0.8043 time 24.90s
Final training  3098/4999 loss: 0.7929 time 24.62s
Final training  3099/4999 loss: 0.8190 time 24.60s
Dice accuracy for each class:  (tensor([0.9951, 0.9482, 0.9378, 0.9384, 0.7245, 0.7133, 0.8793, 0.8083, 0.9048,
        0.8445, 0.7548, 0.7848, 0.6462, 0.6585], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3099/4999 acc [ 0.825] time 183.40s
trigger times: 2
Final training  3100/4999 loss: 0.7680 time 24.79s
Final training  3101/4999 loss: 0.8263 time 24.68s
Final training  3102/4999 loss: 0.7854 time 24.68s
Final training  3103/4999 loss: 0.8176 time 24.79s
Final training  3104/4999 loss: 0.7890 time 24.73s
Final training  3105/4999 loss: 0.8078 time 24.52s
Final training  3106/4999 loss: 0.8062 time 24.61s
Final training  3107/4999 loss: 0.7892 time 24.38s
Final training  3108/4999 loss: 0.7986 time 24.46s
Final training  3109/4999 loss: 0.7542 time 24.60s
Final training  3110/4999 loss: 0.7984 time 24.83s
Final training  3111/4999 loss: 0.8166 time 24.66s
Final training  3112/4999 loss: 0.7893 time 24.81s
Final training  3113/4999 loss: 0.7886 time 24.12s
Final training  3114/4999 loss: 0.8089 time 24.67s
Final training  3115/4999 loss: 0.7903 time 24.69s
Final training  3116/4999 loss: 0.8093 time 24.64s
Final training  3117/4999 loss: 0.7945 time 24.58s
Final training  3118/4999 loss: 0.7766 time 24.74s
Final training  3119/4999 loss: 0.7606 time 24.40s
Final training  3120/4999 loss: 0.7923 time 24.54s
Final training  3121/4999 loss: 0.7967 time 24.69s
Final training  3122/4999 loss: 0.7829 time 25.01s
Final training  3123/4999 loss: 0.8152 time 24.97s
Final training  3124/4999 loss: 0.7905 time 25.04s
Final training  3125/4999 loss: 0.8286 time 24.67s
Final training  3126/4999 loss: 0.8081 time 24.53s
Final training  3127/4999 loss: 0.8055 time 23.96s
Final training  3128/4999 loss: 0.7894 time 24.76s
Final training  3129/4999 loss: 0.8072 time 24.67s
Final training  3130/4999 loss: 0.7895 time 24.85s
Final training  3131/4999 loss: 0.8382 time 24.75s
Final training  3132/4999 loss: 0.7917 time 24.66s
Final training  3133/4999 loss: 0.8260 time 24.45s
Final training  3134/4999 loss: 0.8285 time 24.54s
Final training  3135/4999 loss: 0.8063 time 24.60s
Final training  3136/4999 loss: 0.7989 time 24.87s
Final training  3137/4999 loss: 0.7792 time 24.90s
Final training  3138/4999 loss: 0.8035 time 24.80s
Final training  3139/4999 loss: 0.7840 time 24.38s
Final training  3140/4999 loss: 0.8212 time 24.59s
Final training  3141/4999 loss: 0.7916 time 24.55s
Final training  3142/4999 loss: 0.8139 time 24.33s
Final training  3143/4999 loss: 0.8026 time 24.60s
Final training  3144/4999 loss: 0.8096 time 24.58s
Final training  3145/4999 loss: 0.7980 time 24.94s
Final training  3146/4999 loss: 0.7996 time 25.07s
Final training  3147/4999 loss: 0.7881 time 24.66s
Final training  3148/4999 loss: 0.8043 time 24.64s
Final training  3149/4999 loss: 0.7843 time 24.75s
Final training  3150/4999 loss: 0.8136 time 25.06s
Final training  3151/4999 loss: 0.8065 time 24.58s
Final training  3152/4999 loss: 0.7919 time 24.79s
Final training  3153/4999 loss: 0.8075 time 24.61s
Final training  3154/4999 loss: 0.8006 time 24.93s
Final training  3155/4999 loss: 0.8029 time 24.60s
Final training  3156/4999 loss: 0.7896 time 24.65s
Final training  3157/4999 loss: 0.8050 time 24.66s
Final training  3158/4999 loss: 0.7843 time 24.77s
Final training  3159/4999 loss: 0.7788 time 24.60s
Final training  3160/4999 loss: 0.8056 time 24.63s
Final training  3161/4999 loss: 0.7900 time 24.54s
Final training  3162/4999 loss: 0.7892 time 24.57s
Final training  3163/4999 loss: 0.7788 time 24.58s
Final training  3164/4999 loss: 0.7998 time 24.78s
Final training  3165/4999 loss: 0.7903 time 24.67s
Final training  3166/4999 loss: 0.7761 time 24.54s
Final training  3167/4999 loss: 0.7910 time 24.64s
Final training  3168/4999 loss: 0.7912 time 24.45s
Final training  3169/4999 loss: 0.8093 time 24.44s
Final training  3170/4999 loss: 0.8061 time 24.72s
Final training  3171/4999 loss: 0.7967 time 24.80s
Final training  3172/4999 loss: 0.8170 time 24.63s
Final training  3173/4999 loss: 0.7932 time 24.55s
Final training  3174/4999 loss: 0.8057 time 24.36s
Final training  3175/4999 loss: 0.8135 time 24.44s
Final training  3176/4999 loss: 0.8169 time 24.45s
Final training  3177/4999 loss: 0.7908 time 24.73s
Final training  3178/4999 loss: 0.7879 time 24.65s
Final training  3179/4999 loss: 0.8057 time 24.94s
Final training  3180/4999 loss: 0.7987 time 24.62s
Final training  3181/4999 loss: 0.8006 time 24.78s
Final training  3182/4999 loss: 0.8381 time 25.02s
Final training  3183/4999 loss: 0.8077 time 24.87s
Final training  3184/4999 loss: 0.7758 time 24.38s
Final training  3185/4999 loss: 0.8036 time 24.54s
Final training  3186/4999 loss: 0.8050 time 24.59s
Final training  3187/4999 loss: 0.7851 time 24.31s
Final training  3188/4999 loss: 0.8199 time 24.66s
Final training  3189/4999 loss: 0.7931 time 24.67s
Final training  3190/4999 loss: 0.7995 time 24.58s
Final training  3191/4999 loss: 0.8042 time 24.83s
Final training  3192/4999 loss: 0.8176 time 24.64s
Final training  3193/4999 loss: 0.8003 time 24.88s
Final training  3194/4999 loss: 0.7923 time 24.48s
Final training  3195/4999 loss: 0.7781 time 24.58s
Final training  3196/4999 loss: 0.7927 time 24.68s
Final training  3197/4999 loss: 0.7978 time 24.47s
Final training  3198/4999 loss: 0.7860 time 24.67s
Final training  3199/4999 loss: 0.8107 time 24.98s
Dice accuracy for each class:  (tensor([0.9955, 0.9536, 0.9427, 0.9428, 0.7468, 0.7185, 0.8951, 0.7943, 0.9041,
        0.8568, 0.7468, 0.7706, 0.6487, 0.6343], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3199/4999 acc [ 0.826] time 183.45s
trigger times: 3
Final training  3200/4999 loss: 0.7986 time 24.40s
Final training  3201/4999 loss: 0.8031 time 24.61s
Final training  3202/4999 loss: 0.7944 time 24.55s
Final training  3203/4999 loss: 0.7913 time 24.60s
Final training  3204/4999 loss: 0.8005 time 24.51s
Final training  3205/4999 loss: 0.7844 time 24.64s
Final training  3206/4999 loss: 0.7988 time 24.63s
Final training  3207/4999 loss: 0.8160 time 24.76s
Final training  3208/4999 loss: 0.7883 time 24.61s
Final training  3209/4999 loss: 0.8355 time 24.52s
Final training  3210/4999 loss: 0.8245 time 24.54s
Final training  3211/4999 loss: 0.8161 time 24.17s
Final training  3212/4999 loss: 0.8015 time 24.48s
Final training  3213/4999 loss: 0.7854 time 24.71s
Final training  3214/4999 loss: 0.7934 time 24.94s
Final training  3215/4999 loss: 0.8174 time 24.74s
Final training  3216/4999 loss: 0.8198 time 24.66s
Final training  3217/4999 loss: 0.7979 time 24.68s
Final training  3218/4999 loss: 0.8195 time 24.05s
Final training  3219/4999 loss: 0.7715 time 24.73s
Final training  3220/4999 loss: 0.8007 time 24.82s
Final training  3221/4999 loss: 0.8065 time 24.73s
Final training  3222/4999 loss: 0.7952 time 24.52s
Final training  3223/4999 loss: 0.7841 time 24.57s
Final training  3224/4999 loss: 0.7821 time 24.80s
Final training  3225/4999 loss: 0.8173 time 24.61s
Final training  3226/4999 loss: 0.7793 time 24.41s
Final training  3227/4999 loss: 0.8103 time 24.63s
Final training  3228/4999 loss: 0.8084 time 25.17s
Final training  3229/4999 loss: 0.8035 time 25.33s
Final training  3230/4999 loss: 0.7887 time 25.26s
Final training  3231/4999 loss: 0.8428 time 25.02s
Final training  3232/4999 loss: 0.8018 time 24.85s
Final training  3233/4999 loss: 0.8128 time 24.98s
Final training  3234/4999 loss: 0.7742 time 24.96s
Final training  3235/4999 loss: 0.7789 time 25.03s
Final training  3236/4999 loss: 0.7960 time 25.19s
Final training  3237/4999 loss: 0.7842 time 25.14s
Final training  3238/4999 loss: 0.8003 time 25.28s
Final training  3239/4999 loss: 0.7910 time 25.16s
Final training  3240/4999 loss: 0.7761 time 24.82s
Final training  3241/4999 loss: 0.7837 time 25.07s
Final training  3242/4999 loss: 0.7867 time 24.76s
Final training  3243/4999 loss: 0.7959 time 25.02s
Final training  3244/4999 loss: 0.7683 time 25.14s
Final training  3245/4999 loss: 0.7987 time 25.03s
Final training  3246/4999 loss: 0.7975 time 24.76s
Final training  3247/4999 loss: 0.8141 time 24.96s
Final training  3248/4999 loss: 0.7996 time 24.83s
Final training  3249/4999 loss: 0.7876 time 24.61s
Final training  3250/4999 loss: 0.8130 time 24.74s
Final training  3251/4999 loss: 0.7881 time 24.66s
Final training  3252/4999 loss: 0.7824 time 24.62s
Final training  3253/4999 loss: 0.7872 time 24.68s
Final training  3254/4999 loss: 0.7988 time 24.77s
Final training  3255/4999 loss: 0.7978 time 24.81s
Final training  3256/4999 loss: 0.7874 time 24.52s
Final training  3257/4999 loss: 0.8050 time 24.74s
Final training  3258/4999 loss: 0.8118 time 24.60s
Final training  3259/4999 loss: 0.7894 time 24.66s
Final training  3260/4999 loss: 0.7897 time 24.85s
Final training  3261/4999 loss: 0.8258 time 24.96s
Final training  3262/4999 loss: 0.7952 time 25.27s
Final training  3263/4999 loss: 0.8017 time 25.28s
Final training  3264/4999 loss: 0.7844 time 24.88s
Final training  3265/4999 loss: 0.7862 time 25.28s
Final training  3266/4999 loss: 0.8088 time 24.67s
Final training  3267/4999 loss: 0.8071 time 25.07s
Final training  3268/4999 loss: 0.7898 time 24.40s
Final training  3269/4999 loss: 0.8101 time 24.52s
Final training  3270/4999 loss: 0.8067 time 24.96s
Final training  3271/4999 loss: 0.7953 time 24.63s
Final training  3272/4999 loss: 0.7992 time 24.55s
Final training  3273/4999 loss: 0.7972 time 24.82s
Final training  3274/4999 loss: 0.7851 time 24.76s
Final training  3275/4999 loss: 0.8118 time 24.65s
Final training  3276/4999 loss: 0.7676 time 24.76s
Final training  3277/4999 loss: 0.8093 time 24.63s
Final training  3278/4999 loss: 0.8118 time 24.84s
Final training  3279/4999 loss: 0.7888 time 24.62s
Final training  3280/4999 loss: 0.8191 time 24.74s
Final training  3281/4999 loss: 0.8019 time 24.95s
Final training  3282/4999 loss: 0.8250 time 24.75s
Final training  3283/4999 loss: 0.7819 time 24.81s
Final training  3284/4999 loss: 0.7875 time 24.63s
Final training  3285/4999 loss: 0.7828 time 24.88s
Final training  3286/4999 loss: 0.7974 time 24.68s
Final training  3287/4999 loss: 0.7839 time 24.56s
Final training  3288/4999 loss: 0.8171 time 24.30s
Final training  3289/4999 loss: 0.7931 time 24.56s
Final training  3290/4999 loss: 0.7923 time 24.46s
Final training  3291/4999 loss: 0.7901 time 24.61s
Final training  3292/4999 loss: 0.8149 time 24.49s
Final training  3293/4999 loss: 0.7905 time 24.49s
Final training  3294/4999 loss: 0.7906 time 24.50s
Final training  3295/4999 loss: 0.7972 time 24.66s
Final training  3296/4999 loss: 0.8042 time 25.00s
Final training  3297/4999 loss: 0.8009 time 25.24s
Final training  3298/4999 loss: 0.8062 time 25.12s
Final training  3299/4999 loss: 0.7987 time 25.02s
Dice accuracy for each class:  (tensor([0.9957, 0.9473, 0.9424, 0.9441, 0.7423, 0.7086, 0.9039, 0.7878, 0.9023,
        0.8455, 0.7531, 0.7799, 0.6536, 0.6402], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3299/4999 acc [ 0.826] time 181.86s
trigger times: 4
Final training  3300/4999 loss: 0.8018 time 24.39s
Final training  3301/4999 loss: 0.8263 time 25.11s
Final training  3302/4999 loss: 0.7899 time 24.22s
Final training  3303/4999 loss: 0.7846 time 24.99s
Final training  3304/4999 loss: 0.8086 time 24.99s
Final training  3305/4999 loss: 0.7985 time 25.14s
Final training  3306/4999 loss: 0.8140 time 25.09s
Final training  3307/4999 loss: 0.7985 time 25.06s
Final training  3308/4999 loss: 0.7766 time 24.69s
Final training  3309/4999 loss: 0.7910 time 24.80s
Final training  3310/4999 loss: 0.7846 time 24.86s
Final training  3311/4999 loss: 0.7989 time 24.90s
Final training  3312/4999 loss: 0.7972 time 24.89s
Final training  3313/4999 loss: 0.7994 time 24.51s
Final training  3314/4999 loss: 0.8064 time 24.71s
Final training  3315/4999 loss: 0.8004 time 24.56s
Final training  3316/4999 loss: 0.7856 time 24.54s
Final training  3317/4999 loss: 0.8065 time 24.56s
Final training  3318/4999 loss: 0.7953 time 24.74s
Final training  3319/4999 loss: 0.8086 time 25.04s
Final training  3320/4999 loss: 0.7969 time 24.94s
Final training  3321/4999 loss: 0.7752 time 24.43s
Final training  3322/4999 loss: 0.7868 time 24.69s
Final training  3323/4999 loss: 0.7854 time 24.51s
Final training  3324/4999 loss: 0.7524 time 24.66s
Final training  3325/4999 loss: 0.8043 time 24.38s
Final training  3326/4999 loss: 0.7918 time 24.44s
Final training  3327/4999 loss: 0.8024 time 24.76s
Final training  3328/4999 loss: 0.7996 time 24.63s
Final training  3329/4999 loss: 0.8008 time 24.92s
Final training  3330/4999 loss: 0.7766 time 24.63s
Final training  3331/4999 loss: 0.7927 time 24.58s
Final training  3332/4999 loss: 0.7795 time 24.61s
Final training  3333/4999 loss: 0.8156 time 24.64s
Final training  3334/4999 loss: 0.7977 time 24.61s
Final training  3335/4999 loss: 0.7808 time 24.48s
Final training  3336/4999 loss: 0.8097 time 24.53s
Final training  3337/4999 loss: 0.8033 time 24.59s
Final training  3338/4999 loss: 0.7794 time 24.57s
Final training  3339/4999 loss: 0.7984 time 24.54s
Final training  3340/4999 loss: 0.8067 time 24.44s
Final training  3341/4999 loss: 0.7891 time 24.48s
Final training  3342/4999 loss: 0.7951 time 24.72s
Final training  3343/4999 loss: 0.8155 time 24.44s
Final training  3344/4999 loss: 0.7964 time 24.61s
Final training  3345/4999 loss: 0.7978 time 24.40s
Final training  3346/4999 loss: 0.7960 time 24.80s
Final training  3347/4999 loss: 0.7956 time 24.76s
Final training  3348/4999 loss: 0.8104 time 24.55s
Final training  3349/4999 loss: 0.7925 time 24.51s
Final training  3350/4999 loss: 0.8032 time 24.73s
Final training  3351/4999 loss: 0.7931 time 24.92s
Final training  3352/4999 loss: 0.8032 time 24.98s
Final training  3353/4999 loss: 0.8019 time 25.05s
Final training  3354/4999 loss: 0.8170 time 25.06s
Final training  3355/4999 loss: 0.7801 time 24.99s
Final training  3356/4999 loss: 0.8030 time 24.87s
Final training  3357/4999 loss: 0.7842 time 24.79s
Final training  3358/4999 loss: 0.7835 time 24.87s
Final training  3359/4999 loss: 0.8002 time 24.99s
Final training  3360/4999 loss: 0.7886 time 24.91s
Final training  3361/4999 loss: 0.7949 time 24.64s
Final training  3362/4999 loss: 0.7978 time 24.54s
Final training  3363/4999 loss: 0.7961 time 24.65s
Final training  3364/4999 loss: 0.7852 time 24.45s
Final training  3365/4999 loss: 0.7885 time 24.63s
Final training  3366/4999 loss: 0.8009 time 24.39s
Final training  3367/4999 loss: 0.7825 time 24.65s
Final training  3368/4999 loss: 0.7726 time 24.72s
Final training  3369/4999 loss: 0.8146 time 25.03s
Final training  3370/4999 loss: 0.7860 time 24.95s
Final training  3371/4999 loss: 0.7972 time 24.62s
Final training  3372/4999 loss: 0.7932 time 24.65s
Final training  3373/4999 loss: 0.7801 time 24.70s
Final training  3374/4999 loss: 0.7645 time 24.62s
Final training  3375/4999 loss: 0.7989 time 24.66s
Final training  3376/4999 loss: 0.8114 time 24.58s
Final training  3377/4999 loss: 0.8077 time 24.78s
Final training  3378/4999 loss: 0.8018 time 24.41s
Final training  3379/4999 loss: 0.7919 time 24.57s
Final training  3380/4999 loss: 0.7916 time 24.71s
Final training  3381/4999 loss: 0.8093 time 24.26s
Final training  3382/4999 loss: 0.7934 time 24.84s
Final training  3383/4999 loss: 0.8107 time 24.85s
Final training  3384/4999 loss: 0.7919 time 24.72s
Final training  3385/4999 loss: 0.7922 time 24.42s
Final training  3386/4999 loss: 0.8092 time 24.63s
Final training  3387/4999 loss: 0.8208 time 25.05s
Final training  3388/4999 loss: 0.7837 time 24.83s
Final training  3389/4999 loss: 0.7773 time 25.01s
Final training  3390/4999 loss: 0.7942 time 25.06s
Final training  3391/4999 loss: 0.8031 time 25.05s
Final training  3392/4999 loss: 0.7897 time 24.96s
Final training  3393/4999 loss: 0.7810 time 25.12s
Final training  3394/4999 loss: 0.8003 time 24.80s
Final training  3395/4999 loss: 0.8115 time 24.78s
Final training  3396/4999 loss: 0.7794 time 24.81s
Final training  3397/4999 loss: 0.7906 time 25.28s
Final training  3398/4999 loss: 0.8306 time 24.98s
Final training  3399/4999 loss: 0.7987 time 24.33s
Dice accuracy for each class:  (tensor([0.9955, 0.9554, 0.9429, 0.9435, 0.7453, 0.6907, 0.8949, 0.7910, 0.9049,
        0.8525, 0.7477, 0.7558, 0.6719, 0.6155], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3399/4999 acc [ 0.823] time 183.65s
trigger times: 5
Final training  3400/4999 loss: 0.7821 time 24.57s
Final training  3401/4999 loss: 0.8175 time 24.95s
Final training  3402/4999 loss: 0.7955 time 24.95s
Final training  3403/4999 loss: 0.8224 time 25.00s
Final training  3404/4999 loss: 0.8130 time 24.78s
Final training  3405/4999 loss: 0.8064 time 24.91s
Final training  3406/4999 loss: 0.7959 time 24.90s
Final training  3407/4999 loss: 0.7798 time 24.64s
Final training  3408/4999 loss: 0.7915 time 24.53s
Final training  3409/4999 loss: 0.8037 time 24.16s
Final training  3410/4999 loss: 0.7971 time 24.61s
Final training  3411/4999 loss: 0.8197 time 24.46s
Final training  3412/4999 loss: 0.7781 time 24.63s
Final training  3413/4999 loss: 0.8155 time 24.90s
Final training  3414/4999 loss: 0.7890 time 24.76s
Final training  3415/4999 loss: 0.8071 time 24.71s
Final training  3416/4999 loss: 0.7901 time 24.79s
Final training  3417/4999 loss: 0.8241 time 24.63s
Final training  3418/4999 loss: 0.8038 time 24.50s
Final training  3419/4999 loss: 0.7844 time 24.59s
Final training  3420/4999 loss: 0.7950 time 24.75s
Final training  3421/4999 loss: 0.7953 time 24.82s
Final training  3422/4999 loss: 0.8016 time 24.52s
Final training  3423/4999 loss: 0.7883 time 24.68s
Final training  3424/4999 loss: 0.7882 time 24.63s
Final training  3425/4999 loss: 0.8103 time 24.47s
Final training  3426/4999 loss: 0.7784 time 24.78s
Final training  3427/4999 loss: 0.8074 time 24.41s
Final training  3428/4999 loss: 0.7767 time 24.55s
Final training  3429/4999 loss: 0.7815 time 24.60s
Final training  3430/4999 loss: 0.7772 time 24.50s
Final training  3431/4999 loss: 0.8048 time 24.55s
Final training  3432/4999 loss: 0.7868 time 24.68s
Final training  3433/4999 loss: 0.7880 time 24.26s
Final training  3434/4999 loss: 0.8001 time 24.67s
Final training  3435/4999 loss: 0.7980 time 25.07s
Final training  3436/4999 loss: 0.7760 time 24.69s
Final training  3437/4999 loss: 0.7628 time 24.68s
Final training  3438/4999 loss: 0.7867 time 24.76s
Final training  3439/4999 loss: 0.8117 time 24.92s
Final training  3440/4999 loss: 0.8333 time 24.96s
Final training  3441/4999 loss: 0.8306 time 25.13s
Final training  3442/4999 loss: 0.7821 time 24.71s
Final training  3443/4999 loss: 0.8184 time 24.65s
Final training  3444/4999 loss: 0.7933 time 24.74s
Final training  3445/4999 loss: 0.8192 time 24.56s
Final training  3446/4999 loss: 0.8165 time 24.76s
Final training  3447/4999 loss: 0.8019 time 24.70s
Final training  3448/4999 loss: 0.7779 time 24.60s
Final training  3449/4999 loss: 0.7754 time 24.47s
Final training  3450/4999 loss: 0.7922 time 24.64s
Final training  3451/4999 loss: 0.7878 time 24.76s
Final training  3452/4999 loss: 0.7970 time 24.70s
Final training  3453/4999 loss: 0.7719 time 24.73s
Final training  3454/4999 loss: 0.8078 time 24.59s
Final training  3455/4999 loss: 0.7825 time 24.33s
Final training  3456/4999 loss: 0.7884 time 24.49s
Final training  3457/4999 loss: 0.8101 time 24.33s
Final training  3458/4999 loss: 0.7914 time 24.78s
Final training  3459/4999 loss: 0.7971 time 24.65s
Final training  3460/4999 loss: 0.7970 time 24.57s
Final training  3461/4999 loss: 0.7898 time 24.56s
Final training  3462/4999 loss: 0.7776 time 24.52s
Final training  3463/4999 loss: 0.8096 time 24.66s
Final training  3464/4999 loss: 0.8188 time 24.60s
Final training  3465/4999 loss: 0.7947 time 24.61s
Final training  3466/4999 loss: 0.7804 time 24.60s
Final training  3467/4999 loss: 0.8017 time 24.64s
Final training  3468/4999 loss: 0.7940 time 24.66s
Final training  3469/4999 loss: 0.8076 time 24.57s
Final training  3470/4999 loss: 0.8092 time 24.60s
Final training  3471/4999 loss: 0.7951 time 24.75s
Final training  3472/4999 loss: 0.7949 time 24.65s
Final training  3473/4999 loss: 0.8219 time 24.75s
Final training  3474/4999 loss: 0.7809 time 24.66s
Final training  3475/4999 loss: 0.8163 time 24.56s
Final training  3476/4999 loss: 0.8037 time 24.41s
Final training  3477/4999 loss: 0.8017 time 24.72s
Final training  3478/4999 loss: 0.8113 time 24.74s
Final training  3479/4999 loss: 0.7921 time 24.45s
Final training  3480/4999 loss: 0.7691 time 24.84s
Final training  3481/4999 loss: 0.7745 time 24.76s
Final training  3482/4999 loss: 0.7864 time 25.02s
Final training  3483/4999 loss: 0.7838 time 25.09s
Final training  3484/4999 loss: 0.8032 time 24.95s
Final training  3485/4999 loss: 0.8058 time 24.56s
Final training  3486/4999 loss: 0.7954 time 24.63s
Final training  3487/4999 loss: 0.8105 time 24.39s
Final training  3488/4999 loss: 0.7965 time 24.79s
Final training  3489/4999 loss: 0.7978 time 24.71s
Final training  3490/4999 loss: 0.7994 time 24.71s
Final training  3491/4999 loss: 0.7892 time 24.58s
Final training  3492/4999 loss: 0.7846 time 24.77s
Final training  3493/4999 loss: 0.7956 time 24.61s
Final training  3494/4999 loss: 0.7815 time 24.74s
Final training  3495/4999 loss: 0.8063 time 24.42s
Final training  3496/4999 loss: 0.7888 time 24.84s
Final training  3497/4999 loss: 0.7896 time 24.76s
Final training  3498/4999 loss: 0.8022 time 24.57s
Final training  3499/4999 loss: 0.7792 time 24.61s
Dice accuracy for each class:  (tensor([0.9950, 0.9566, 0.9421, 0.9444, 0.7395, 0.7144, 0.8779, 0.7951, 0.9050,
        0.8527, 0.7457, 0.7663, 0.6653, 0.6665], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3499/4999 acc [ 0.827] time 182.33s
trigger times: 6
Final training  3500/4999 loss: 0.7905 time 25.03s
Final training  3501/4999 loss: 0.7648 time 24.84s
Final training  3502/4999 loss: 0.7995 time 25.39s
Final training  3503/4999 loss: 0.7767 time 24.75s
Final training  3504/4999 loss: 0.8002 time 24.84s
Final training  3505/4999 loss: 0.8004 time 25.05s
Final training  3506/4999 loss: 0.7853 time 25.13s
Final training  3507/4999 loss: 0.7864 time 25.25s
Final training  3508/4999 loss: 0.8043 time 24.89s
Final training  3509/4999 loss: 0.7795 time 24.86s
Final training  3510/4999 loss: 0.7976 time 24.79s
Final training  3511/4999 loss: 0.8029 time 24.79s
Final training  3512/4999 loss: 0.8009 time 24.65s
Final training  3513/4999 loss: 0.7813 time 24.86s
Final training  3514/4999 loss: 0.7781 time 24.49s
Final training  3515/4999 loss: 0.7952 time 24.77s
Final training  3516/4999 loss: 0.8024 time 24.45s
Final training  3517/4999 loss: 0.7745 time 24.39s
Final training  3518/4999 loss: 0.7946 time 24.50s
Final training  3519/4999 loss: 0.8059 time 24.61s
Final training  3520/4999 loss: 0.8128 time 24.66s
Final training  3521/4999 loss: 0.7852 time 24.70s
Final training  3522/4999 loss: 0.7853 time 24.72s
Final training  3523/4999 loss: 0.7984 time 24.65s
Final training  3524/4999 loss: 0.8097 time 24.81s
Final training  3525/4999 loss: 0.7798 time 24.71s
Final training  3526/4999 loss: 0.7940 time 24.45s
Final training  3527/4999 loss: 0.7874 time 24.62s
Final training  3528/4999 loss: 0.8028 time 24.67s
Final training  3529/4999 loss: 0.7840 time 24.54s
Final training  3530/4999 loss: 0.7929 time 24.49s
Final training  3531/4999 loss: 0.8034 time 24.41s
Final training  3532/4999 loss: 0.7821 time 24.60s
Final training  3533/4999 loss: 0.7820 time 24.65s
Final training  3534/4999 loss: 0.7900 time 24.50s
Final training  3535/4999 loss: 0.7999 time 24.44s
Final training  3536/4999 loss: 0.8099 time 24.71s
Final training  3537/4999 loss: 0.8035 time 24.54s
Final training  3538/4999 loss: 0.8003 time 24.67s
Final training  3539/4999 loss: 0.7973 time 24.56s
Final training  3540/4999 loss: 0.7720 time 25.05s
Final training  3541/4999 loss: 0.7837 time 24.95s
Final training  3542/4999 loss: 0.7902 time 25.06s
Final training  3543/4999 loss: 0.7957 time 24.71s
Final training  3544/4999 loss: 0.7656 time 24.62s
Final training  3545/4999 loss: 0.8050 time 24.55s
Final training  3546/4999 loss: 0.8013 time 24.77s
Final training  3547/4999 loss: 0.7941 time 24.52s
Final training  3548/4999 loss: 0.8065 time 24.98s
Final training  3549/4999 loss: 0.7828 time 24.82s
Final training  3550/4999 loss: 0.7992 time 24.78s
Final training  3551/4999 loss: 0.7751 time 24.83s
Final training  3552/4999 loss: 0.7945 time 24.46s
Final training  3553/4999 loss: 0.7846 time 24.79s
Final training  3554/4999 loss: 0.7886 time 24.52s
Final training  3555/4999 loss: 0.8076 time 24.57s
Final training  3556/4999 loss: 0.7662 time 24.52s
Final training  3557/4999 loss: 0.7823 time 24.50s
Final training  3558/4999 loss: 0.8027 time 24.44s
Final training  3559/4999 loss: 0.7755 time 24.39s
Final training  3560/4999 loss: 0.8010 time 24.49s
Final training  3561/4999 loss: 0.8001 time 24.31s
Final training  3562/4999 loss: 0.7918 time 24.76s
Final training  3563/4999 loss: 0.7846 time 24.41s
Final training  3564/4999 loss: 0.8031 time 24.68s
Final training  3565/4999 loss: 0.7937 time 24.62s
Final training  3566/4999 loss: 0.8140 time 24.81s
Final training  3567/4999 loss: 0.7746 time 24.84s
Final training  3568/4999 loss: 0.7815 time 24.69s
Final training  3569/4999 loss: 0.7871 time 24.65s
Final training  3570/4999 loss: 0.7905 time 24.64s
Final training  3571/4999 loss: 0.7773 time 24.49s
Final training  3572/4999 loss: 0.7804 time 24.67s
Final training  3573/4999 loss: 0.7904 time 24.78s
Final training  3574/4999 loss: 0.7970 time 24.66s
Final training  3575/4999 loss: 0.7939 time 24.75s
Final training  3576/4999 loss: 0.7670 time 24.75s
Final training  3577/4999 loss: 0.7808 time 24.74s
Final training  3578/4999 loss: 0.8121 time 24.86s
Final training  3579/4999 loss: 0.7704 time 24.75s
Final training  3580/4999 loss: 0.7939 time 24.60s
Final training  3581/4999 loss: 0.7744 time 24.62s
Final training  3582/4999 loss: 0.7854 time 24.50s
Final training  3583/4999 loss: 0.8242 time 24.63s
Final training  3584/4999 loss: 0.7459 time 24.88s
Final training  3585/4999 loss: 0.8116 time 24.84s
Final training  3586/4999 loss: 0.7515 time 24.67s
Final training  3587/4999 loss: 0.7860 time 24.57s
Final training  3588/4999 loss: 0.7863 time 24.59s
Final training  3589/4999 loss: 0.8254 time 24.56s
Final training  3590/4999 loss: 0.8255 time 24.64s
Final training  3591/4999 loss: 0.8073 time 24.69s
Final training  3592/4999 loss: 0.8084 time 24.58s
Final training  3593/4999 loss: 0.7775 time 24.81s
Final training  3594/4999 loss: 0.7962 time 24.82s
Final training  3595/4999 loss: 0.8105 time 24.79s
Final training  3596/4999 loss: 0.8415 time 25.08s
Final training  3597/4999 loss: 0.7992 time 24.81s
Final training  3598/4999 loss: 0.7922 time 24.68s
Final training  3599/4999 loss: 0.8125 time 24.59s
Dice accuracy for each class:  (tensor([0.9953, 0.9536, 0.9421, 0.9431, 0.7174, 0.7286, 0.8866, 0.8042, 0.9037,
        0.8505, 0.7448, 0.7677, 0.6690, 0.6494], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3599/4999 acc [ 0.827] time 181.65s
trigger times: 7
Final training  3600/4999 loss: 0.7693 time 24.56s
Final training  3601/4999 loss: 0.8083 time 24.79s
Final training  3602/4999 loss: 0.7933 time 24.41s
Final training  3603/4999 loss: 0.7920 time 24.69s
Final training  3604/4999 loss: 0.7919 time 24.67s
Final training  3605/4999 loss: 0.7976 time 24.64s
Final training  3606/4999 loss: 0.7750 time 24.69s
Final training  3607/4999 loss: 0.8044 time 24.51s
Final training  3608/4999 loss: 0.7721 time 24.62s
Final training  3609/4999 loss: 0.7708 time 24.83s
Final training  3610/4999 loss: 0.8018 time 24.69s
Final training  3611/4999 loss: 0.8114 time 24.70s
Final training  3612/4999 loss: 0.7845 time 24.91s
Final training  3613/4999 loss: 0.7741 time 24.41s
Final training  3614/4999 loss: 0.7733 time 24.49s
Final training  3615/4999 loss: 0.7927 time 24.79s
Final training  3616/4999 loss: 0.7701 time 25.00s
Final training  3617/4999 loss: 0.7881 time 24.84s
Final training  3618/4999 loss: 0.8057 time 24.77s
Final training  3619/4999 loss: 0.7863 time 24.61s
Final training  3620/4999 loss: 0.7975 time 24.71s
Final training  3621/4999 loss: 0.7868 time 24.62s
Final training  3622/4999 loss: 0.7832 time 24.54s
Final training  3623/4999 loss: 0.7858 time 25.09s
Final training  3624/4999 loss: 0.7905 time 25.35s
Final training  3625/4999 loss: 0.7917 time 24.88s
Final training  3626/4999 loss: 0.7677 time 25.42s
Final training  3627/4999 loss: 0.8095 time 25.23s
Final training  3628/4999 loss: 0.7833 time 25.15s
Final training  3629/4999 loss: 0.7841 time 24.96s
Final training  3630/4999 loss: 0.7926 time 24.74s
Final training  3631/4999 loss: 0.7774 time 24.79s
Final training  3632/4999 loss: 0.7892 time 25.10s
Final training  3633/4999 loss: 0.7873 time 25.13s
Final training  3634/4999 loss: 0.7972 time 25.19s
Final training  3635/4999 loss: 0.7934 time 25.01s
Final training  3636/4999 loss: 0.7970 time 25.09s
Final training  3637/4999 loss: 0.7900 time 24.83s
Final training  3638/4999 loss: 0.8049 time 25.32s
Final training  3639/4999 loss: 0.7813 time 25.18s
Final training  3640/4999 loss: 0.8155 time 25.55s
Final training  3641/4999 loss: 0.7803 time 25.11s
Final training  3642/4999 loss: 0.8117 time 25.09s
Final training  3643/4999 loss: 0.7981 time 24.94s
Final training  3644/4999 loss: 0.7750 time 24.79s
Final training  3645/4999 loss: 0.7629 time 24.72s
Final training  3646/4999 loss: 0.7770 time 24.46s
Final training  3647/4999 loss: 0.7853 time 24.65s
Final training  3648/4999 loss: 0.7882 time 24.48s
Final training  3649/4999 loss: 0.7947 time 24.40s
Final training  3650/4999 loss: 0.8003 time 24.83s
Final training  3651/4999 loss: 0.7926 time 25.10s
Final training  3652/4999 loss: 0.8198 time 24.80s
Final training  3653/4999 loss: 0.7843 time 24.57s
Final training  3654/4999 loss: 0.7884 time 24.99s
Final training  3655/4999 loss: 0.7960 time 24.95s
Final training  3656/4999 loss: 0.7987 time 25.01s
Final training  3657/4999 loss: 0.7900 time 24.96s
Final training  3658/4999 loss: 0.7768 time 24.67s
Final training  3659/4999 loss: 0.7770 time 24.76s
Final training  3660/4999 loss: 0.7712 time 25.04s
Final training  3661/4999 loss: 0.7882 time 24.83s
Final training  3662/4999 loss: 0.7530 time 24.97s
Final training  3663/4999 loss: 0.7840 time 25.07s
Final training  3664/4999 loss: 0.7943 time 24.29s
Final training  3665/4999 loss: 0.7880 time 24.98s
Final training  3666/4999 loss: 0.7819 time 24.73s
Final training  3667/4999 loss: 0.7592 time 24.79s
Final training  3668/4999 loss: 0.7818 time 24.61s
Final training  3669/4999 loss: 0.7992 time 24.54s
Final training  3670/4999 loss: 0.7822 time 24.39s
Final training  3671/4999 loss: 0.7843 time 24.89s
Final training  3672/4999 loss: 0.7673 time 24.70s
Final training  3673/4999 loss: 0.7576 time 24.44s
Final training  3674/4999 loss: 0.7728 time 24.63s
Final training  3675/4999 loss: 0.8027 time 24.52s
Final training  3676/4999 loss: 0.7740 time 24.49s
Final training  3677/4999 loss: 0.7789 time 24.70s
Final training  3678/4999 loss: 0.8006 time 24.61s
Final training  3679/4999 loss: 0.7902 time 24.48s
Final training  3680/4999 loss: 0.8030 time 24.54s
Final training  3681/4999 loss: 0.8023 time 24.70s
Final training  3682/4999 loss: 0.8075 time 24.48s
Final training  3683/4999 loss: 0.7677 time 23.98s
Final training  3684/4999 loss: 0.8086 time 24.57s
Final training  3685/4999 loss: 0.8013 time 24.11s
Final training  3686/4999 loss: 0.7692 time 24.58s
Final training  3687/4999 loss: 0.8041 time 24.75s
Final training  3688/4999 loss: 0.8105 time 24.98s
Final training  3689/4999 loss: 0.7779 time 24.89s
Final training  3690/4999 loss: 0.7837 time 24.90s
Final training  3691/4999 loss: 0.7975 time 25.13s
Final training  3692/4999 loss: 0.7727 time 24.96s
Final training  3693/4999 loss: 0.7968 time 24.94s
Final training  3694/4999 loss: 0.7656 time 24.84s
Final training  3695/4999 loss: 0.8023 time 24.83s
Final training  3696/4999 loss: 0.7913 time 24.87s
Final training  3697/4999 loss: 0.7820 time 24.78s
Final training  3698/4999 loss: 0.7981 time 24.77s
Final training  3699/4999 loss: 0.7887 time 24.81s
Dice accuracy for each class:  (tensor([0.9952, 0.9567, 0.9422, 0.9438, 0.7349, 0.7180, 0.8820, 0.7990, 0.9029,
        0.8555, 0.7427, 0.7895, 0.6665, 0.6386], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3699/4999 acc [ 0.827] time 183.26s
trigger times: 8
Final training  3700/4999 loss: 0.7864 time 25.03s
Final training  3701/4999 loss: 0.7979 time 25.14s
Final training  3702/4999 loss: 0.7907 time 24.80s
Final training  3703/4999 loss: 0.7608 time 25.26s
Final training  3704/4999 loss: 0.7665 time 25.39s
Final training  3705/4999 loss: 0.7889 time 25.21s
Final training  3706/4999 loss: 0.8033 time 25.17s
Final training  3707/4999 loss: 0.7913 time 24.95s
Final training  3708/4999 loss: 0.7725 time 25.02s
Final training  3709/4999 loss: 0.8093 time 24.94s
Final training  3710/4999 loss: 0.7838 time 24.87s
Final training  3711/4999 loss: 0.7815 time 24.69s
Final training  3712/4999 loss: 0.8096 time 24.63s
Final training  3713/4999 loss: 0.7932 time 24.66s
Final training  3714/4999 loss: 0.7572 time 24.57s
Final training  3715/4999 loss: 0.7818 time 24.64s
Final training  3716/4999 loss: 0.7814 time 24.61s
Final training  3717/4999 loss: 0.7919 time 24.50s
Final training  3718/4999 loss: 0.7750 time 24.64s
Final training  3719/4999 loss: 0.7708 time 24.53s
Final training  3720/4999 loss: 0.7786 time 24.65s
Final training  3721/4999 loss: 0.8030 time 24.72s
Final training  3722/4999 loss: 0.8348 time 24.71s
Final training  3723/4999 loss: 0.7694 time 24.60s
Final training  3724/4999 loss: 0.7959 time 24.57s
Final training  3725/4999 loss: 0.7737 time 24.57s
Final training  3726/4999 loss: 0.8005 time 24.81s
Final training  3727/4999 loss: 0.7747 time 24.71s
Final training  3728/4999 loss: 0.8034 time 24.82s
Final training  3729/4999 loss: 0.7796 time 24.92s
Final training  3730/4999 loss: 0.7854 time 24.67s
Final training  3731/4999 loss: 0.8105 time 24.94s
Final training  3732/4999 loss: 0.7901 time 24.84s
Final training  3733/4999 loss: 0.7913 time 24.73s
Final training  3734/4999 loss: 0.7880 time 25.05s
Final training  3735/4999 loss: 0.8086 time 24.63s
Final training  3736/4999 loss: 0.7854 time 24.99s
Final training  3737/4999 loss: 0.8079 time 25.05s
Final training  3738/4999 loss: 0.8022 time 24.87s
Final training  3739/4999 loss: 0.7862 time 25.02s
Final training  3740/4999 loss: 0.7917 time 25.21s
Final training  3741/4999 loss: 0.7860 time 25.24s
Final training  3742/4999 loss: 0.7904 time 25.38s
Final training  3743/4999 loss: 0.7737 time 24.70s
Final training  3744/4999 loss: 0.7728 time 25.23s
Final training  3745/4999 loss: 0.8013 time 24.71s
Final training  3746/4999 loss: 0.7944 time 24.73s
Final training  3747/4999 loss: 0.7756 time 24.64s
Final training  3748/4999 loss: 0.7738 time 24.68s
Final training  3749/4999 loss: 0.8041 time 24.88s
Final training  3750/4999 loss: 0.7833 time 25.04s
Final training  3751/4999 loss: 0.7876 time 25.47s
Final training  3752/4999 loss: 0.8050 time 25.09s
Final training  3753/4999 loss: 0.7689 time 24.71s
Final training  3754/4999 loss: 0.7917 time 24.50s
Final training  3755/4999 loss: 0.7879 time 25.07s
Final training  3756/4999 loss: 0.7979 time 25.18s
Final training  3757/4999 loss: 0.7876 time 24.97s
Final training  3758/4999 loss: 0.7888 time 24.97s
Final training  3759/4999 loss: 0.8219 time 24.74s
Final training  3760/4999 loss: 0.7922 time 24.47s
Final training  3761/4999 loss: 0.7761 time 24.88s
Final training  3762/4999 loss: 0.8092 time 24.66s
Final training  3763/4999 loss: 0.7869 time 24.65s
Final training  3764/4999 loss: 0.8053 time 24.20s
Final training  3765/4999 loss: 0.7701 time 24.65s
Final training  3766/4999 loss: 0.7750 time 24.56s
Final training  3767/4999 loss: 0.7790 time 24.81s
Final training  3768/4999 loss: 0.8015 time 24.80s
Final training  3769/4999 loss: 0.8014 time 24.67s
Final training  3770/4999 loss: 0.8007 time 24.93s
Final training  3771/4999 loss: 0.8042 time 24.74s
Final training  3772/4999 loss: 0.7957 time 24.77s
Final training  3773/4999 loss: 0.7765 time 24.45s
Final training  3774/4999 loss: 0.7721 time 24.70s
Final training  3775/4999 loss: 0.7674 time 24.51s
Final training  3776/4999 loss: 0.8031 time 24.75s
Final training  3777/4999 loss: 0.7812 time 24.94s
Final training  3778/4999 loss: 0.7962 time 24.98s
Final training  3779/4999 loss: 0.7826 time 25.08s
Final training  3780/4999 loss: 0.7852 time 24.66s
Final training  3781/4999 loss: 0.7759 time 24.86s
Final training  3782/4999 loss: 0.7962 time 24.75s
Final training  3783/4999 loss: 0.8107 time 24.43s
Final training  3784/4999 loss: 0.7860 time 24.71s
Final training  3785/4999 loss: 0.7799 time 24.60s
Final training  3786/4999 loss: 0.8012 time 24.88s
Final training  3787/4999 loss: 0.7840 time 24.71s
Final training  3788/4999 loss: 0.7852 time 24.84s
Final training  3789/4999 loss: 0.8023 time 25.33s
Final training  3790/4999 loss: 0.8039 time 25.30s
Final training  3791/4999 loss: 0.7909 time 25.02s
Final training  3792/4999 loss: 0.7917 time 25.25s
Final training  3793/4999 loss: 0.7988 time 25.14s
Final training  3794/4999 loss: 0.7899 time 25.16s
Final training  3795/4999 loss: 0.7868 time 25.12s
Final training  3796/4999 loss: 0.7983 time 24.91s
Final training  3797/4999 loss: 0.7830 time 24.82s
Final training  3798/4999 loss: 0.8036 time 24.79s
Final training  3799/4999 loss: 0.7944 time 24.56s
Dice accuracy for each class:  (tensor([0.9952, 0.9556, 0.9425, 0.9445, 0.7378, 0.7170, 0.8845, 0.7959, 0.9055,
        0.8457, 0.7440, 0.7735, 0.6593, 0.6349], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3799/4999 acc [ 0.825] time 183.51s
trigger times: 9
Final training  3800/4999 loss: 0.7814 time 24.57s
Final training  3801/4999 loss: 0.8106 time 24.53s
Final training  3802/4999 loss: 0.8095 time 24.70s
Final training  3803/4999 loss: 0.8065 time 24.64s
Final training  3804/4999 loss: 0.7940 time 24.55s
Final training  3805/4999 loss: 0.7907 time 25.07s
Final training  3806/4999 loss: 0.7930 time 24.83s
Final training  3807/4999 loss: 0.7873 time 24.75s
Final training  3808/4999 loss: 0.7800 time 24.66s
Final training  3809/4999 loss: 0.7848 time 24.63s
Final training  3810/4999 loss: 0.8029 time 24.57s
Final training  3811/4999 loss: 0.8040 time 24.84s
Final training  3812/4999 loss: 0.7896 time 25.04s
Final training  3813/4999 loss: 0.8047 time 24.50s
Final training  3814/4999 loss: 0.7758 time 24.48s
Final training  3815/4999 loss: 0.7947 time 24.86s
Final training  3816/4999 loss: 0.7582 time 24.79s
Final training  3817/4999 loss: 0.7779 time 25.07s
Final training  3818/4999 loss: 0.7843 time 24.91s
Final training  3819/4999 loss: 0.8090 time 24.84s
Final training  3820/4999 loss: 0.7697 time 24.59s
Final training  3821/4999 loss: 0.7755 time 24.81s
Final training  3822/4999 loss: 0.7907 time 24.89s
Final training  3823/4999 loss: 0.7889 time 25.08s
Final training  3824/4999 loss: 0.8035 time 25.28s
Final training  3825/4999 loss: 0.8224 time 25.05s
Final training  3826/4999 loss: 0.7742 time 24.99s
Final training  3827/4999 loss: 0.7713 time 25.11s
Final training  3828/4999 loss: 0.7762 time 24.99s
Final training  3829/4999 loss: 0.7862 time 24.71s
Final training  3830/4999 loss: 0.7923 time 24.72s
Final training  3831/4999 loss: 0.7733 time 24.90s
Final training  3832/4999 loss: 0.7951 time 24.49s
Final training  3833/4999 loss: 0.7841 time 24.93s
Final training  3834/4999 loss: 0.7742 time 24.60s
Final training  3835/4999 loss: 0.7945 time 24.68s
Final training  3836/4999 loss: 0.7820 time 24.58s
Final training  3837/4999 loss: 0.7915 time 24.46s
Final training  3838/4999 loss: 0.7837 time 24.34s
Final training  3839/4999 loss: 0.7837 time 24.79s
Final training  3840/4999 loss: 0.8067 time 24.59s
Final training  3841/4999 loss: 0.7932 time 24.64s
Final training  3842/4999 loss: 0.7842 time 24.74s
Final training  3843/4999 loss: 0.8183 time 24.59s
Final training  3844/4999 loss: 0.8002 time 24.61s
Final training  3845/4999 loss: 0.7750 time 24.42s
Final training  3846/4999 loss: 0.7784 time 24.53s
Final training  3847/4999 loss: 0.7954 time 24.45s
Final training  3848/4999 loss: 0.8078 time 24.68s
Final training  3849/4999 loss: 0.7699 time 24.64s
Final training  3850/4999 loss: 0.7845 time 24.66s
Final training  3851/4999 loss: 0.7847 time 24.74s
Final training  3852/4999 loss: 0.7767 time 25.22s
Final training  3853/4999 loss: 0.7949 time 24.96s
Final training  3854/4999 loss: 0.8071 time 24.73s
Final training  3855/4999 loss: 0.8157 time 24.61s
Final training  3856/4999 loss: 0.7921 time 24.58s
Final training  3857/4999 loss: 0.7749 time 24.48s
Final training  3858/4999 loss: 0.7889 time 24.70s
Final training  3859/4999 loss: 0.7865 time 24.71s
Final training  3860/4999 loss: 0.7659 time 24.68s
Final training  3861/4999 loss: 0.7760 time 24.82s
Final training  3862/4999 loss: 0.7994 time 24.39s
Final training  3863/4999 loss: 0.7908 time 24.60s
Final training  3864/4999 loss: 0.8029 time 24.37s
Final training  3865/4999 loss: 0.7992 time 25.00s
Final training  3866/4999 loss: 0.7896 time 24.95s
Final training  3867/4999 loss: 0.8004 time 25.06s
Final training  3868/4999 loss: 0.7946 time 25.07s
Final training  3869/4999 loss: 0.8022 time 24.93s
Final training  3870/4999 loss: 0.7827 time 25.32s
Final training  3871/4999 loss: 0.7809 time 24.63s
Final training  3872/4999 loss: 0.7741 time 24.87s
Final training  3873/4999 loss: 0.7985 time 24.80s
Final training  3874/4999 loss: 0.7820 time 25.08s
Final training  3875/4999 loss: 0.8218 time 25.10s
Final training  3876/4999 loss: 0.7879 time 24.66s
Final training  3877/4999 loss: 0.7930 time 25.05s
Final training  3878/4999 loss: 0.7940 time 24.86s
Final training  3879/4999 loss: 0.7821 time 24.57s
Final training  3880/4999 loss: 0.7768 time 24.43s
Final training  3881/4999 loss: 0.8140 time 24.61s
Final training  3882/4999 loss: 0.7681 time 24.76s
Final training  3883/4999 loss: 0.7954 time 24.46s
Final training  3884/4999 loss: 0.7923 time 24.38s
Final training  3885/4999 loss: 0.8014 time 24.56s
Final training  3886/4999 loss: 0.8000 time 24.65s
Final training  3887/4999 loss: 0.8116 time 24.83s
Final training  3888/4999 loss: 0.8228 time 24.82s
Final training  3889/4999 loss: 0.8109 time 24.83s
Final training  3890/4999 loss: 0.7763 time 24.53s
Final training  3891/4999 loss: 0.7762 time 24.74s
Final training  3892/4999 loss: 0.7896 time 24.76s
Final training  3893/4999 loss: 0.7882 time 24.59s
Final training  3894/4999 loss: 0.8060 time 24.71s
Final training  3895/4999 loss: 0.7748 time 24.94s
Final training  3896/4999 loss: 0.7650 time 25.18s
Final training  3897/4999 loss: 0.7950 time 24.77s
Final training  3898/4999 loss: 0.7900 time 24.73s
Final training  3899/4999 loss: 0.8116 time 24.70s
Dice accuracy for each class:  (tensor([0.9949, 0.9543, 0.9432, 0.9446, 0.7407, 0.7196, 0.8747, 0.7893, 0.9069,
        0.8578, 0.7510, 0.7704, 0.6674, 0.6002], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3899/4999 acc [ 0.823] time 182.06s
trigger times: 10
Final training  3900/4999 loss: 0.7959 time 24.77s
Final training  3901/4999 loss: 0.7733 time 25.10s
Final training  3902/4999 loss: 0.7963 time 24.82s
Final training  3903/4999 loss: 0.7956 time 25.14s
Final training  3904/4999 loss: 0.7817 time 25.26s
Final training  3905/4999 loss: 0.7841 time 25.16s
Final training  3906/4999 loss: 0.7868 time 25.28s
Final training  3907/4999 loss: 0.7697 time 25.03s
Final training  3908/4999 loss: 0.8153 time 25.06s
Final training  3909/4999 loss: 0.7826 time 24.68s
Final training  3910/4999 loss: 0.7762 time 24.96s
Final training  3911/4999 loss: 0.7922 time 24.96s
Final training  3912/4999 loss: 0.7670 time 24.76s
Final training  3913/4999 loss: 0.7806 time 24.84s
Final training  3914/4999 loss: 0.7660 time 24.34s
Final training  3915/4999 loss: 0.8094 time 24.78s
Final training  3916/4999 loss: 0.7861 time 24.50s
Final training  3917/4999 loss: 0.7647 time 24.55s
Final training  3918/4999 loss: 0.7546 time 24.67s
Final training  3919/4999 loss: 0.7894 time 24.43s
Final training  3920/4999 loss: 0.7840 time 24.58s
Final training  3921/4999 loss: 0.7772 time 24.75s
Final training  3922/4999 loss: 0.7866 time 24.62s
Final training  3923/4999 loss: 0.8135 time 24.64s
Final training  3924/4999 loss: 0.7759 time 24.61s
Final training  3925/4999 loss: 0.7839 time 24.35s
Final training  3926/4999 loss: 0.7849 time 24.77s
Final training  3927/4999 loss: 0.7813 time 24.72s
Final training  3928/4999 loss: 0.7942 time 24.57s
Final training  3929/4999 loss: 0.7808 time 24.45s
Final training  3930/4999 loss: 0.7533 time 24.75s
Final training  3931/4999 loss: 0.7845 time 24.62s
Final training  3932/4999 loss: 0.7767 time 24.38s
Final training  3933/4999 loss: 0.7702 time 24.58s
Final training  3934/4999 loss: 0.7837 time 24.96s
Final training  3935/4999 loss: 0.8039 time 24.97s
Final training  3936/4999 loss: 0.7593 time 24.99s
Final training  3937/4999 loss: 0.7840 time 24.82s
Final training  3938/4999 loss: 0.7765 time 24.87s
Final training  3939/4999 loss: 0.7976 time 24.89s
Final training  3940/4999 loss: 0.7983 time 24.97s
Final training  3941/4999 loss: 0.7739 time 25.03s
Final training  3942/4999 loss: 0.7815 time 24.86s
Final training  3943/4999 loss: 0.8027 time 25.12s
Final training  3944/4999 loss: 0.7560 time 25.39s
Final training  3945/4999 loss: 0.7641 time 24.99s
Final training  3946/4999 loss: 0.7885 time 24.77s
Final training  3947/4999 loss: 0.7958 time 24.51s
Final training  3948/4999 loss: 0.7928 time 24.96s
Final training  3949/4999 loss: 0.7979 time 25.01s
Final training  3950/4999 loss: 0.7737 time 25.07s
Final training  3951/4999 loss: 0.7843 time 25.30s
Final training  3952/4999 loss: 0.7832 time 25.22s
Final training  3953/4999 loss: 0.7693 time 24.74s
Final training  3954/4999 loss: 0.8088 time 24.90s
Final training  3955/4999 loss: 0.7869 time 25.27s
Final training  3956/4999 loss: 0.7732 time 25.19s
Final training  3957/4999 loss: 0.7867 time 25.07s
Final training  3958/4999 loss: 0.7881 time 25.00s
Final training  3959/4999 loss: 0.7772 time 24.83s
Final training  3960/4999 loss: 0.7533 time 25.13s
Final training  3961/4999 loss: 0.8003 time 25.16s
Final training  3962/4999 loss: 0.7857 time 25.02s
Final training  3963/4999 loss: 0.7855 time 25.17s
Final training  3964/4999 loss: 0.7783 time 25.05s
Final training  3965/4999 loss: 0.8040 time 24.57s
Final training  3966/4999 loss: 0.7504 time 24.92s
Final training  3967/4999 loss: 0.7862 time 24.41s
Final training  3968/4999 loss: 0.7972 time 25.15s
Final training  3969/4999 loss: 0.7813 time 25.27s
Final training  3970/4999 loss: 0.8047 time 25.03s
Final training  3971/4999 loss: 0.7880 time 24.97s
Final training  3972/4999 loss: 0.7939 time 24.96s
Final training  3973/4999 loss: 0.8163 time 24.87s
Final training  3974/4999 loss: 0.7835 time 25.03s
Final training  3975/4999 loss: 0.7644 time 25.15s
Final training  3976/4999 loss: 0.7863 time 24.89s
Final training  3977/4999 loss: 0.8108 time 24.85s
Final training  3978/4999 loss: 0.7781 time 24.80s
Final training  3979/4999 loss: 0.7519 time 24.71s
Final training  3980/4999 loss: 0.8030 time 24.89s
Final training  3981/4999 loss: 0.7786 time 24.84s
Final training  3982/4999 loss: 0.7847 time 24.81s
Final training  3983/4999 loss: 0.7962 time 24.93s
Final training  3984/4999 loss: 0.7888 time 25.00s
Final training  3985/4999 loss: 0.7889 time 24.78s
Final training  3986/4999 loss: 0.7970 time 24.95s
Final training  3987/4999 loss: 0.8052 time 25.08s
Final training  3988/4999 loss: 0.7912 time 24.94s
Final training  3989/4999 loss: 0.8004 time 25.13s
Final training  3990/4999 loss: 0.8098 time 24.98s
Final training  3991/4999 loss: 0.7679 time 25.17s
Final training  3992/4999 loss: 0.7689 time 25.31s
Final training  3993/4999 loss: 0.8007 time 24.87s
Final training  3994/4999 loss: 0.7941 time 24.95s
Final training  3995/4999 loss: 0.7864 time 24.91s
Final training  3996/4999 loss: 0.7906 time 25.21s
Final training  3997/4999 loss: 0.7594 time 25.10s
Final training  3998/4999 loss: 0.7700 time 25.12s
Final training  3999/4999 loss: 0.7783 time 25.17s
Dice accuracy for each class:  (tensor([0.9951, 0.9574, 0.9436, 0.9454, 0.7433, 0.7197, 0.8781, 0.8050, 0.9051,
        0.8577, 0.7426, 0.7887, 0.6753, 0.6237], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  3999/4999 acc [ 0.828] time 182.32s
trigger times: 11
Final training  4000/4999 loss: 0.7818 time 24.61s
Final training  4001/4999 loss: 0.7977 time 24.56s
Final training  4002/4999 loss: 0.8171 time 24.01s
Final training  4003/4999 loss: 0.7768 time 24.73s
Final training  4004/4999 loss: 0.7787 time 24.51s
Final training  4005/4999 loss: 0.7963 time 24.54s
Final training  4006/4999 loss: 0.8009 time 24.54s
Final training  4007/4999 loss: 0.7852 time 24.77s
Final training  4008/4999 loss: 0.7815 time 24.71s
Final training  4009/4999 loss: 0.7777 time 24.90s
Final training  4010/4999 loss: 0.7876 time 25.01s
Final training  4011/4999 loss: 0.7863 time 25.18s
Final training  4012/4999 loss: 0.7851 time 24.96s
Final training  4013/4999 loss: 0.7740 time 25.34s
Final training  4014/4999 loss: 0.7756 time 25.39s
Final training  4015/4999 loss: 0.7785 time 25.35s
Final training  4016/4999 loss: 0.7951 time 25.13s
Final training  4017/4999 loss: 0.8035 time 25.04s
Final training  4018/4999 loss: 0.7591 time 25.15s
Final training  4019/4999 loss: 0.7888 time 25.02s
Final training  4020/4999 loss: 0.7812 time 24.79s
Final training  4021/4999 loss: 0.7803 time 24.85s
Final training  4022/4999 loss: 0.8172 time 24.88s
Final training  4023/4999 loss: 0.7862 time 24.72s
Final training  4024/4999 loss: 0.8035 time 24.91s
Final training  4025/4999 loss: 0.7817 time 24.92s
Final training  4026/4999 loss: 0.7642 time 24.67s
Final training  4027/4999 loss: 0.7979 time 24.84s
Final training  4028/4999 loss: 0.7871 time 24.88s
Final training  4029/4999 loss: 0.7924 time 24.66s
Final training  4030/4999 loss: 0.7924 time 24.40s
Final training  4031/4999 loss: 0.7942 time 24.16s
Final training  4032/4999 loss: 0.7893 time 24.55s
Final training  4033/4999 loss: 0.7726 time 24.47s
Final training  4034/4999 loss: 0.7817 time 24.77s
Final training  4035/4999 loss: 0.7916 time 24.50s
Final training  4036/4999 loss: 0.7485 time 24.87s
Final training  4037/4999 loss: 0.8092 time 24.45s
Final training  4038/4999 loss: 0.7728 time 25.04s
Final training  4039/4999 loss: 0.7894 time 24.86s
Final training  4040/4999 loss: 0.7966 time 24.96s
Final training  4041/4999 loss: 0.8021 time 24.77s
Final training  4042/4999 loss: 0.8086 time 24.56s
Final training  4043/4999 loss: 0.8092 time 24.83s
Final training  4044/4999 loss: 0.7850 time 24.68s
Final training  4045/4999 loss: 0.7814 time 24.80s
Final training  4046/4999 loss: 0.7981 time 24.24s
Final training  4047/4999 loss: 0.7748 time 24.34s
Final training  4048/4999 loss: 0.7640 time 24.51s
Final training  4049/4999 loss: 0.7650 time 24.72s
Final training  4050/4999 loss: 0.7797 time 24.48s
Final training  4051/4999 loss: 0.8144 time 24.44s
Final training  4052/4999 loss: 0.7913 time 24.36s
Final training  4053/4999 loss: 0.7644 time 24.44s
Final training  4054/4999 loss: 0.8037 time 24.52s
Final training  4055/4999 loss: 0.8130 time 24.72s
Final training  4056/4999 loss: 0.7732 time 24.45s
Final training  4057/4999 loss: 0.7775 time 24.53s
Final training  4058/4999 loss: 0.7899 time 24.46s
Final training  4059/4999 loss: 0.7894 time 24.59s
Final training  4060/4999 loss: 0.7837 time 24.61s
Final training  4061/4999 loss: 0.7898 time 24.41s
Final training  4062/4999 loss: 0.7830 time 24.63s
Final training  4063/4999 loss: 0.7892 time 24.88s
Final training  4064/4999 loss: 0.7785 time 24.85s
Final training  4065/4999 loss: 0.7651 time 24.95s
Final training  4066/4999 loss: 0.7708 time 24.64s
Final training  4067/4999 loss: 0.7991 time 24.93s
Final training  4068/4999 loss: 0.7873 time 24.63s
Final training  4069/4999 loss: 0.8109 time 25.04s
Final training  4070/4999 loss: 0.7980 time 25.03s
Final training  4071/4999 loss: 0.7715 time 24.52s
Final training  4072/4999 loss: 0.7675 time 25.15s
Final training  4073/4999 loss: 0.7803 time 24.88s
Final training  4074/4999 loss: 0.7986 time 25.23s
Final training  4075/4999 loss: 0.7919 time 25.04s
Final training  4076/4999 loss: 0.7844 time 24.90s
Final training  4077/4999 loss: 0.7555 time 25.15s
Final training  4078/4999 loss: 0.7961 time 24.75s
Final training  4079/4999 loss: 0.7873 time 24.63s
Final training  4080/4999 loss: 0.7809 time 24.80s
Final training  4081/4999 loss: 0.7915 time 24.81s
Final training  4082/4999 loss: 0.7977 time 24.85s
Final training  4083/4999 loss: 0.7893 time 24.95s
Final training  4084/4999 loss: 0.7705 time 24.81s
Final training  4085/4999 loss: 0.7843 time 24.78s
Final training  4086/4999 loss: 0.7789 time 24.65s
Final training  4087/4999 loss: 0.7917 time 24.83s
Final training  4088/4999 loss: 0.8088 time 24.75s
Final training  4089/4999 loss: 0.7718 time 24.57s
Final training  4090/4999 loss: 0.7893 time 24.65s
Final training  4091/4999 loss: 0.7839 time 24.90s
Final training  4092/4999 loss: 0.8061 time 24.31s
Final training  4093/4999 loss: 0.7845 time 24.36s
Final training  4094/4999 loss: 0.7883 time 24.56s
Final training  4095/4999 loss: 0.7930 time 24.43s
Final training  4096/4999 loss: 0.7857 time 24.79s
Final training  4097/4999 loss: 0.7711 time 24.49s
Final training  4098/4999 loss: 0.7802 time 24.61s
Final training  4099/4999 loss: 0.7880 time 24.48s
Dice accuracy for each class:  (tensor([0.9950, 0.9577, 0.9436, 0.9452, 0.7261, 0.7116, 0.8743, 0.8069, 0.9077,
        0.8587, 0.7516, 0.7886, 0.6627, 0.6096], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4099/4999 acc [ 0.825] time 182.51s
trigger times: 12
Final training  4100/4999 loss: 0.7604 time 24.68s
Final training  4101/4999 loss: 0.7733 time 24.63s
Final training  4102/4999 loss: 0.8052 time 24.64s
Final training  4103/4999 loss: 0.7919 time 24.61s
Final training  4104/4999 loss: 0.7883 time 24.84s
Final training  4105/4999 loss: 0.7648 time 24.68s
Final training  4106/4999 loss: 0.7624 time 24.76s
Final training  4107/4999 loss: 0.7723 time 24.26s
Final training  4108/4999 loss: 0.7785 time 24.82s
Final training  4109/4999 loss: 0.7749 time 24.75s
Final training  4110/4999 loss: 0.7805 time 24.88s
Final training  4111/4999 loss: 0.8064 time 24.73s
Final training  4112/4999 loss: 0.7667 time 24.67s
Final training  4113/4999 loss: 0.8025 time 24.86s
Final training  4114/4999 loss: 0.7896 time 25.10s
Final training  4115/4999 loss: 0.8045 time 25.25s
Final training  4116/4999 loss: 0.7940 time 25.56s
Final training  4117/4999 loss: 0.7853 time 25.23s
Final training  4118/4999 loss: 0.7722 time 25.21s
Final training  4119/4999 loss: 0.7909 time 24.82s
Final training  4120/4999 loss: 0.7687 time 24.97s
Final training  4121/4999 loss: 0.7787 time 25.31s
Final training  4122/4999 loss: 0.7983 time 25.07s
Final training  4123/4999 loss: 0.7767 time 24.77s
Final training  4124/4999 loss: 0.7933 time 24.77s
Final training  4125/4999 loss: 0.7939 time 25.31s
Final training  4126/4999 loss: 0.7950 time 25.10s
Final training  4127/4999 loss: 0.7720 time 25.09s
Final training  4128/4999 loss: 0.7860 time 24.84s
Final training  4129/4999 loss: 0.7680 time 25.25s
Final training  4130/4999 loss: 0.7957 time 25.13s
Final training  4131/4999 loss: 0.7873 time 25.11s
Final training  4132/4999 loss: 0.7751 time 25.15s
Final training  4133/4999 loss: 0.7783 time 24.88s
Final training  4134/4999 loss: 0.7629 time 24.83s
Final training  4135/4999 loss: 0.8152 time 25.21s
Final training  4136/4999 loss: 0.7870 time 24.83s
Final training  4137/4999 loss: 0.7976 time 24.79s
Final training  4138/4999 loss: 0.7997 time 24.27s
Final training  4139/4999 loss: 0.7727 time 24.67s
Final training  4140/4999 loss: 0.8037 time 24.82s
Final training  4141/4999 loss: 0.8087 time 24.59s
Final training  4142/4999 loss: 0.7570 time 24.67s
Final training  4143/4999 loss: 0.7683 time 24.30s
Final training  4144/4999 loss: 0.7966 time 24.67s
Final training  4145/4999 loss: 0.8258 time 24.58s
Final training  4146/4999 loss: 0.7906 time 24.54s
Final training  4147/4999 loss: 0.7848 time 24.43s
Final training  4148/4999 loss: 0.7833 time 24.43s
Final training  4149/4999 loss: 0.7601 time 23.90s
Final training  4150/4999 loss: 0.7823 time 24.46s
Final training  4151/4999 loss: 0.7868 time 24.66s
Final training  4152/4999 loss: 0.7840 time 24.73s
Final training  4153/4999 loss: 0.7969 time 24.72s
Final training  4154/4999 loss: 0.7745 time 24.77s
Final training  4155/4999 loss: 0.7664 time 24.68s
Final training  4156/4999 loss: 0.7931 time 24.60s
Final training  4157/4999 loss: 0.7615 time 24.69s
Final training  4158/4999 loss: 0.7812 time 24.79s
Final training  4159/4999 loss: 0.7939 time 24.75s
Final training  4160/4999 loss: 0.7914 time 24.96s
Final training  4161/4999 loss: 0.7848 time 24.50s
Final training  4162/4999 loss: 0.7920 time 24.66s
Final training  4163/4999 loss: 0.7844 time 24.82s
Final training  4164/4999 loss: 0.7800 time 24.54s
Final training  4165/4999 loss: 0.7752 time 24.35s
Final training  4166/4999 loss: 0.7881 time 24.61s
Final training  4167/4999 loss: 0.7881 time 24.56s
Final training  4168/4999 loss: 0.7861 time 24.80s
Final training  4169/4999 loss: 0.7957 time 24.61s
Final training  4170/4999 loss: 0.7622 time 24.47s
Final training  4171/4999 loss: 0.7516 time 24.82s
Final training  4172/4999 loss: 0.7923 time 24.82s
Final training  4173/4999 loss: 0.7837 time 24.68s
Final training  4174/4999 loss: 0.8098 time 24.71s
Final training  4175/4999 loss: 0.7852 time 24.36s
Final training  4176/4999 loss: 0.7725 time 24.65s
Final training  4177/4999 loss: 0.7728 time 24.54s
Final training  4178/4999 loss: 0.7932 time 25.14s
Final training  4179/4999 loss: 0.7772 time 24.86s
Final training  4180/4999 loss: 0.7985 time 24.71s
Final training  4181/4999 loss: 0.7986 time 24.75s
Final training  4182/4999 loss: 0.8111 time 24.21s
Final training  4183/4999 loss: 0.7715 time 24.66s
Final training  4184/4999 loss: 0.7915 time 24.84s
Final training  4185/4999 loss: 0.7841 time 24.97s
Final training  4186/4999 loss: 0.7682 time 24.69s
Final training  4187/4999 loss: 0.8086 time 24.51s
Final training  4188/4999 loss: 0.7744 time 24.70s
Final training  4189/4999 loss: 0.7896 time 24.46s
Final training  4190/4999 loss: 0.7671 time 24.56s
Final training  4191/4999 loss: 0.8061 time 24.52s
Final training  4192/4999 loss: 0.7935 time 24.87s
Final training  4193/4999 loss: 0.7872 time 24.95s
Final training  4194/4999 loss: 0.7781 time 24.83s
Final training  4195/4999 loss: 0.7623 time 24.74s
Final training  4196/4999 loss: 0.8165 time 24.53s
Final training  4197/4999 loss: 0.7823 time 24.52s
Final training  4198/4999 loss: 0.7917 time 24.44s
Final training  4199/4999 loss: 0.8105 time 24.82s
Dice accuracy for each class:  (tensor([0.9950, 0.9543, 0.9433, 0.9448, 0.7471, 0.7049, 0.8775, 0.7883, 0.9055,
        0.8504, 0.7481, 0.7851, 0.6563, 0.6023], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4199/4999 acc [ 0.822] time 181.88s
trigger times: 13
Final training  4200/4999 loss: 0.7760 time 24.67s
Final training  4201/4999 loss: 0.7816 time 24.71s
Final training  4202/4999 loss: 0.7916 time 24.68s
Final training  4203/4999 loss: 0.7940 time 24.70s
Final training  4204/4999 loss: 0.7913 time 24.86s
Final training  4205/4999 loss: 0.8061 time 24.59s
Final training  4206/4999 loss: 0.7802 time 24.66s
Final training  4207/4999 loss: 0.7987 time 23.92s
Final training  4208/4999 loss: 0.7700 time 24.54s
Final training  4209/4999 loss: 0.8210 time 24.67s
Final training  4210/4999 loss: 0.7690 time 24.69s
Final training  4211/4999 loss: 0.7701 time 24.44s
Final training  4212/4999 loss: 0.8028 time 24.94s
Final training  4213/4999 loss: 0.7965 time 24.88s
Final training  4214/4999 loss: 0.7510 time 24.74s
Final training  4215/4999 loss: 0.7839 time 24.81s
Final training  4216/4999 loss: 0.7936 time 24.82s
Final training  4217/4999 loss: 0.7559 time 24.87s
Final training  4218/4999 loss: 0.7724 time 24.66s
Final training  4219/4999 loss: 0.8050 time 24.66s
Final training  4220/4999 loss: 0.7721 time 24.58s
Final training  4221/4999 loss: 0.7884 time 24.62s
Final training  4222/4999 loss: 0.7697 time 24.43s
Final training  4223/4999 loss: 0.7924 time 24.79s
Final training  4224/4999 loss: 0.7814 time 24.58s
Final training  4225/4999 loss: 0.7971 time 24.55s
Final training  4226/4999 loss: 0.8121 time 24.83s
Final training  4227/4999 loss: 0.7709 time 24.75s
Final training  4228/4999 loss: 0.7905 time 24.59s
Final training  4229/4999 loss: 0.7951 time 24.43s
Final training  4230/4999 loss: 0.7497 time 24.40s
Final training  4231/4999 loss: 0.8005 time 24.58s
Final training  4232/4999 loss: 0.7893 time 24.57s
Final training  4233/4999 loss: 0.7887 time 24.63s
Final training  4234/4999 loss: 0.8146 time 25.11s
Final training  4235/4999 loss: 0.7917 time 25.14s
Final training  4236/4999 loss: 0.7956 time 25.24s
Final training  4237/4999 loss: 0.7842 time 24.93s
Final training  4238/4999 loss: 0.7830 time 24.79s
Final training  4239/4999 loss: 0.7760 time 25.03s
Final training  4240/4999 loss: 0.7837 time 24.84s
Final training  4241/4999 loss: 0.7947 time 25.32s
Final training  4242/4999 loss: 0.7988 time 25.14s
Final training  4243/4999 loss: 0.7866 time 24.84s
Final training  4244/4999 loss: 0.7910 time 24.79s
Final training  4245/4999 loss: 0.7973 time 24.80s
Final training  4246/4999 loss: 0.7834 time 24.51s
Final training  4247/4999 loss: 0.8104 time 24.67s
Final training  4248/4999 loss: 0.7677 time 24.75s
Final training  4249/4999 loss: 0.7942 time 24.57s
Final training  4250/4999 loss: 0.7943 time 24.39s
Final training  4251/4999 loss: 0.8242 time 24.36s
Final training  4252/4999 loss: 0.7725 time 24.54s
Final training  4253/4999 loss: 0.8012 time 24.95s
Final training  4254/4999 loss: 0.8032 time 24.84s
Final training  4255/4999 loss: 0.7850 time 24.66s
Final training  4256/4999 loss: 0.7855 time 24.69s
Final training  4257/4999 loss: 0.7840 time 24.42s
Final training  4258/4999 loss: 0.7802 time 24.92s
Final training  4259/4999 loss: 0.7866 time 24.28s
Final training  4260/4999 loss: 0.7870 time 24.69s
Final training  4261/4999 loss: 0.7962 time 24.73s
Final training  4262/4999 loss: 0.7906 time 24.84s
Final training  4263/4999 loss: 0.8019 time 25.12s
Final training  4264/4999 loss: 0.7855 time 24.73s
Final training  4265/4999 loss: 0.7913 time 24.60s
Final training  4266/4999 loss: 0.7905 time 24.67s
Final training  4267/4999 loss: 0.7823 time 24.65s
Final training  4268/4999 loss: 0.7790 time 24.43s
Final training  4269/4999 loss: 0.7889 time 24.62s
Final training  4270/4999 loss: 0.8005 time 24.52s
Final training  4271/4999 loss: 0.8053 time 24.49s
Final training  4272/4999 loss: 0.8025 time 24.78s
Final training  4273/4999 loss: 0.7450 time 24.71s
Final training  4274/4999 loss: 0.8052 time 24.74s
Final training  4275/4999 loss: 0.8009 time 24.62s
Final training  4276/4999 loss: 0.7854 time 24.55s
Final training  4277/4999 loss: 0.8118 time 24.65s
Final training  4278/4999 loss: 0.7917 time 24.57s
Final training  4279/4999 loss: 0.7739 time 24.63s
Final training  4280/4999 loss: 0.7756 time 24.79s
Final training  4281/4999 loss: 0.7565 time 24.42s
Final training  4282/4999 loss: 0.7800 time 25.12s
Final training  4283/4999 loss: 0.7825 time 24.96s
Final training  4284/4999 loss: 0.7892 time 25.04s
Final training  4285/4999 loss: 0.7965 time 25.00s
Final training  4286/4999 loss: 0.7749 time 24.93s
Final training  4287/4999 loss: 0.8053 time 24.89s
Final training  4288/4999 loss: 0.8083 time 24.91s
Final training  4289/4999 loss: 0.7969 time 25.02s
Final training  4290/4999 loss: 0.7801 time 24.94s
Final training  4291/4999 loss: 0.7868 time 25.03s
Final training  4292/4999 loss: 0.7941 time 25.19s
Final training  4293/4999 loss: 0.7547 time 24.54s
Final training  4294/4999 loss: 0.8064 time 24.78s
Final training  4295/4999 loss: 0.7722 time 24.64s
Final training  4296/4999 loss: 0.7574 time 24.79s
Final training  4297/4999 loss: 0.7711 time 24.86s
Final training  4298/4999 loss: 0.7878 time 24.99s
Final training  4299/4999 loss: 0.7621 time 24.93s
Dice accuracy for each class:  (tensor([0.9951, 0.9557, 0.9436, 0.9449, 0.7606, 0.7140, 0.8785, 0.7996, 0.9085,
        0.8565, 0.7487, 0.7796, 0.6592, 0.6342], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4299/4999 acc [ 0.828] time 181.71s
trigger times: 14
Final training  4300/4999 loss: 0.7773 time 25.02s
Final training  4301/4999 loss: 0.7983 time 24.87s
Final training  4302/4999 loss: 0.7951 time 24.77s
Final training  4303/4999 loss: 0.7769 time 24.82s
Final training  4304/4999 loss: 0.8029 time 24.71s
Final training  4305/4999 loss: 0.7972 time 24.87s
Final training  4306/4999 loss: 0.8035 time 24.62s
Final training  4307/4999 loss: 0.7933 time 24.84s
Final training  4308/4999 loss: 0.7899 time 24.57s
Final training  4309/4999 loss: 0.7771 time 24.62s
Final training  4310/4999 loss: 0.7775 time 24.87s
Final training  4311/4999 loss: 0.7963 time 24.79s
Final training  4312/4999 loss: 0.8038 time 24.95s
Final training  4313/4999 loss: 0.7895 time 24.92s
Final training  4314/4999 loss: 0.7893 time 24.53s
Final training  4315/4999 loss: 0.7805 time 24.28s
Final training  4316/4999 loss: 0.7980 time 24.46s
Final training  4317/4999 loss: 0.7816 time 24.61s
Final training  4318/4999 loss: 0.7834 time 24.50s
Final training  4319/4999 loss: 0.7748 time 24.44s
Final training  4320/4999 loss: 0.7787 time 24.69s
Final training  4321/4999 loss: 0.7598 time 24.79s
Final training  4322/4999 loss: 0.7879 time 24.65s
Final training  4323/4999 loss: 0.7977 time 24.69s
Final training  4324/4999 loss: 0.7862 time 25.02s
Final training  4325/4999 loss: 0.7684 time 24.89s
Final training  4326/4999 loss: 0.7744 time 24.74s
Final training  4327/4999 loss: 0.7859 time 24.74s
Final training  4328/4999 loss: 0.7655 time 24.48s
Final training  4329/4999 loss: 0.7915 time 24.62s
Final training  4330/4999 loss: 0.7770 time 24.92s
Final training  4331/4999 loss: 0.8011 time 24.38s
Final training  4332/4999 loss: 0.7824 time 24.73s
Final training  4333/4999 loss: 0.7850 time 24.66s
Final training  4334/4999 loss: 0.7677 time 24.46s
Final training  4335/4999 loss: 0.7858 time 24.60s
Final training  4336/4999 loss: 0.8032 time 24.67s
Final training  4337/4999 loss: 0.7863 time 24.50s
Final training  4338/4999 loss: 0.7806 time 24.61s
Final training  4339/4999 loss: 0.7957 time 24.56s
Final training  4340/4999 loss: 0.7797 time 24.91s
Final training  4341/4999 loss: 0.7804 time 25.39s
Final training  4342/4999 loss: 0.7854 time 24.38s
Final training  4343/4999 loss: 0.7706 time 25.09s
Final training  4344/4999 loss: 0.7803 time 25.12s
Final training  4345/4999 loss: 0.8043 time 24.89s
Final training  4346/4999 loss: 0.7818 time 24.90s
Final training  4347/4999 loss: 0.7949 time 25.31s
Final training  4348/4999 loss: 0.7836 time 25.09s
Final training  4349/4999 loss: 0.7670 time 25.24s
Final training  4350/4999 loss: 0.7789 time 25.30s
Final training  4351/4999 loss: 0.7931 time 25.10s
Final training  4352/4999 loss: 0.7744 time 24.74s
Final training  4353/4999 loss: 0.7876 time 24.62s
Final training  4354/4999 loss: 0.8117 time 24.67s
Final training  4355/4999 loss: 0.7796 time 24.67s
Final training  4356/4999 loss: 0.7928 time 24.70s
Final training  4357/4999 loss: 0.8055 time 24.67s
Final training  4358/4999 loss: 0.8076 time 24.75s
Final training  4359/4999 loss: 0.7844 time 24.42s
Final training  4360/4999 loss: 0.7715 time 24.67s
Final training  4361/4999 loss: 0.7768 time 24.54s
Final training  4362/4999 loss: 0.7818 time 24.61s
Final training  4363/4999 loss: 0.7953 time 24.77s
Final training  4364/4999 loss: 0.7910 time 24.79s
Final training  4365/4999 loss: 0.7814 time 25.00s
Final training  4366/4999 loss: 0.7990 time 25.03s
Final training  4367/4999 loss: 0.7658 time 25.12s
Final training  4368/4999 loss: 0.8015 time 25.11s
Final training  4369/4999 loss: 0.7859 time 25.12s
Final training  4370/4999 loss: 0.7877 time 25.09s
Final training  4371/4999 loss: 0.7856 time 24.95s
Final training  4372/4999 loss: 0.7788 time 24.97s
Final training  4373/4999 loss: 0.7908 time 25.19s
Final training  4374/4999 loss: 0.7884 time 25.04s
Final training  4375/4999 loss: 0.7695 time 24.89s
Final training  4376/4999 loss: 0.7667 time 25.38s
Final training  4377/4999 loss: 0.7965 time 24.81s
Final training  4378/4999 loss: 0.8061 time 24.92s
Final training  4379/4999 loss: 0.7689 time 25.06s
Final training  4380/4999 loss: 0.7784 time 24.76s
Final training  4381/4999 loss: 0.7959 time 24.96s
Final training  4382/4999 loss: 0.7827 time 24.86s
Final training  4383/4999 loss: 0.8125 time 25.00s
Final training  4384/4999 loss: 0.7985 time 24.47s
Final training  4385/4999 loss: 0.7771 time 25.07s
Final training  4386/4999 loss: 0.8061 time 25.03s
Final training  4387/4999 loss: 0.7939 time 25.11s
Final training  4388/4999 loss: 0.8141 time 24.85s
Final training  4389/4999 loss: 0.7919 time 25.05s
Final training  4390/4999 loss: 0.8178 time 24.53s
Final training  4391/4999 loss: 0.7824 time 24.81s
Final training  4392/4999 loss: 0.8171 time 24.58s
Final training  4393/4999 loss: 0.7829 time 24.97s
Final training  4394/4999 loss: 0.7905 time 24.56s
Final training  4395/4999 loss: 0.7922 time 24.57s
Final training  4396/4999 loss: 0.7609 time 24.92s
Final training  4397/4999 loss: 0.7926 time 24.81s
Final training  4398/4999 loss: 0.7801 time 24.92s
Final training  4399/4999 loss: 0.7798 time 24.97s
Dice accuracy for each class:  (tensor([0.9949, 0.9539, 0.9429, 0.9448, 0.7355, 0.7099, 0.8717, 0.7988, 0.9077,
        0.8552, 0.7483, 0.7844, 0.6587, 0.6225], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4399/4999 acc [ 0.824] time 181.86s
trigger times: 15
Final training  4400/4999 loss: 0.8053 time 24.86s
Final training  4401/4999 loss: 0.7731 time 24.94s
Final training  4402/4999 loss: 0.7836 time 24.91s
Final training  4403/4999 loss: 0.7834 time 24.72s
Final training  4404/4999 loss: 0.7901 time 24.81s
Final training  4405/4999 loss: 0.7659 time 25.05s
Final training  4406/4999 loss: 0.7649 time 24.83s
Final training  4407/4999 loss: 0.7812 time 24.89s
Final training  4408/4999 loss: 0.7655 time 24.99s
Final training  4409/4999 loss: 0.7814 time 24.89s
Final training  4410/4999 loss: 0.7845 time 24.81s
Final training  4411/4999 loss: 0.8070 time 24.81s
Final training  4412/4999 loss: 0.7982 time 24.96s
Final training  4413/4999 loss: 0.7883 time 24.41s
Final training  4414/4999 loss: 0.8025 time 24.75s
Final training  4415/4999 loss: 0.7840 time 24.89s
Final training  4416/4999 loss: 0.7825 time 24.65s
Final training  4417/4999 loss: 0.7866 time 24.63s
Final training  4418/4999 loss: 0.7972 time 24.67s
Final training  4419/4999 loss: 0.7785 time 24.45s
Final training  4420/4999 loss: 0.7897 time 24.70s
Final training  4421/4999 loss: 0.7687 time 24.86s
Final training  4422/4999 loss: 0.7841 time 25.30s
Final training  4423/4999 loss: 0.7870 time 25.12s
Final training  4424/4999 loss: 0.7812 time 24.61s
Final training  4425/4999 loss: 0.7723 time 24.98s
Final training  4426/4999 loss: 0.8008 time 24.76s
Final training  4427/4999 loss: 0.8073 time 24.70s
Final training  4428/4999 loss: 0.7754 time 24.54s
Final training  4429/4999 loss: 0.7830 time 24.70s
Final training  4430/4999 loss: 0.8123 time 25.29s
Final training  4431/4999 loss: 0.7724 time 24.71s
Final training  4432/4999 loss: 0.7522 time 25.45s
Final training  4433/4999 loss: 0.8012 time 25.18s
Final training  4434/4999 loss: 0.7897 time 25.18s
Final training  4435/4999 loss: 0.7911 time 25.02s
Final training  4436/4999 loss: 0.8025 time 24.62s
Final training  4437/4999 loss: 0.7720 time 24.44s
Final training  4438/4999 loss: 0.8030 time 24.86s
Final training  4439/4999 loss: 0.7739 time 24.71s
Final training  4440/4999 loss: 0.8038 time 24.68s
Final training  4441/4999 loss: 0.7986 time 25.07s
Final training  4442/4999 loss: 0.7981 time 25.00s
Final training  4443/4999 loss: 0.7678 time 25.07s
Final training  4444/4999 loss: 0.7819 time 24.92s
Final training  4445/4999 loss: 0.7810 time 24.81s
Final training  4446/4999 loss: 0.7810 time 25.01s
Final training  4447/4999 loss: 0.7878 time 25.02s
Final training  4448/4999 loss: 0.7839 time 24.88s
Final training  4449/4999 loss: 0.8031 time 25.33s
Final training  4450/4999 loss: 0.8076 time 25.05s
Final training  4451/4999 loss: 0.7742 time 24.85s
Final training  4452/4999 loss: 0.8087 time 24.73s
Final training  4453/4999 loss: 0.8122 time 24.85s
Final training  4454/4999 loss: 0.7662 time 25.25s
Final training  4455/4999 loss: 0.7733 time 24.78s
Final training  4456/4999 loss: 0.7838 time 25.20s
Final training  4457/4999 loss: 0.8089 time 25.09s
Final training  4458/4999 loss: 0.7844 time 24.72s
Final training  4459/4999 loss: 0.7943 time 24.73s
Final training  4460/4999 loss: 0.7868 time 24.67s
Final training  4461/4999 loss: 0.7799 time 24.96s
Final training  4462/4999 loss: 0.7779 time 24.86s
Final training  4463/4999 loss: 0.7821 time 24.75s
Final training  4464/4999 loss: 0.7739 time 24.82s
Final training  4465/4999 loss: 0.7757 time 24.62s
Final training  4466/4999 loss: 0.7806 time 24.83s
Final training  4467/4999 loss: 0.7907 time 24.48s
Final training  4468/4999 loss: 0.7846 time 24.60s
Final training  4469/4999 loss: 0.7768 time 24.70s
Final training  4470/4999 loss: 0.8082 time 25.04s
Final training  4471/4999 loss: 0.7482 time 24.60s
Final training  4472/4999 loss: 0.7891 time 24.71s
Final training  4473/4999 loss: 0.7895 time 24.88s
Final training  4474/4999 loss: 0.7912 time 24.77s
Final training  4475/4999 loss: 0.7805 time 24.88s
Final training  4476/4999 loss: 0.7656 time 24.91s
Final training  4477/4999 loss: 0.7881 time 24.87s
Final training  4478/4999 loss: 0.7494 time 24.57s
Final training  4479/4999 loss: 0.7694 time 24.51s
Final training  4480/4999 loss: 0.7516 time 24.73s
Final training  4481/4999 loss: 0.7686 time 24.73s
Final training  4482/4999 loss: 0.7914 time 24.72s
Final training  4483/4999 loss: 0.8095 time 24.63s
Final training  4484/4999 loss: 0.7707 time 24.53s
Final training  4485/4999 loss: 0.7841 time 24.93s
Final training  4486/4999 loss: 0.7735 time 25.40s
Final training  4487/4999 loss: 0.7840 time 25.07s
Final training  4488/4999 loss: 0.7908 time 24.91s
Final training  4489/4999 loss: 0.7685 time 24.71s
Final training  4490/4999 loss: 0.7808 time 24.63s
Final training  4491/4999 loss: 0.7924 time 24.86s
Final training  4492/4999 loss: 0.7892 time 24.62s
Final training  4493/4999 loss: 0.7732 time 24.96s
Final training  4494/4999 loss: 0.7726 time 24.96s
Final training  4495/4999 loss: 0.7809 time 25.00s
Final training  4496/4999 loss: 0.7661 time 24.85s
Final training  4497/4999 loss: 0.7780 time 24.84s
Final training  4498/4999 loss: 0.7812 time 25.26s
Final training  4499/4999 loss: 0.7640 time 24.97s
Dice accuracy for each class:  (tensor([0.9951, 0.9517, 0.9437, 0.9451, 0.7433, 0.7073, 0.8791, 0.8036, 0.9060,
        0.8536, 0.7497, 0.7785, 0.6564, 0.6253], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4499/4999 acc [ 0.825] time 183.78s
trigger times: 16
Final training  4500/4999 loss: 0.7795 time 24.48s
Final training  4501/4999 loss: 0.7764 time 24.86s
Final training  4502/4999 loss: 0.7917 time 24.89s
Final training  4503/4999 loss: 0.7847 time 24.80s
Final training  4504/4999 loss: 0.7824 time 24.62s
Final training  4505/4999 loss: 0.7761 time 24.50s
Final training  4506/4999 loss: 0.7970 time 24.76s
Final training  4507/4999 loss: 0.7762 time 24.71s
Final training  4508/4999 loss: 0.7506 time 24.78s
Final training  4509/4999 loss: 0.7692 time 24.61s
Final training  4510/4999 loss: 0.8046 time 24.58s
Final training  4511/4999 loss: 0.7829 time 24.83s
Final training  4512/4999 loss: 0.7882 time 24.59s
Final training  4513/4999 loss: 0.8005 time 24.38s
Final training  4514/4999 loss: 0.8005 time 24.58s
Final training  4515/4999 loss: 0.7861 time 24.60s
Final training  4516/4999 loss: 0.7855 time 24.82s
Final training  4517/4999 loss: 0.7772 time 24.93s
Final training  4518/4999 loss: 0.7783 time 24.84s
Final training  4519/4999 loss: 0.7987 time 25.05s
Final training  4520/4999 loss: 0.7981 time 24.84s
Final training  4521/4999 loss: 0.7768 time 24.60s
Final training  4522/4999 loss: 0.7953 time 24.62s
Final training  4523/4999 loss: 0.7786 time 24.70s
Final training  4524/4999 loss: 0.7542 time 24.74s
Final training  4525/4999 loss: 0.7906 time 24.68s
Final training  4526/4999 loss: 0.7880 time 24.64s
Final training  4527/4999 loss: 0.7742 time 24.51s
Final training  4528/4999 loss: 0.7854 time 24.67s
Final training  4529/4999 loss: 0.7913 time 24.64s
Final training  4530/4999 loss: 0.7666 time 24.47s
Final training  4531/4999 loss: 0.7889 time 24.68s
Final training  4532/4999 loss: 0.7804 time 24.57s
Final training  4533/4999 loss: 0.7920 time 24.84s
Final training  4534/4999 loss: 0.7676 time 24.77s
Final training  4535/4999 loss: 0.7710 time 24.64s
Final training  4536/4999 loss: 0.7862 time 24.55s
Final training  4537/4999 loss: 0.7842 time 24.71s
Final training  4538/4999 loss: 0.7988 time 24.52s
Final training  4539/4999 loss: 0.7934 time 24.43s
Final training  4540/4999 loss: 0.7864 time 23.88s
Final training  4541/4999 loss: 0.7703 time 24.61s
Final training  4542/4999 loss: 0.7838 time 24.63s
Final training  4543/4999 loss: 0.8050 time 24.76s
Final training  4544/4999 loss: 0.7763 time 24.60s
Final training  4545/4999 loss: 0.7518 time 24.74s
Final training  4546/4999 loss: 0.7698 time 24.36s
Final training  4547/4999 loss: 0.7887 time 24.63s
Final training  4548/4999 loss: 0.7900 time 24.67s
Final training  4549/4999 loss: 0.8148 time 24.97s
Final training  4550/4999 loss: 0.7896 time 24.67s
Final training  4551/4999 loss: 0.7661 time 24.75s
Final training  4552/4999 loss: 0.7936 time 24.75s
Final training  4553/4999 loss: 0.7896 time 24.56s
Final training  4554/4999 loss: 0.8000 time 24.53s
Final training  4555/4999 loss: 0.7945 time 24.62s
Final training  4556/4999 loss: 0.7953 time 24.56s
Final training  4557/4999 loss: 0.7940 time 24.70s
Final training  4558/4999 loss: 0.7697 time 24.60s
Final training  4559/4999 loss: 0.7900 time 24.62s
Final training  4560/4999 loss: 0.7937 time 24.83s
Final training  4561/4999 loss: 0.7880 time 25.11s
Final training  4562/4999 loss: 0.8068 time 24.70s
Final training  4563/4999 loss: 0.7743 time 25.11s
Final training  4564/4999 loss: 0.7563 time 24.76s
Final training  4565/4999 loss: 0.8050 time 24.80s
Final training  4566/4999 loss: 0.7667 time 24.62s
Final training  4567/4999 loss: 0.7830 time 24.54s
Final training  4568/4999 loss: 0.7799 time 24.65s
Final training  4569/4999 loss: 0.8055 time 24.86s
Final training  4570/4999 loss: 0.7905 time 24.95s
Final training  4571/4999 loss: 0.7722 time 24.71s
Final training  4572/4999 loss: 0.7626 time 24.78s
Final training  4573/4999 loss: 0.7898 time 24.73s
Final training  4574/4999 loss: 0.7867 time 24.59s
Final training  4575/4999 loss: 0.7758 time 24.66s
Final training  4576/4999 loss: 0.7719 time 24.75s
Final training  4577/4999 loss: 0.7672 time 24.80s
Final training  4578/4999 loss: 0.8138 time 24.72s
Final training  4579/4999 loss: 0.7712 time 24.58s
Final training  4580/4999 loss: 0.7878 time 24.77s
Final training  4581/4999 loss: 0.8022 time 24.60s
Final training  4582/4999 loss: 0.7913 time 24.70s
Final training  4583/4999 loss: 0.7732 time 24.56s
Final training  4584/4999 loss: 0.7815 time 24.79s
Final training  4585/4999 loss: 0.7925 time 24.97s
Final training  4586/4999 loss: 0.7811 time 24.98s
Final training  4587/4999 loss: 0.7850 time 24.83s
Final training  4588/4999 loss: 0.7931 time 24.44s
Final training  4589/4999 loss: 0.7762 time 24.60s
Final training  4590/4999 loss: 0.7856 time 24.66s
Final training  4591/4999 loss: 0.7942 time 24.44s
Final training  4592/4999 loss: 0.7715 time 24.64s
Final training  4593/4999 loss: 0.8005 time 24.64s
Final training  4594/4999 loss: 0.7771 time 24.68s
Final training  4595/4999 loss: 0.7844 time 24.67s
Final training  4596/4999 loss: 0.7923 time 24.87s
Final training  4597/4999 loss: 0.7787 time 24.80s
Final training  4598/4999 loss: 0.7859 time 24.70s
Final training  4599/4999 loss: 0.8095 time 24.68s
Dice accuracy for each class:  (tensor([0.9949, 0.9513, 0.9440, 0.9453, 0.7342, 0.7113, 0.8742, 0.8040, 0.9069,
        0.8568, 0.7495, 0.7911, 0.6587, 0.6256], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4599/4999 acc [ 0.826] time 183.71s
trigger times: 17
Final training  4600/4999 loss: 0.7930 time 24.23s
Final training  4601/4999 loss: 0.8070 time 24.67s
Final training  4602/4999 loss: 0.7709 time 24.51s
Final training  4603/4999 loss: 0.7890 time 24.64s
Final training  4604/4999 loss: 0.7573 time 24.55s
Final training  4605/4999 loss: 0.7818 time 24.16s
Final training  4606/4999 loss: 0.7664 time 24.59s
Final training  4607/4999 loss: 0.8064 time 24.80s
Final training  4608/4999 loss: 0.8038 time 24.42s
Final training  4609/4999 loss: 0.7834 time 24.53s
Final training  4610/4999 loss: 0.7767 time 24.54s
Final training  4611/4999 loss: 0.7854 time 25.01s
Final training  4612/4999 loss: 0.7878 time 25.04s
Final training  4613/4999 loss: 0.7701 time 25.06s
Final training  4614/4999 loss: 0.7692 time 25.08s
Final training  4615/4999 loss: 0.8153 time 24.67s
Final training  4616/4999 loss: 0.7782 time 25.11s
Final training  4617/4999 loss: 0.7879 time 25.08s
Final training  4618/4999 loss: 0.8021 time 25.47s
Final training  4619/4999 loss: 0.7726 time 25.16s
Final training  4620/4999 loss: 0.7689 time 25.23s
Final training  4621/4999 loss: 0.7821 time 24.86s
Final training  4622/4999 loss: 0.7841 time 24.71s
Final training  4623/4999 loss: 0.7648 time 24.70s
Final training  4624/4999 loss: 0.7559 time 24.96s
Final training  4625/4999 loss: 0.7913 time 24.90s
Final training  4626/4999 loss: 0.7893 time 25.10s
Final training  4627/4999 loss: 0.7983 time 24.79s
Final training  4628/4999 loss: 0.7689 time 24.88s
Final training  4629/4999 loss: 0.7900 time 24.76s
Final training  4630/4999 loss: 0.7912 time 24.58s
Final training  4631/4999 loss: 0.8016 time 24.66s
Final training  4632/4999 loss: 0.8059 time 24.77s
Final training  4633/4999 loss: 0.7785 time 24.68s
Final training  4634/4999 loss: 0.7660 time 24.86s
Final training  4635/4999 loss: 0.7905 time 24.91s
Final training  4636/4999 loss: 0.7754 time 24.80s
Final training  4637/4999 loss: 0.8172 time 24.77s
Final training  4638/4999 loss: 0.7805 time 25.11s
Final training  4639/4999 loss: 0.7702 time 24.82s
Final training  4640/4999 loss: 0.7865 time 24.68s
Final training  4641/4999 loss: 0.7848 time 24.49s
Final training  4642/4999 loss: 0.7757 time 24.51s
Final training  4643/4999 loss: 0.8007 time 24.38s
Final training  4644/4999 loss: 0.7906 time 25.08s
Final training  4645/4999 loss: 0.7688 time 25.13s
Final training  4646/4999 loss: 0.7821 time 24.80s
Final training  4647/4999 loss: 0.7878 time 24.58s
Final training  4648/4999 loss: 0.7930 time 24.85s
Final training  4649/4999 loss: 0.7790 time 25.10s
Final training  4650/4999 loss: 0.7792 time 24.93s
Final training  4651/4999 loss: 0.7772 time 24.59s
Final training  4652/4999 loss: 0.7819 time 24.81s
Final training  4653/4999 loss: 0.7782 time 24.85s
Final training  4654/4999 loss: 0.7956 time 24.91s
Final training  4655/4999 loss: 0.7645 time 24.93s
Final training  4656/4999 loss: 0.7969 time 25.14s
Final training  4657/4999 loss: 0.8075 time 24.82s
Final training  4658/4999 loss: 0.7971 time 24.76s
Final training  4659/4999 loss: 0.7851 time 24.84s
Final training  4660/4999 loss: 0.8070 time 24.97s
Final training  4661/4999 loss: 0.7854 time 24.93s
Final training  4662/4999 loss: 0.7957 time 24.62s
Final training  4663/4999 loss: 0.7954 time 24.94s
Final training  4664/4999 loss: 0.8079 time 24.56s
Final training  4665/4999 loss: 0.7821 time 24.59s
Final training  4666/4999 loss: 0.7660 time 24.90s
Final training  4667/4999 loss: 0.7736 time 24.87s
Final training  4668/4999 loss: 0.7779 time 24.82s
Final training  4669/4999 loss: 0.7840 time 24.74s
Final training  4670/4999 loss: 0.7812 time 24.62s
Final training  4671/4999 loss: 0.7770 time 24.76s
Final training  4672/4999 loss: 0.7868 time 24.45s
Final training  4673/4999 loss: 0.7553 time 24.63s
Final training  4674/4999 loss: 0.7696 time 24.90s
Final training  4675/4999 loss: 0.7919 time 25.25s
Final training  4676/4999 loss: 0.7794 time 25.05s
Final training  4677/4999 loss: 0.7988 time 24.94s
Final training  4678/4999 loss: 0.8015 time 24.65s
Final training  4679/4999 loss: 0.7877 time 24.58s
Final training  4680/4999 loss: 0.8008 time 24.63s
Final training  4681/4999 loss: 0.7832 time 24.52s
Final training  4682/4999 loss: 0.8024 time 24.88s
Final training  4683/4999 loss: 0.7954 time 24.85s
Final training  4684/4999 loss: 0.7587 time 24.78s
Final training  4685/4999 loss: 0.7924 time 24.69s
Final training  4686/4999 loss: 0.7556 time 24.58s
Final training  4687/4999 loss: 0.7897 time 24.90s
Final training  4688/4999 loss: 0.7816 time 24.79s
Final training  4689/4999 loss: 0.7920 time 25.18s
Final training  4690/4999 loss: 0.7723 time 24.67s
Final training  4691/4999 loss: 0.7638 time 24.66s
Final training  4692/4999 loss: 0.7681 time 24.59s
Final training  4693/4999 loss: 0.7748 time 24.63s
Final training  4694/4999 loss: 0.7906 time 24.78s
Final training  4695/4999 loss: 0.7852 time 24.61s
Final training  4696/4999 loss: 0.7970 time 24.90s
Final training  4697/4999 loss: 0.8029 time 24.75s
Final training  4698/4999 loss: 0.7993 time 24.73s
Final training  4699/4999 loss: 0.7634 time 24.73s
Dice accuracy for each class:  (tensor([0.9949, 0.9522, 0.9440, 0.9451, 0.7431, 0.7110, 0.8735, 0.8016, 0.9065,
        0.8575, 0.7485, 0.7855, 0.6620, 0.6393], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4699/4999 acc [ 0.827] time 183.28s
trigger times: 18
Final training  4700/4999 loss: 0.7868 time 24.55s
Final training  4701/4999 loss: 0.8022 time 25.09s
Final training  4702/4999 loss: 0.7447 time 26.16s
Final training  4703/4999 loss: 0.7702 time 26.36s
Final training  4704/4999 loss: 0.7829 time 26.28s
Final training  4705/4999 loss: 0.7634 time 26.30s
Final training  4706/4999 loss: 0.7951 time 26.24s
Final training  4707/4999 loss: 0.7869 time 26.23s
Final training  4708/4999 loss: 0.7909 time 26.00s
Final training  4709/4999 loss: 0.7865 time 26.14s
Final training  4710/4999 loss: 0.7681 time 26.12s
Final training  4711/4999 loss: 0.7763 time 26.32s
Final training  4712/4999 loss: 0.7894 time 26.36s
Final training  4713/4999 loss: 0.7792 time 26.25s
Final training  4714/4999 loss: 0.7896 time 26.18s
Final training  4715/4999 loss: 0.7931 time 26.16s
Final training  4716/4999 loss: 0.7782 time 26.28s
Final training  4717/4999 loss: 0.7656 time 26.17s
Final training  4718/4999 loss: 0.7946 time 25.99s
Final training  4719/4999 loss: 0.7814 time 25.99s
Final training  4720/4999 loss: 0.7742 time 25.98s
Final training  4721/4999 loss: 0.7707 time 26.58s
Final training  4722/4999 loss: 0.7727 time 26.27s
Final training  4723/4999 loss: 0.7902 time 26.14s
Final training  4724/4999 loss: 0.7923 time 25.89s
Final training  4725/4999 loss: 0.7814 time 26.17s
Final training  4726/4999 loss: 0.7769 time 26.94s
Final training  4727/4999 loss: 0.7506 time 26.41s
Final training  4728/4999 loss: 0.7739 time 26.58s
Final training  4729/4999 loss: 0.7834 time 26.20s
Final training  4730/4999 loss: 0.7928 time 26.24s
Final training  4731/4999 loss: 0.7830 time 26.33s
Final training  4732/4999 loss: 0.7628 time 25.87s
Final training  4733/4999 loss: 0.7628 time 26.10s
Final training  4734/4999 loss: 0.7871 time 26.25s
Final training  4735/4999 loss: 0.7523 time 26.47s
Final training  4736/4999 loss: 0.7865 time 26.33s
Final training  4737/4999 loss: 0.7934 time 26.58s
Final training  4738/4999 loss: 0.7371 time 26.51s
Final training  4739/4999 loss: 0.7794 time 26.28s
Final training  4740/4999 loss: 0.7890 time 26.34s
Final training  4741/4999 loss: 0.8134 time 26.51s
Final training  4742/4999 loss: 0.7763 time 26.13s
Final training  4743/4999 loss: 0.7667 time 26.08s
Final training  4744/4999 loss: 0.7824 time 26.33s
Final training  4745/4999 loss: 0.8032 time 25.89s
Final training  4746/4999 loss: 0.7881 time 26.06s
Final training  4747/4999 loss: 0.7783 time 26.42s
Final training  4748/4999 loss: 0.7591 time 26.39s
Final training  4749/4999 loss: 0.7815 time 26.21s
Final training  4750/4999 loss: 0.8032 time 26.23s
Final training  4751/4999 loss: 0.7685 time 25.99s
Final training  4752/4999 loss: 0.7938 time 26.62s
Final training  4753/4999 loss: 0.7883 time 26.50s
Final training  4754/4999 loss: 0.7828 time 26.52s
Final training  4755/4999 loss: 0.7357 time 26.97s
Final training  4756/4999 loss: 0.7758 time 26.86s
Final training  4757/4999 loss: 0.7900 time 26.81s
Final training  4758/4999 loss: 0.7840 time 26.37s
Final training  4759/4999 loss: 0.7848 time 26.23s
Final training  4760/4999 loss: 0.7789 time 26.20s
Final training  4761/4999 loss: 0.7815 time 26.20s
Final training  4762/4999 loss: 0.7964 time 26.28s
Final training  4763/4999 loss: 0.7917 time 26.37s
Final training  4764/4999 loss: 0.7872 time 26.45s
Final training  4765/4999 loss: 0.7723 time 26.28s
Final training  4766/4999 loss: 0.7910 time 26.27s
Final training  4767/4999 loss: 0.7901 time 26.33s
Final training  4768/4999 loss: 0.7715 time 26.16s
Final training  4769/4999 loss: 0.8003 time 26.43s
Final training  4770/4999 loss: 0.7762 time 26.29s
Final training  4771/4999 loss: 0.7892 time 26.11s
Final training  4772/4999 loss: 0.8063 time 26.01s
Final training  4773/4999 loss: 0.7920 time 26.23s
Final training  4774/4999 loss: 0.7954 time 26.48s
Final training  4775/4999 loss: 0.7885 time 26.29s
Final training  4776/4999 loss: 0.7960 time 26.23s
Final training  4777/4999 loss: 0.7907 time 26.45s
Final training  4778/4999 loss: 0.7811 time 26.40s
Final training  4779/4999 loss: 0.7881 time 25.92s
Final training  4780/4999 loss: 0.7658 time 26.16s
Final training  4781/4999 loss: 0.7724 time 26.44s
Final training  4782/4999 loss: 0.7885 time 26.26s
Final training  4783/4999 loss: 0.7622 time 26.45s
Final training  4784/4999 loss: 0.7958 time 26.04s
Final training  4785/4999 loss: 0.7827 time 26.29s
Final training  4786/4999 loss: 0.7752 time 26.16s
Final training  4787/4999 loss: 0.7658 time 26.42s
Final training  4788/4999 loss: 0.7871 time 26.05s
Final training  4789/4999 loss: 0.7660 time 26.26s
Final training  4790/4999 loss: 0.7798 time 26.15s
Final training  4791/4999 loss: 0.7840 time 25.91s
Final training  4792/4999 loss: 0.7749 time 26.48s
Final training  4793/4999 loss: 0.7880 time 26.33s
Final training  4794/4999 loss: 0.7860 time 26.10s
Final training  4795/4999 loss: 0.7951 time 26.01s
Final training  4796/4999 loss: 0.7887 time 26.23s
Final training  4797/4999 loss: 0.7615 time 26.67s
Final training  4798/4999 loss: 0.7719 time 27.04s
Final training  4799/4999 loss: 0.7736 time 27.12s
Dice accuracy for each class:  (tensor([0.9948, 0.9528, 0.9439, 0.9449, 0.7405, 0.7098, 0.8708, 0.8019, 0.9070,
        0.8573, 0.7495, 0.7846, 0.6618, 0.6329], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4799/4999 acc [ 0.826] time 187.06s
trigger times: 19
Final training  4800/4999 loss: 0.7715 time 26.37s
Final training  4801/4999 loss: 0.7886 time 26.45s
Final training  4802/4999 loss: 0.7947 time 27.12s
Final training  4803/4999 loss: 0.7784 time 27.21s
Final training  4804/4999 loss: 0.7761 time 26.65s
Final training  4805/4999 loss: 0.7611 time 27.12s
Final training  4806/4999 loss: 0.8031 time 26.90s
Final training  4807/4999 loss: 0.7797 time 27.06s
Final training  4808/4999 loss: 0.7770 time 26.78s
Final training  4809/4999 loss: 0.8013 time 26.84s
Final training  4810/4999 loss: 0.7719 time 27.08s
Final training  4811/4999 loss: 0.7901 time 27.40s
Final training  4812/4999 loss: 0.7927 time 27.46s
Final training  4813/4999 loss: 0.7826 time 27.34s
Final training  4814/4999 loss: 0.7978 time 27.26s
Final training  4815/4999 loss: 0.7864 time 26.37s
Final training  4816/4999 loss: 0.7693 time 26.27s
Final training  4817/4999 loss: 0.7615 time 26.49s
Final training  4818/4999 loss: 0.7557 time 26.26s
Final training  4819/4999 loss: 0.7697 time 26.11s
Final training  4820/4999 loss: 0.7716 time 26.21s
Final training  4821/4999 loss: 0.7524 time 26.33s
Final training  4822/4999 loss: 0.7925 time 26.35s
Final training  4823/4999 loss: 0.7766 time 26.58s
Final training  4824/4999 loss: 0.7729 time 26.25s
Final training  4825/4999 loss: 0.7893 time 26.24s
Final training  4826/4999 loss: 0.7858 time 26.19s
Final training  4827/4999 loss: 0.7861 time 26.46s
Final training  4828/4999 loss: 0.7840 time 26.16s
Final training  4829/4999 loss: 0.7566 time 26.11s
Final training  4830/4999 loss: 0.7913 time 26.10s
Final training  4831/4999 loss: 0.7840 time 26.23s
Final training  4832/4999 loss: 0.7965 time 25.70s
Final training  4833/4999 loss: 0.7895 time 26.17s
Final training  4834/4999 loss: 0.7962 time 26.20s
Final training  4835/4999 loss: 0.7903 time 26.60s
Final training  4836/4999 loss: 0.7860 time 26.37s
Final training  4837/4999 loss: 0.7671 time 26.31s
Final training  4838/4999 loss: 0.7655 time 26.30s
Final training  4839/4999 loss: 0.7922 time 26.14s
Final training  4840/4999 loss: 0.7926 time 26.25s
Final training  4841/4999 loss: 0.7690 time 26.17s
Final training  4842/4999 loss: 0.7743 time 26.12s
Final training  4843/4999 loss: 0.7912 time 26.56s
Final training  4844/4999 loss: 0.7620 time 26.39s
Final training  4845/4999 loss: 0.7585 time 26.28s
Final training  4846/4999 loss: 0.7922 time 26.19s
Final training  4847/4999 loss: 0.7946 time 26.11s
Final training  4848/4999 loss: 0.7876 time 26.12s
Final training  4849/4999 loss: 0.7772 time 25.99s
Final training  4850/4999 loss: 0.7818 time 26.37s
Final training  4851/4999 loss: 0.7956 time 26.14s
Final training  4852/4999 loss: 0.7942 time 26.15s
Final training  4853/4999 loss: 0.7857 time 26.20s
Final training  4854/4999 loss: 0.8109 time 26.09s
Final training  4855/4999 loss: 0.7823 time 26.16s
Final training  4856/4999 loss: 0.7728 time 25.89s
Final training  4857/4999 loss: 0.7955 time 26.19s
Final training  4858/4999 loss: 0.7827 time 26.09s
Final training  4859/4999 loss: 0.8039 time 26.16s
Final training  4860/4999 loss: 0.7688 time 26.42s
Final training  4861/4999 loss: 0.7856 time 26.41s
Final training  4862/4999 loss: 0.8066 time 26.93s
Final training  4863/4999 loss: 0.7910 time 26.45s
Final training  4864/4999 loss: 0.7775 time 26.92s
Final training  4865/4999 loss: 0.7733 time 27.00s
Final training  4866/4999 loss: 0.7628 time 26.80s
Final training  4867/4999 loss: 0.7792 time 27.13s
Final training  4868/4999 loss: 0.8065 time 26.92s
Final training  4869/4999 loss: 0.7759 time 26.50s
Final training  4870/4999 loss: 0.7826 time 26.23s
Final training  4871/4999 loss: 0.7922 time 26.16s
Final training  4872/4999 loss: 0.7709 time 26.17s
Final training  4873/4999 loss: 0.7707 time 26.38s
Final training  4874/4999 loss: 0.7879 time 26.19s
Final training  4875/4999 loss: 0.8056 time 26.88s
Final training  4876/4999 loss: 0.7776 time 26.59s
Final training  4877/4999 loss: 0.7962 time 26.10s
Final training  4878/4999 loss: 0.7756 time 26.44s
Final training  4879/4999 loss: 0.7825 time 26.13s
Final training  4880/4999 loss: 0.8008 time 26.29s
Final training  4881/4999 loss: 0.7839 time 26.23s
Final training  4882/4999 loss: 0.7903 time 26.64s
Final training  4883/4999 loss: 0.7846 time 27.54s
Final training  4884/4999 loss: 0.7727 time 27.59s
Final training  4885/4999 loss: 0.8109 time 27.04s
Final training  4886/4999 loss: 0.7769 time 27.28s
Final training  4887/4999 loss: 0.7624 time 26.96s
Final training  4888/4999 loss: 0.7875 time 26.29s
Final training  4889/4999 loss: 0.7881 time 25.96s
Final training  4890/4999 loss: 0.7767 time 26.11s
Final training  4891/4999 loss: 0.7785 time 26.20s
Final training  4892/4999 loss: 0.7955 time 25.89s
Final training  4893/4999 loss: 0.7792 time 26.38s
Final training  4894/4999 loss: 0.7856 time 26.05s
Final training  4895/4999 loss: 0.7959 time 25.89s
Final training  4896/4999 loss: 0.7960 time 26.02s
Final training  4897/4999 loss: 0.7575 time 26.18s
Final training  4898/4999 loss: 0.7722 time 26.28s
Final training  4899/4999 loss: 0.7712 time 26.34s
Dice accuracy for each class:  (tensor([0.9949, 0.9525, 0.9441, 0.9454, 0.7407, 0.7110, 0.8725, 0.7999, 0.9069,
        0.8567, 0.7494, 0.7827, 0.6621, 0.6330], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4899/4999 acc [ 0.826] time 186.71s
trigger times: 20
Final training  4900/4999 loss: 0.7667 time 26.69s
Final training  4901/4999 loss: 0.7732 time 25.91s
Final training  4902/4999 loss: 0.7896 time 26.22s
Final training  4903/4999 loss: 0.7887 time 26.27s
Final training  4904/4999 loss: 0.7841 time 26.08s
Final training  4905/4999 loss: 0.7788 time 26.30s
Final training  4906/4999 loss: 0.7969 time 26.01s
Final training  4907/4999 loss: 0.7726 time 25.99s
Final training  4908/4999 loss: 0.7887 time 26.04s
Final training  4909/4999 loss: 0.7798 time 25.97s
Final training  4910/4999 loss: 0.7746 time 25.75s
Final training  4911/4999 loss: 0.7905 time 25.96s
Final training  4912/4999 loss: 0.7961 time 26.22s
Final training  4913/4999 loss: 0.7685 time 26.13s
Final training  4914/4999 loss: 0.7745 time 25.66s
Final training  4915/4999 loss: 0.8041 time 25.68s
Final training  4916/4999 loss: 0.7929 time 26.03s
Final training  4917/4999 loss: 0.7857 time 26.21s
Final training  4918/4999 loss: 0.7691 time 25.86s
Final training  4919/4999 loss: 0.7850 time 26.11s
Final training  4920/4999 loss: 0.7798 time 25.94s
Final training  4921/4999 loss: 0.7772 time 26.09s
Final training  4922/4999 loss: 0.7573 time 27.32s
Final training  4923/4999 loss: 0.8054 time 27.40s
Final training  4924/4999 loss: 0.7736 time 27.45s
Final training  4925/4999 loss: 0.7887 time 26.84s
Final training  4926/4999 loss: 0.7999 time 26.95s
Final training  4927/4999 loss: 0.7922 time 26.35s
Final training  4928/4999 loss: 0.7808 time 26.16s
Final training  4929/4999 loss: 0.8111 time 26.39s
Final training  4930/4999 loss: 0.7999 time 26.73s
Final training  4931/4999 loss: 0.7758 time 26.80s
Final training  4932/4999 loss: 0.7951 time 26.05s
Final training  4933/4999 loss: 0.7851 time 26.00s
Final training  4934/4999 loss: 0.7746 time 25.94s
Final training  4935/4999 loss: 0.7605 time 26.06s
Final training  4936/4999 loss: 0.7852 time 26.34s
Final training  4937/4999 loss: 0.7680 time 26.77s
Final training  4938/4999 loss: 0.7680 time 26.46s
Final training  4939/4999 loss: 0.7795 time 25.94s
Final training  4940/4999 loss: 0.8005 time 26.28s
Final training  4941/4999 loss: 0.7870 time 26.07s
Final training  4942/4999 loss: 0.7812 time 26.23s
Final training  4943/4999 loss: 0.8091 time 26.34s
Final training  4944/4999 loss: 0.7709 time 26.01s
Final training  4945/4999 loss: 0.7807 time 26.16s
Final training  4946/4999 loss: 0.7993 time 26.30s
Final training  4947/4999 loss: 0.7997 time 26.14s
Final training  4948/4999 loss: 0.8084 time 26.15s
Final training  4949/4999 loss: 0.7940 time 26.23s
Final training  4950/4999 loss: 0.7625 time 25.89s
Final training  4951/4999 loss: 0.7559 time 25.88s
Final training  4952/4999 loss: 0.7912 time 26.23s
Final training  4953/4999 loss: 0.7875 time 26.01s
Final training  4954/4999 loss: 0.7750 time 26.02s
Final training  4955/4999 loss: 0.7715 time 26.40s
Final training  4956/4999 loss: 0.7467 time 25.77s
Final training  4957/4999 loss: 0.7803 time 26.66s
Final training  4958/4999 loss: 0.7543 time 26.44s
Final training  4959/4999 loss: 0.8140 time 27.13s
Final training  4960/4999 loss: 0.7819 time 26.72s
Final training  4961/4999 loss: 0.7654 time 26.31s
Final training  4962/4999 loss: 0.8054 time 26.39s
Final training  4963/4999 loss: 0.7960 time 26.29s
Final training  4964/4999 loss: 0.7859 time 26.05s
Final training  4965/4999 loss: 0.7743 time 25.86s
Final training  4966/4999 loss: 0.7881 time 26.14s
Final training  4967/4999 loss: 0.7896 time 26.07s
Final training  4968/4999 loss: 0.7642 time 26.05s
Final training  4969/4999 loss: 0.7961 time 26.06s
Final training  4970/4999 loss: 0.7911 time 26.10s
Final training  4971/4999 loss: 0.7761 time 26.35s
Final training  4972/4999 loss: 0.7690 time 26.55s
Final training  4973/4999 loss: 0.7889 time 25.95s
Final training  4974/4999 loss: 0.8076 time 25.86s
Final training  4975/4999 loss: 0.7825 time 24.69s
Final training  4976/4999 loss: 0.7890 time 24.17s
Final training  4977/4999 loss: 0.7930 time 23.97s
Final training  4978/4999 loss: 0.7846 time 23.82s
Final training  4979/4999 loss: 0.7601 time 24.02s
Final training  4980/4999 loss: 0.7952 time 24.02s
Final training  4981/4999 loss: 0.7939 time 23.76s
Final training  4982/4999 loss: 0.7852 time 23.87s
Final training  4983/4999 loss: 0.7911 time 23.73s
Final training  4984/4999 loss: 0.7590 time 24.28s
Final training  4985/4999 loss: 0.7649 time 24.20s
Final training  4986/4999 loss: 0.7695 time 24.26s
Final training  4987/4999 loss: 0.7682 time 24.45s
Final training  4988/4999 loss: 0.7607 time 24.32s
Final training  4989/4999 loss: 0.7736 time 24.17s
Final training  4990/4999 loss: 0.7969 time 24.06s
Final training  4991/4999 loss: 0.7824 time 23.79s
Final training  4992/4999 loss: 0.7686 time 23.93s
Final training  4993/4999 loss: 0.7511 time 24.00s
Final training  4994/4999 loss: 0.8006 time 24.18s
Final training  4995/4999 loss: 0.7807 time 24.05s
Final training  4996/4999 loss: 0.7779 time 24.06s
Final training  4997/4999 loss: 0.7817 time 24.14s
Final training  4998/4999 loss: 0.7621 time 24.05s
Final training  4999/4999 loss: 0.7913 time 23.75s
Dice accuracy for each class:  (tensor([0.9949, 0.9525, 0.9441, 0.9454, 0.7412, 0.7110, 0.8721, 0.8012, 0.9069,
        0.8569, 0.7490, 0.7837, 0.6624, 0.6332], device='cuda:0'), tensor([6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6.],
       device='cuda:0'))
Final validation  4999/4999 acc [ 0.826] time 182.60s
trigger times: 21
Training Finished !, Best Accuracy:  0.830516517162323


Training Finished !, Best mean Validation Accuracy:  0.830516517162323
